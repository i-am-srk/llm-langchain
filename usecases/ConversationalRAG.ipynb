{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ddb519-7769-4c2c-adfa-6ebc85046af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2035f07-5aca-4662-ba6f-ada9372d1ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['COHERE_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dea3f2-57e9-4171-9d54-f147967517ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_api_key = os.environ['COHERE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cbbc74-fd73-4ad0-b830-1e8458dc02a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6930464-fc8a-49d8-8436-d40db2436ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFPlumberLoader('https://arxiv.org/pdf/2405.10725')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513b3510-3669-4eb5-a52c-2e43e2145b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='INDUS: Effective and Efficient Language Models for Scientific Applications\\nBishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\\nMuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\\nBharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\\nElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\\nThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\\nDavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\\nPanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\\nArminMehrabian9,TsendgarLee7\\n1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\\n7 NASAHQ,8 JPL,9 NASAGSFC\\nAbstract generation tasks. Most popular LLMs rely on the\\ntransformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-\\nlike Wikipedia or CommonCrawl (Devlin et al.,\\nsults on natural language processing (NLP)\\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\\ntasks. However, previous research demon-\\net al., 2020; Brown et al., 2020; Touvron et al.,\\nstratedLLMstrainedusingdomain-focusedcor-\\npora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\\nspired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\\nINDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\\nfortheEarthscience,biology,physics,helio- domain-specific natural language understanding\\nphysics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific\\ncorporadrawnfromdiversedatasources. The\\nLLMs such as SCIBERT (Beltagy et al., 2019),\\nsuiteofmodelsinclude: (1)anencodermodel\\ntrainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\\ncorpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\\nstandingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\\ngeneral text embedding model trained using developedwiththegoalofimprovingaccuracyon\\na diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\\nple sources to address information retrieval\\nWuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-\\ndisciplinaryfieldsrelatedtotheEarth,celestialbod-\\ntencyorresourceconstraints. Wealsocreated\\nies, the Sun, and planets within our solar system\\nthreenewscientificbenchmarkdatasetsnamely,\\nsuchasphysics,Earthscience,astrophysics,helio-\\nCLIMATE-CHANGE NER (entity-recognition),\\nNASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\\naccelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\\nfields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\\nelsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\\n(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the\\nasexistingbenchmarktasksinthedomainsof\\ninterdisciplinarynatureofthesedomainsofinter-\\ninterest.\\nestisreflectedinavastbodyofliteraturescattered\\n1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\\na collection of encoder-based LLMs focused on\\nLarge language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\\namountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\\npabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\\nzationsandenterprisesworkinginthesefieldsby\\nContact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\\nmuthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\\n4202\\nyaM\\n02\\n]LC.sc[\\n2v52701.5042:viXra\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Earth Science Data Pretraining\\n(Masked Finetuning Scientific Corpora\\nBioMedical Data Language Indus-Base (Contrastive\\nModelling) Learning) Open QA\\nAstrophysics Data\\nDuplicate Pairs\\nAstronomy Data\\nRepresentation Indus-Retriever-Base Citation Pairs\\nGeneral Science Distillation\\nData\\nFact Verification\\nGeneral English\\nData\\nGeneric Corpora\\nOutput\\nEncoder Training Corpus Indus-Small Distillation Embedding Training\\nCorpus\\nData Teacher (KD)\\nInitialization Output Indus-Retriever-\\nSmall\\nBC5-CHEM BC5-Disease NCBI-Disease BC2GM\\nNASA-QA TREC-Covid NFCorpus NQ HotpotQA\\nJNLPBA EBM-PICO ChemProt DDI\\nClimate FiQA Arguana Touche DBPedia NASA-IR\\nGAD HoC PubMedQA BioASQ Change NER\\nSciDocs FEVER C Fl Eim VEat Re SciFact\\nBIOSSES BLURB Benchmark BEIR Benchmark\\nNatural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir\\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\\nNASA-QA,CLIMATE-CHANGENERandNASA-IR.\\nenablingthemininformeddecision-making. (Beltagy et al., 2019). We also show that the\\nSpecifically, we make the following contribu- knowledge-distilledmodelsachievedasignifi-\\ntions: cantboostinlatencywhilemaintainingstrong\\n1. Utilizing the byte-pair encoding algorithm, empiricalperformancecomparedtotheoriginal\\nwe constructed INDUSBPE, a customized to- modelsonmostofthebenchmarktasks.\\nkenizerfromthecuratedscientificcorpus.\\n2 Data\\n2. Wepretrainedmultipleencoder-onlyLLMsus-\\ning the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than\\nsentence-embeddingmodelsbyfine-tuningthe\\ntheircounterpartstrainedonopen-domaincorpora.\\nencoder-only models with a contrastive learn-\\nWemeticulouslyidentifiedcorporaforeachofthe\\ningobjectivetolearn“universal”sentenceem-\\naforementioneddomains,andcreatedEnglish-only\\nbeddings (Gao et al., 2021) (§4). We also\\nmodels for the sake of containment. Specifically,\\ntrained smaller, more efficient versions of\\nforeachofthedomains,weusedopen-sourcedata\\nthesemodelsusingknowledge-distillationtech-\\nwhich has a permissive license, and further aug-\\nniques(§3.3,§4.2).\\nmentedthemwithspecificdatafromNASAandits\\n3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in\\nrecognitiontask),NASA-QA(anextractiveques-\\nourtrainingcorpora. Webrieflydescribeeachdata\\ntionansweringtask)and NASA-IR (aretrieval\\nsourcebelow,andpresentstatisticsofthedatain\\ntask)(§5)tofurtheraccelerateresearchinthis\\nTable1.\\nmulti-disciplinaryfield.\\n• SAO/NASAAstrophysicsDataSystem(ADS)1:\\n4. Throughexperimentalresults,weshowstrong\\nADSisthebiggestsourceofdata,coveringpub-\\nperformancebyourmodelsonthesebenchmark\\nlications in the areas of astronomy and astro-\\ntasks as well as on existing domain-specific\\nphysics,physicsandgeneralscienceincluding\\nbenchmarks, outperforming general-purpose\\nallarXive-prints.\\nmodels like RoBERTa (Liu et al., 2019) as\\nwellasscientific-domainencoderslikeSCIBERT 1https://ui.adsabs.harvard.edu\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Dataset Domain #Tokens Ratio Tokenizer ADS PMC Wikipedia\\nNASACMR EarthScience 0.3B 1% RoBERTa 12,867,439 7,549,075 15,859\\nAMSandAGUpapers EarthScience 2.8B 4% +lower_cased 12,862,227 7,557,868 16,901\\nEnglishWikipedia General 5.0B 8% INDUSBPE 12,309,023 6,920,659 16,056\\nPubMedAbstracts Biomedical 6.9B 10%\\nTable2: NumberoftokensproducedbyRoBERTaand\\nPMC Biomedical 18.5B 28%\\nINDUSBPEtokenizersappliedto1ksamplesfromeach\\nSAO/NASAADS Astronomy, 32.7B 49%\\ndataset. Fewer tokens lead to a smaller computation\\nAstrophysics,\\ncost.\\nPhysics,\\nGeneralScience\\nTotal 66.2B 100% ingdataset(§2)6. Forafaircomparison,wesetthe\\nvocabularysizeto50,265,whichisequaltothatof\\nTable1: Basicstatisticsofourpretrainingdataset.\\nthe RoBERTatokenizer(Liuetal.,2019)andused\\ntheuncasedvariationofboththetokenizers.\\n• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-\\nliterature maintained by National Library of BPE and the RoBERTa tokenizer. Out of 50,265\\nMedicineandNationalInstitutesofHealth. We tokens,22,355(44.5%)tokensarecommoninboth\\nusedtheportionofPMCthathasacommercial- thetokenizerswhiletheremaining27,910(55.5%)\\nfriendly license, along with the PubMed ab- tokensareincludedonlyineithertokenizer, indi-\\nstractsofallthearticlesinPMC. catingasignificantdistributionalshiftindomain.\\n• American Meteorological Society (AMS)3: Tofurther understandthe effect, weapplied both\\nWeusedfull-textjournaldocumentsspanning RoBERTaandINDUSBPEon1,000randomlysam-\\ntopicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts\\noceanography, atmospheric sciences, climate, tosinglesentences. AsshowninTable2, INDUS-\\nhydrometeorology, weather and forecasting, BPE tokenizer produced fewer tokens than the\\nandsocietalimpacts. RoBERTa tokenizer, leading to 8˜% drop in com-\\n• American Geophysical Union (AGU)4: The putationcostduringtraining.\\nAGUdatasetincludedjournaldocumentsacross Table3comparestheRoBERTatokenzierandIN-\\nthe topics of atmospheres, biogeosciences, DUSBPE tokenizer,illustratingthattheproposed\\nEarth surface, machine learning and compu- tokenizertreatedscientificterms(suchasbiomak-\\ntation, oceans, planets, solid earth, and space ers, phosphorylated, alzheimer) as single tokens\\nphysics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-\\n3.2 EncoderModel\\nquality,continuouslyevolvingmetadatasystem\\nthat catalogs all data and service metadata Wefirsttrainedanencoder-onlymodel,INDUS ,\\nBASE\\nrecords for NASA’s Earth Science Data and usingamaskedlanguagemodelingobjective. The\\nInformation System (ESDIS). It contains text modelarchitecturefollowsRoBERTa (Liuetal.,\\nBASE\\ndescriptions of the NASA Earth science data 2019),whichconsistsof12layersandhas125M\\nproducts. parameters. Weadoptedthedefaulthyperparame-\\nters7 butwithaneffectivebatchsizeof92,16. We\\n3 Methodology: EncoderModels trainedthemodelfor500Kstepsusing192V100\\nGPUs.\\n3.1 INDUSBPE Tokenizer\\n3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL\\n2https://www.ncbi.nlm.nih.gov/pmc 38M parameters through knowledge distillation\\n3https://www.ametsoc.org/index.cfm/ams/publications/\\n4https://agupubs.onlinelibrary.wiley.com/ 6We used HF tokenizers, https://github.com/\\n5https://www.earthdata.nasa.gov/eosdis/science-system- huggingface/tokenizers\\ndescription/eosdis-components/cmr 7WereferreaderstoTable9in(Liuetal.,2019).\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Inputtext\\nnoveltaubiomarkersphosphorylatedatt181,t217ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected\\nTokenizationbyRoBERTa\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTokenizationby INDUSBPE\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTable3: TokenizationcomparisonbetweenRoBERTaandourtokenizers. InputtextadaptedfromSuárez-Calvet\\netal.(2020).\\n(cid:88) (cid:88)\\ntechniquesbyusingINDUS\\nBASE\\nastheteacher. IN- Z\\ni\\n= es(qi,pj)+ es(qj,pi)\\nDUS follows a 4-layer architecture recom- j j\\nSMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the\\nwheres(q,p)isameasureoftemperature-scaled\\ndistillationobjectiveproposedinMiniLMv2(Wang\\ncosinesimilaritybetweentheembeddingsofquery\\netal.,2021)totransferfine-grainedself-attention\\nandapassagemeasuredby:\\nrelations,whichhasbeenshowntobethecurrent\\nstate-of-the-art(Udagawaetal.,2023). Usingthis 1 E(q)·E(p)\\ns(q,p) = (3)\\nobjective,wetrainedthemodelfor500Kstepswith τ ∥E(q)∥∥E(p)∥\\naneffectivebatchsizeof480on30V100GPUs.\\nTrainingData Similartopriorwork(Wangetal.,\\n2022; Li et al., 2023; Xiao et al., 2023), we em-\\n4 Methodology: SentenceEmbedding\\nployedastage-wisetrainingapproachforoursen-\\nModels\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional\\noccurringpairscollectedfrominternetsources,\\nvectors,allowingforefficientuseindenseretrieval\\nsuch as Wikipedia, StackExchange, etc. We\\nsystems,whererelevantpassagesforagivenquery\\nalsoincludedscientificdatafromPubMed,PMC\\nareidentifiedonthebasisofthesimilaritybetween\\n(§2),Arxivand S2ORC (Loetal.,2020)asin-\\ntheirembeddings(Karpukhinetal.,2020).\\ndomaindataforourscience-orientedretriever\\nmodel. Furthermore, we created a domain-\\nContrastiveLearningObjective Sentenceem-\\nspecific dataset from the ADS data (§2) by in-\\nbeddingmodelstrainedusingacontrastivelearn-\\ncludingtitle-abstractpairs.\\ningobjective(Khoslaetal.,2020;Gaoetal.,2021)\\n2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa\\n(Kwiatkowskietal.,2019),SQuAD(Rajpurkar\\nnon-relevantpassage.\\net al., 2016), SPECTER pairs (Cohan et al.,\\nInspired by recent work (Li et al., 2023), we\\n2020), etc. We included the aforementioned\\nusedanimprovedcontrastivelossbyintroducing\\nADS data and a sample of the S2ORC data in\\nanadditionalbidirectionalsignal. Specifically,for\\nthisstep,toboostdomain-specificsignals.\\na triple {q,p+,P−} of a query, a relevant (posi-\\nAppendixAcontainscomprehensivedetailsabout\\ntive)passage,andasetofnon-relevant(negative)\\nthe datasets used in training. For both training\\npassagesP− = {p−}m ,WedefinetheInfoNCE\\nj j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches\\n1 (cid:88)n es(qi,p+ i ) from each data source proportionately to its size,\\nL = − log (1)\\nIC\\nn Z i similartoLietal.(2023).\\ni=1\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Model Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K\\ninthelargermodelbutimprovestheperformance\\nstepsfollowedbyanadditional100Kstepswiththe\\nofthesmallermodel.\\nsuperviseddata. Weusedalearningrateof2e−5\\nFor distilling the sentence embedding model,\\nduringboththesesteps.\\nwefoundthatastage-wisetrainingapproachdoes\\n4.2 KnowledgeDistillationforEmbedding not benefit performance as much as in the non-\\nModels distillation case (ablation presented in Appendix\\nB). We thus distilled in a single step with all the\\nTooptimizethelatencyforretrievalapplications,\\ndatadescribedin§4.1andAppendixAandadded\\nwe also created a small retriever model with the\\nlabelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE\\nmodel(INDUS SMALL),bydistillingtheteacher’sdis- ModelSpecifications Webuiltthesentenceem-\\ntributionofsimilarityscores. Furthermore,wefind beddingmodelbydistillinginto INDUS . This\\nSMALL\\nthatitisnecessarytomodifythetrainingstrategy isa4-layermodelwithanembeddingdimension\\nfordistillation,asdescribedbelow. of576. Werefertotheresultingretrievermodelas\\nDistillationLoss Weusedknowledgedistillation\\nINDUS-RETRIEVER SMALL. Itfollowsabi-encoder\\nframework,andherewefindthatusingthevector\\ntechniquesintroducedin(Xuetal.,2023). Specif-\\nrepresentationofthefirsttokenastheembedding\\nically, for a sentence x and its corresponding in-\\ni\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-\\ntionp ofsimilarityscoresbetweenpairsandthe\\nt TrainingDetails FortheRetro-MAEstylepre-\\nstudent’sdistribution, p . FollowingHintonetal.\\ns training(Xiaoetal.,2022),wetrainedon8A100\\n(2014), we also scaled the output distribution of\\nGPUs with an effective batch size of 128 for 2\\nbothteacherandstudentbyatemperature,τ :\\nKD epochswithalearningrateof2e−5. Forthestage-\\nn m wisedistillation,wetrainedon2A100GPUsfor\\n(cid:88)(cid:88)\\nL = − p (x ,x )logp (x ,x ) (4) 300K steps with an effective batch size of 2,048,\\nKD t i j s i j\\ni=1 j=1 andlearningrateof7e−4. Throughexperimenta-\\ntion,Wefoundthatτ = 4performedthebest.\\nKD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)\\nt i j (cid:80)m est(xi,x k)/τKD ingthelanguageunderstandingcapabilitiesmodels.\\nk=1\\nHowever,tothebestourknowledge,thereisano-\\nHere,s (x ,x )ands (x ,x )representthesim-\\ns i j t i j\\nticeableabsenceofdatasetstailoredforthediverse\\nilarityscoresbetweentwopairs{x ,x },definedin\\ni j\\nand multidisciplinary field under study. Thus, to\\nEquation3forthestudentandteacherrespectively.\\neffectively benchmark the proposed NLP models\\nTrainingData Wefirstconductedaembedding- and further accelerate research in this multidisci-\\noriented pretraining step, as presented in Retro- plinarydomain,Weintroducedthreenewdatasets,\\nMAE (Xiao et al., 2022), on English Wikipedia, anNERtask,aQAtask,andanIRtask,described\\nBooksCorpus,andStackExchangedata,totalling below.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Train Validation Test spansoftheparagraphwhichanswerthequestions.\\nNum.Abstracts 382 77 75\\nWeused29paragraphs(with145QApairsintotal)\\nNum.Tokens 32,031 6,443 5,850\\nEntityLabels asthetrainingsetandtheremaining10paragraphs\\nclimate-nature,climate-greenhouse-gases,climate-assets,\\n(with 50 questions in total) as the evaluation set.\\nclimate-problem-origins,climate-mitigations,\\nclimate-properties,climate-impacts,climate-datasets, Thetrainingsetwasfurtheraugmentedwithpara-\\nclimate-organizations,climate-observations,\\ngraphsand QA pairsrelatedtoEarthsciencefrom\\nclimate-models,climate-hazards,climate-organisms\\nthe SQuADdataset(Rajpurkaretal.,2018). Specif-\\nTable4: CLIMATE-CHANGE NER statisticsandentity ically,thoserelatedtooxygen,Amazonrainforest\\nlabels\\nandgeologywereused. Thisresultedinapruned\\nSQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-\\nby augmenting these SQuAD pairs to the training\\nfer some assistance in exploring data related to\\ndatasourcedfromEarthsciencepapers,whilekeep-\\nclimatechange,thecomplexityofclimate-related\\ningtheevaluationsetintact.\\nqueries often requires more sophisticated natural\\nlanguageprocessingtechniques. Thisnecessityis 5.3 NASA-IR\\nunderscoredbytheextensivearrayofclimatemod-\\nWe introduced a domain-specific information re-\\nels, datasets, and organizations involved, which\\ntrieval benchmark, NASA-IR10, spanning almost\\ndemand meticulous curation and continuous up-\\n500question-answerpairsrelatedtotheEarthsci-\\ndates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-\\ndata,comprehensiveoverviewsofclimatemodels\\nically, we sampled a set of 166 paragraphs from\\nandimpactassessmentsarescarceandnoteasily\\nAGU, AMS, ADS, PMC and Pubmed (§2) and\\naccessible.\\nmanually annotated with 3 questions that are an-\\nInordertobridgethisgap,weintroducedacom-\\nswerablefromeachoftheseparagraphs,resulting\\nprehensive dataset for developing and evaluating\\nin498questions. Weused398ofthesequestions\\nNLP models tailored towards understanding and\\nas the training set and the remaining 100 as the\\naddressing climate-related topics across various\\nvalidation set. To comprehensively evaluate the\\ndomains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-\\nwhichthenamedentitiesofinterestoriginatefrom\\nstracts with these annotated paragraphs. On an\\ncomplextaxonomiesusedinclimate-relatedlitera-\\naverage, each query is 12 words long, and each\\nture. Thisdatasetcomprises534abstractssourced\\nparagraphis120wordslong. WeusedRecall@10\\nfromSemanticScholarAcademicGraph(Kinney\\nasevaluationmetricsinceeachquestionhasonly\\netal.,2023),collectedusingaseedsetofclimate-\\nonerelevantdocument.\\nrelated keywords such as wildfire or floods. The\\nabstractswereannotatedusingtheIOB(inside,out-\\n6 ExperimentalResults\\nside,beginning)taggingschemeandencompasses\\nadiversearrayofentitytypes,asshowninTable4. Baselines Wecompared INDUS modelsagainst\\nopensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.\\nsweringtaskfocusedontheEarthsciencedomain. • INDUS was compared to MINILM (6-\\nSMALL\\nFirst, 39 paragraphs from Earth science papers\\nlayer)13 andTINYBERT(4-layer)14.\\nwhichappearedinAGUandAMSjournals(§2)were\\n10https://huggingface.co/datasets/nasa-impact/nasa-smd-\\nsourced. Subject matter experts from NASA for- IR-benchmark\\nmulatedquestionsandmarkedthecorresponding 11https://huggingface.co/FacebookAI/roberta-base\\n12https://huggingface.co/allenai/scibert_scivocab_uncased\\n8https://huggingface.co/datasets/ibm/Climate-Change- 13https://huggingface.co/nreimers/MiniLM-L6-H384-\\nNER uncased\\n9https://huggingface.co/datasets/nasa-impact/nasa-smd- 14https://huggingface.co/huawei-\\nqa-benchmark noah/TinyBERT_General_4L_312D\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Basemodel(125Mparams.) Smallmodel(∼30Mparams.)\\nTask Metric Dataset RoBERTa SCIBERT INDUSBASE TINYBERT MINILM INDUSSMALL\\nBC5-chem 90.3(0.2) 91.4(0.2) 93.3(0.2) 84.6(0.2) 86.1(0.3) 90.7(0.1)\\nBC5-disease 81.5(0.3) 83.7(0.3) 85.2(0.3) 74.0(0.4) 77.4(0.3) 81.3(0.3)\\nNER EntityF1 NCBI-disease 87.6(0.6) 87.6(0.4) 88.3(0.4) 81.2(0.4) 83.1(0.5) 85.6(0.6)\\nBC2GM 82.1(0.3) 82.3(0.2) 84.0(0.3) 74.7(0.4) 77.1(0.2) 79.7(0.3)\\nJNLPBA 79.1(0.2) 78.2(0.2) 80.3(0.2) 70.3(0.2) 73.4(0.3) 75.7(0.2)\\nPICO MacroF1 EBMPICO 72.3(0.3) 72.4(0.3) 73.1(0.2) 67.4(0.2) 70.3(0.1) 73.1(0.2)\\nChemProt 50.4(28.2) 73.9(0.7) 76.9(0.5) 56.2(3.2) 55.9(2.1) 71.7(0.9)\\nRelation\\nMicroF1 DDI 78.6(1.5) 80.1(1.0) 81.7(0.5) 39.3(5.3) 51.5(2.9) 69.0(1.2)\\nExtraction\\nGAD 80.0(1.1) 81.6(1.2) 79.4(5.6) 76.4(1.3) 77.3(1.0) 81.3(0.7)\\nDocument\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy\\nAnswering BioASQ 69.1(4.8) 74.6(4.5) 69.6(5.8) 74.3(3.6) 66.7(2.3) 75.4(3.3)\\nSentence\\nSimilarity Pearson BIOSSES 79.8(6.3) 86.3(3.5) 72.2(9.5) 88.2(1.1) 26.6(8.7) 70.4(3.3)\\nMicroAverage - - 75.9(3.7) 79.2(1.3) 78.9(2.4) 67.6(1.9) 66.1(1.9) 76.2(1.0)\\nMacroAverage - - 74.9(3.7) 78.2(1.6) 76.4(3.2) 65.6(2.4) 60.6(3.0) 74.3(1.3)\\nTable5:EvaluationresultsonBLURB.Resultsreportedareaveragedon10randomseedswithstandarddeviationin\\nparenthesis. Microaverageisreportedacrossdatasetswhilemacroaverageiscomputedbyfirstaveragingscoreson\\neachtask(say,taskaverage),followedbyaveragingthetaskaverageacrosstasks. Resultsinboldindicatehighest\\nperformancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE\\nBGE 15 andaRoBERTa modelfinetuned RoBERTa 60.8(0.8)\\nBASE BASE\\nwiththesamemethodpresentedin§4.1. SCIBERT 61.8(0.7)\\n• INDUS-RETRIEVER\\nSMALL\\nwas compared to INDUS\\nBASE\\n64.0(1.0)\\nMINILM-V216 and BGE 17. TINYBERT 34.3(1.6)\\nSMALL\\nMINILM 44.7(1.3)\\n6.1 NaturalLanguageUnderstanding INDUS 54.8(0.8)\\nSMALL\\nBenchmarks\\nTable 6: CLIMATE-CHANGE NER benchmark results.\\nWe evaluated our models on BLURB (Gu et al.,\\nStandard deviation over 10 random seeds shown in\\n2021),abenchmarksuitefornaturallanguageun-\\nparenthesis. Resultsinboldandunderlineindicatehigh-\\nderstandingandreasoningtasksinthebiomedical estperformanceandsignificantdifferencefromsecond\\ndomain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.\\nTable 5 shows the evaluation results. Among\\nbase models, INDUS BASE significantly outper- WealsonoticedSCIBERTtendstoperformbetter\\nformsthegeneral-purposeRoBERTamodelonmi-\\nthan our model on paired input-text tasks, such\\ncro/macroaveragewhileachievingcompetitiveper-\\nas QA and semantic similarity tasks, although\\nformance to the bio-domain-specific counterpart,\\nthe results have relatively large standard devia-\\nSCIBERT.\\ntions. We hypothesized that the additional next\\nAsforsmallermodels,wenoticed INDUS SMALL sentence prediction objective during training in\\noutperformed the baselines, TINYBERT and BERT-stylemodels(suchasSCIBERT)incontrastto\\nMINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text\\ninNER,PICO,relationextraction,anddocument\\ntasks. Thistrendwasconsistentwiththeobserva-\\nclassification tasks. This demonstrates the effec-\\ntionsofTinnetal.(2023).\\ntivenessofknowledgedistillationfromourdomain-\\nspecificteachermodel,INDUS . 6.2 CLIMATE-CHANGE NER\\nBASE\\nAs shown in Table 6, our models clearly outper-\\n15https://huggingface.co/BAAI/bge-base-en-v1.5\\n16sentence-transformers/all-MiniLM-L6-v2 formedthecorrespondingbaselinemodelsonthe\\n17https://huggingface.co/BAAI/bge-small-en-v1.5 CLIMATE-CHANGE NERtask,suggestingtheeffec-\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='tivenessoftrainingonlargedomain-specificdata. Model NASA-IR↑ BEIRAvg.↑ Retrieval\\nTime↓\\n6.3 NASA-QA BRo GB EE BAR ST Ea BASE 0 0. .6 66\\n7\\n0 0. .3 57\\n2\\n1 1. .2 10\\n8\\nAs mentioned in §5, we augmented the training\\nINDUS-RETRIEVERBASE 0.71 0.41 1.19\\nMINILM-V2 0.62 0.39 0.24\\nsetwithrelevantSQuADpairsforfine-tuning. All BGESMALL 0.66 0.51 0.42\\nmodelsarefinetunedfor15epochs,andtheresults INDUS-RETRIEVERSMALL 0.73 0.42 0.26\\nareshowninTable7. WeobservedthatINDUS\\nBASE Table 8: Evaluation results on NASA-IR and BEIR.\\noutperformedallmodelsofsimilarsizes,while IN-\\nNASA-IRshowedRecall@10whileBEIRreportedthe\\nDUS hadrelativelystrongperformancecom- averagenDCG@10acrossalltasks. Retrievaltimeper\\nSMALL\\nparedtoitscounterparts. queryontheNQtaskfromBEIR,reportedinseconds.\\nModel F1(SD)\\n7 Conclusions\\nRoBERTa 66.8(3.1)\\nSCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-\\nMINILM 59.2(3.9) enizer and in-domain data for training high qual-\\nINDUS 47.4(1.8) ityencodermodelsandsentenceembeddingmod-\\nSMALL\\nels. Further, we created smaller versions of the\\nTable7: NASA-QA benchmarkresults. Standarddevi- proposedmodelssuitableforapplicationswithla-\\nation over 3 random seeds shown in parenthesis. Re-\\ntencyorresourceconstraintsthroughstate-of-the-\\nsultsinboldandunderlineindicatehighestperformance\\nartknowledgedistillationtechniques. Fortheben-\\nandsignificantdifferencefromsecondhighestresultby\\nefit of the scientific community, we will release\\nmorethantwostandarddeviationsineachmodelsize,\\nthedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.\\nDoguAraci.2019. Finbert: Financialsentimentanaly-\\nsiswithpre-trainedlanguagemodels.\\n6.4 InformationRetrievalBenchmarks\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nWeevaluatedourmodelsonthe NASA-IR dataset\\nERT:Apretrainedlanguagemodelforscientifictext.\\naswellasBEIRBenchmark(Thakuretal.,2021), InProceedingsofthe2019ConferenceonEmpirical\\nwhichconsistsof12retrievaltasksspanningavari- Methods in Natural Language Processing and the\\n9thInternationalJointConferenceonNaturalLan-\\netyofdomains. TheBEIRbenchmarkusedtheNor-\\nguageProcessing(EMNLP-IJCNLP),pages3615–\\nmalized Cumulative Discount Gain (nDCG@10)\\n3620,HongKong,China.AssociationforComputa-\\n(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\ntenceembeddingmodels,alongwithourbaselines.\\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\\nAs shown, both of our sentence embedding mod- Neelakantan,PranavShyam,GirishSastry,Amanda\\nelssignificantlyoutperformedthebaselinesonthe Askell, Sandhini Agarwal, Ariel Herbert-Voss,\\nNASA-IRtaskwhilestillmaintaininggoodperfor- Gretchen Krueger, Tom Henighan, Rewon Child,\\nAdityaRamesh,DanielZiegler,JeffreyWu,Clemens\\nmanceonseveraloftheBEIRtasks. (Wepresented\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nresultsforeachBEIRtaskinAppendixC).\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nWe also measured the average time per query Clark, ChristopherBerner, SamMcCandlish, Alec\\nforretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-\\nquestionssetofBEIR,onasingleA100GPU.This\\nvances in Neural Information Processing Systems,\\ntime includes the time to encode the query, cor-\\nvolume 33, pages 1877–1901. Curran Associates,\\npus,andtimetoretrieverelevantdocuments. No- Inc.\\ntably, INDUS-RETRIEVER outperformed IN-\\nSMALL Colin B. Clement, Matthew Bierbaum, Kevin P.\\nDUS-RETRIEVER BASE,onbothNASA-IRandBEIR,\\nO’Keeffe, and Alexander A. Alemi. 2019. On the\\nwhilebeingabout4.6xfaster. useofarxivasadataset.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Sarna, Yonglong Tian, Phillip Isola, Aaron\\nDowney, and Daniel S. Weld. 2020. SPECTER: Maschinot, CeLiu, andDilipKrishnan.2020. Su-\\nDocument-level Representation Learning using pervisedcontrastivelearning. InAdvancesinNeural\\nCitation-informedTransformers. InACL. InformationProcessingSystems,volume33,pages\\n18661–18673.CurranAssociates,Inc.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of Rodney Kinney, Chloe Anastasiades, Russell Authur,\\ndeepbidirectionaltransformersforlanguageunder- Iz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\\nstanding. InProceedingsofthe2019Conferenceof ski,IsabelCachola,StefanCandra,YoganandChan-\\ntheNorthAmericanChapteroftheAssociationfor drasekhar, Arman Cohan, Miles Crawford, Doug\\nComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-\\n4171–4186,Minneapolis,Minnesota.Associationfor ham, Fangzhou Hu, Regan Huff, Daniel King, Se-\\nComputationalLinguistics. bastianKohlmeier,BaileyKuehl,MichaelLangan,\\nDaniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\\nMatthewDunn,LeventSagun,MikeHiggins,V.Ugur Kelsey MacMillan, Tyler Murray, Chris Newell,\\nGuney, Volkan Cirik, and Kyunghyun Cho. 2017. Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\\nSearchqa: Anewq&adatasetaugmentedwithcon- Shen,AmanpreetSingh,LucaSoldaini,Shivashankar\\ntextfromasearchengine. Subramanian,AmberTanaka,AlexD.Wade,Linda\\nWagner, Lucy Lu Wang, Chris Wilhelm, Caroline\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic\\ntractedknowledgebases. InProceedingsofthe20th scholaropendataplatform.\\nACMSIGKDDInternationalConferenceonKnowl-\\nedge Discovery and Data Mining, KDD ’14, page TomKwiatkowski, JennimariaPalomaki, OliviaRed-\\n1156–1165, New York, NY, USA. Association for field,MichaelCollins,AnkurParikh,ChrisAlberti,\\nComputingMachinery. DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\\ntonLee,KristinaToutanova,LlionJones,Matthew\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nSimCSE:Simplecontrastivelearningofsentenceem- Uszkoreit,QuocLe,andSlavPetrov.2019. Natural\\nbeddings. In Proceedings of the 2021 Conference Questions: A Benchmark for Question Answering\\nonEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-\\nJinhyukLee,WonjinYoon,SungdongKim,Donghyeon\\nminican Republic. Association for Computational\\nKim,SunkyuKim,ChanHoSo,andJaewooKang.\\nLinguistics.\\n2019. BioBERT:apre-trainedbiomedicallanguage\\nrepresentation model for biomedical text mining.\\nYuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto\\nBioinformatics,36(4):1234–1240.\\nUsuyama,XiaodongLiu,TristanNaumann,Jianfeng\\nGao,andHoifungPoon.2021. Domain-specificlan-\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nguagemodelpretrainingforbiomedicalnaturallan-\\nGhazvininejad,AbdelrahmanMohamed,OmerLevy,\\nguageprocessing. ACMTrans.Comput.Healthcare,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\n3(1).\\nBART:Denoisingsequence-to-sequencepre-training\\nfornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In\\ningoftheAssociationforComputationalLinguistics,\\nNeurIPSDeepLearningWorksop.\\npages7871–7880,Online.AssociationforComputa-\\ntionalLinguistics.\\nZhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon\\nDuede,KyleChard,andIanFoster.2023. Thedimin- PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-\\nishingreturnsofmaskedlanguagemodelstoscience. ervini,HeinrichKüttler,AleksandraPiktus,Pontus\\nStenetorp,andSebastianRiedel.2021. PAQ:65Mil-\\nShuHuangandJacquelineMCole.2022. Batterybert:\\nlionProbably-AskedQuestionsandWhatYouCan\\nA pretrained language model for battery database\\nDoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,\\nVladimirKarpukhin,BarlasOguz,SewonMin,Patrick Pengjun Xie, and Meishan Zhang. 2023. Towards\\nLewis,LedellWu,SergeyEdunov,DanqiChen,and generaltextembeddingswithmulti-stagecontrastive\\nWen-tauYih.2020. Densepassageretrievalforopen- learning.\\ndomainquestionanswering. InProceedingsofthe\\n2020ConferenceonEmpiricalMethodsinNatural YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-\\nLanguageProcessing(EMNLP),pages6769–6781, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nOnline.AssociationforComputationalLinguistics. Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron proach. arXivpreprintarXiv:1907.11692.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='KyleLo,LucyLuWang,MarkNeumann,RodneyKin- and VERification. In Proceedings of the 2018\\nney,andDanielWeld.2020. S2ORC:Thesemantic Conference of the North American Chapter of\\nscholaropenresearchcorpus. InProceedingsofthe the Association for Computational Linguistics:\\n58thAnnualMeetingoftheAssociationforCompu- Human Language Technologies, Volume 1 (Long\\ntationalLinguistics,pages4969–4983,Online.Asso- Papers), pages 809–819, New Orleans, Louisiana.\\nciationforComputationalLinguistics. AssociationforComputationalLinguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, RobertTinn,HaoCheng,YuGu,NaotoUsuyama,Xi-\\nDarioAmodei,andIlyaSutskever.2019. Language aodong Liu, Tristan Naumann, Jianfeng Gao, and\\nmodelsareunsupervisedmultitasklearners. HoifungPoon.2023. Fine-tuninglargeneurallan-\\nguagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe HugoTouvron,ThibautLavril,GautierIzacard,Xavier\\nlimitsoftransferlearningwithaunifiedtext-to-text Martinet,Marie-AnneLachaux,TimothéeLacroix,\\ntransformer. JournalofMachineLearningResearch, BaptisteRozière,NamanGoyal,EricHambro,Faisal\\n21(1). Azhar,AurelienRodriguez,ArmandJoulin,Edouard\\nGrave,andGuillaumeLample.2023. Llama: Open\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nandefficientfoundationlanguagemodels.\\nKnowwhatyoudon’tknow:Unanswerablequestions\\nforsquad. CoRR,abs/1806.03822.\\nAashka Trivedi, Takuma Udagawa, Michele Merler,\\nRameswarPanda,YousefEl-Kurdi,andBishwaran-\\nPranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in\\nMachineComprehensionofText. InEMNLP.\\nlanguagemodels. arXivpreprintarXiv:2303.09639.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nTakumaUdagawa,AashkaTrivedi,MicheleMerler,and\\nBERT:SentenceembeddingsusingSiameseBERT-\\nBishwaranjanBhattacharjee.2023. Acomparative\\nnetworks. InProceedingsofthe2019Conferenceon\\nanalysisoftask-agnosticdistillationmethodsforcom-\\nEmpiricalMethodsinNaturalLanguageProcessing\\npressingtransformerlanguagemodels. InProceed-\\nandthe9thInternationalJointConferenceonNatu-\\ningsofthe2023ConferenceonEmpiricalMethodsin\\nralLanguageProcessing(EMNLP-IJCNLP),pages\\nNaturalLanguageProcessing: IndustryTrack,pages\\n3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria\\nEmilio,CarlesFalcon,SherezadeFuentes,LauraHer- Nicholas Walker, Amalie Trewartha, Haoyan Huo,\\nnandez, Gema Huesa, Jordi Huguet, Paula Marne, SanghoonLee,KevinCruse,JohnDagdelen,Alexan-\\nTaniaMenchón,GrégoryOperto,AlbinaPolo,San- der Dunn, Kristin Persson, Gerbrand Ceder, and\\ndra Pradas, Anna Soteras, Marc Vilanova, and Na- AnubhavJain.2021. Theimpactofdomain-specific\\ntalia Vilor-Tejedor. 2020. Novel tau biomarkers pre-trainingonnamedentityrecognitiontasksinma-\\nphosphorylatedatt181,t217ort231riseintheini- terialsscience. AvailableatSSRN3950755.\\ntial stages of the preclinical alzheimer&#x2019;s\\n<i>continuum</i> when only subtle changes in Liang Wang, Nan Yang, Xiaolong Huang, Binxing\\na&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-\\nsupervisedcontrastivepre-training.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishekSrivastava,andIrynaGurevych.2021. Beir: WenhuiWang,HangboBao,ShaohanHuang,LiDong,\\nAheterogenousbenchmarkforzero-shotevaluation and Furu Wei. 2021. MiniLMv2: Multi-head self-\\nofinformationretrievalmodels. attention relation distillation for compressing pre-\\ntrainedtransformers. InFindingsoftheAssociation\\nJames Thorne, Andreas Vlachos, Christos forComputationalLinguistics: ACL-IJCNLP2021,\\nChristodoulopoulos, and Arpit Mittal. 2018. pages2140–2151,Online.AssociationforComputa-\\nFEVER: a large-scale dataset for fact extraction tionalLinguistics.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and Model Training NASA-IR BEIRAvg.\\nTie-Yan Liu. 2013. A theoretical analysis of ndcg INDUS-RETRIEVERSMALL One-Stage 0.73 0.42\\ntyperankingmeasures. InProceedingsofthe26th INDUS-RETRIEVERSMALL Stagewise 0.72 0.41\\nAnnualConferenceonLearningTheory,volume30\\nofProceedingsofMachineLearningResearch,pages\\nTable9: AblationStudy: EvaluationresultsonNASA-\\n25–54,Princeton,NJ,USA.PMLR. QAandBEIR.NASA-QAshowedRecall10whileBEIR\\nreportednDCG10.\\nShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,\\nMarkDredze,SebastianGehrmann,PrabhanjanKam-\\nbadur, David Rosenberg, and Gideon Mann. 2023. C CompleteResultsonBEIRBenchmark\\nBloomberggpt: Alargelanguagemodelforfinance.\\nTable11showstheper-datasetresultsontheBEIR\\nShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao.\\ntasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.\\nA SentenceEmbeddingTrainingData\\nTable 10 shows the various data sources used for\\ntrainingembeddingmodels. Alldataispresented\\nin the form of text-pairs, where each item in the\\npairmaybeasentenceoraparagraph. Inthetable,\\nDataFormatdenotess2pforsentence-to-paragraph\\nmappings,s2sforsentence-to-sentencemappings,\\nandp2pforparagraph-to-paragraphmappings. We\\nusedabout360millionpairsfortrainingandused\\nin-batchnegatives.\\nB AblationStudy: Stage-wiseDistillation\\nforEmbeddingModel\\nFor the distilled embedding models, we find that\\nstage-wisedistillationdoesnotbenefitperformance\\nasmuchasaone-stepprocess, combiningallthe\\nsupervised and unsupervised data. As shown in\\nTable9,thestage-wiseapproachunderperformed\\ntheone-stageapproachby1percentagepointfor\\nbothNASA-QAandonBEIR.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Dataset Num. Pairs DataCategory DataFormat\\nStackOverflow† 18562443 Title-Body s2p\\nStackExchangeMath† 2201906 Title-Body s2p\\nS2ORC[title-abstract](Loetal.,2020) 41769185 Title-Body s2p\\nS2ORCCitationPairs[Abstracts](Loetal.,2020) 52603982 Title-Body p2p\\nStackExchange[title-body]† 5415570 Title-Body s2p\\nWikipedia(Faderetal.,2014) 6458670 Title-Body s2p\\nArxiv(Clementetal.,2019) 2358545 Title-Body s2p\\nNASAADS[title-abstract](§2) 2633240 Title-Body s2p\\nPubMed[title-abstract](§2) 24001387 Title-Body s2p\\nPMC[title-abstract](§2) 2585537 Title-Body s2p\\nStackExchangeDuplicateQuestions[title-body-title-body]† 250460 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[body-body]† 250519 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[title-title]† 304525 DuplicateQuestions s2s\\nWikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s\\nSQuAD(Rajpurkaretal.,2016) 87599 QuestionAnswers s2p\\nNQ(Kwiatkowskietal.,2019) 100231 QuestionAnswers s2p\\nSearchQA(Dunnetal.,2017) 582261 QuestionAnswers s2p\\nStackExchange[title-answer]† 4067139 QuestionAnswers s2p\\nStackExchange[title-body-answer]† 187195 QuestionAnswers p2p\\nPAQ(Lewisetal.,2021) 64371441 QuestionAnswers s2p\\nFEVER(Thorneetal.,2018)∗ 109810 FactVerification s2p\\nHotpotQA(Yangetal.,2018)∗ 85000 QuestionAnswering s2p\\nTable10:TrainingDataforEmbeddingModels. Thetrainingdatatotalstoaround360Mpairs. DataFormatdenotes\\ns2pforsentence-to-paragraphmappings,s2sforsentence-to-sentencemappings,andp2pforparagraph-to-paragraph\\nmappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval\\nTREC- NFCorpus NQ HotPotQA FiQA ArguaAna Touche DBPedia Scidocs FEVER Climate SciFact AVG.\\nCovid FEVER BEIR\\nRoBERTa\\nBASE\\n0.47 0.30 0.54 0.34 0.38 0.52 0.18 0.25 0.22 0.46 0.14 0.67 0.37\\nBGEBASE 0.78 0.37 0.54 0.73 0.41 0.64 0.26 0.41 0.22 0.86 0.31 0.74 0.52\\nINDUS-RETRIEVERBASE 0.56 0.32 0.54 0.49 0.36 0.54 0.17 0.31 0.21 0.56 0.14 0.74 0.41\\nMINILM-V2 0.47 0.32 0.44 0.47 0.35 0.50 0.17 0.32 0.22 0.52 0.25 0.65 0.39\\nBGESMALL 0.76 0.34 0.50 0.70 0.40 0.60 0.26 0.40 0.21 0.87 0.32 0.71 0.51\\nINDUS-RETRIEVERSMALL 0.55 0.31 0.53 0.48 0.29 0.50 0.21 0.33 0.23 0.61 0.23 0.71 0.42\\nTable11: EvaluationresultsBEIR.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'})]\n"
     ]
    }
   ],
   "source": [
    "# total 12 documents\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6edc065-ec1f-4b2e-a9f8-0a29dbd59052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUS: Effective and Efficient Language Models for Scientific Applications\n",
      "BishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\n",
      "MuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\n",
      "BharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\n",
      "ElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\n",
      "ThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\n",
      "DavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\n",
      "PanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\n",
      "ArminMehrabian9,TsendgarLee7\n",
      "1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\n",
      "7 NASAHQ,8 JPL,9 NASAGSFC\n",
      "Abstract generation tasks. Most popular LLMs rely on the\n",
      "transformer architecture (Vaswani et al., 2017)\n",
      "Largelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\n",
      "eral domain corpora showed remarkable re-\n",
      "like Wikipedia or CommonCrawl (Devlin et al.,\n",
      "sults on natural language processing (NLP)\n",
      "2019; Liu et al., 2019; Lewis et al., 2020; Raffel\n",
      "tasks. However, previous research demon-\n",
      "et al., 2020; Brown et al., 2020; Touvron et al.,\n",
      "stratedLLMstrainedusingdomain-focusedcor-\n",
      "pora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\n",
      "spired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\n",
      "INDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\n",
      "fortheEarthscience,biology,physics,helio- domain-specific natural language understanding\n",
      "physics, planetary sciences and astrophysics\n",
      "and generation tasks (Beltagy et al., 2019). Fol-\n",
      "domains and trained using curated scientific\n",
      "lowing this observation, several domain-specific\n",
      "corporadrawnfromdiversedatasources. The\n",
      "LLMs such as SCIBERT (Beltagy et al., 2019),\n",
      "suiteofmodelsinclude: (1)anencodermodel\n",
      "trainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\n",
      "corpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\n",
      "standingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\n",
      "general text embedding model trained using developedwiththegoalofimprovingaccuracyon\n",
      "a diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\n",
      "ple sources to address information retrieval\n",
      "Wuetal.,2023).\n",
      "tasks and (3) smaller versions of these mod-\n",
      "els created using knowledge distillation tech-\n",
      "Inthisresearch,wespecificallyfocusedoninter-\n",
      "niquestoaddressapplicationswhichhavela-\n",
      "disciplinaryfieldsrelatedtotheEarth,celestialbod-\n",
      "tencyorresourceconstraints. Wealsocreated\n",
      "ies, the Sun, and planets within our solar system\n",
      "threenewscientificbenchmarkdatasetsnamely,\n",
      "suchasphysics,Earthscience,astrophysics,helio-\n",
      "CLIMATE-CHANGE NER (entity-recognition),\n",
      "NASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\n",
      "accelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\n",
      "fields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\n",
      "elsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\n",
      "(RoBERTa) and existing domain-specific en-\n",
      "rentlynospecificmodelavailablethatencompasses\n",
      "coders(SCIBERT)onthesenewtasksaswell\n",
      "allofthefieldsofinterestcollectively. Further,the\n",
      "asexistingbenchmarktasksinthedomainsof\n",
      "interdisciplinarynatureofthesedomainsofinter-\n",
      "interest.\n",
      "estisreflectedinavastbodyofliteraturescattered\n",
      "1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\n",
      "a collection of encoder-based LLMs focused on\n",
      "Large language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\n",
      "amountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\n",
      "pabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\n",
      "zationsandenterprisesworkinginthesefieldsby\n",
      "Contact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\n",
      "muthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\n",
      "4202\n",
      "yaM\n",
      "02\n",
      "]LC.sc[\n",
      "2v52701.5042:viXra\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddea242-5b07-44df-83be-6158eb1b4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b4e216e-69de-42fc-844a-69ece0f9fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200, add_start_index = True)\n",
    "doc_splits = text_splitter.split_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bced56a-84bc-4b38-9211-b466cb2aed40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='INDUS: Effective and Efficient Language Models for Scientific Applications\\nBishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\\nMuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\\nBharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\\nElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\\nThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\\nDavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\\nPanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\\nArminMehrabian9,TsendgarLee7\\n1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\\n7 NASAHQ,8 JPL,9 NASAGSFC\\nAbstract generation tasks. Most popular LLMs rely on the\\ntransformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='transformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-\\nlike Wikipedia or CommonCrawl (Devlin et al.,\\nsults on natural language processing (NLP)\\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\\ntasks. However, previous research demon-\\net al., 2020; Brown et al., 2020; Touvron et al.,\\nstratedLLMstrainedusingdomain-focusedcor-\\npora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\\nspired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\\nINDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\\nfortheEarthscience,biology,physics,helio- domain-specific natural language understanding\\nphysics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 808}), Document(page_content='physics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific\\ncorporadrawnfromdiversedatasources. The\\nLLMs such as SCIBERT (Beltagy et al., 2019),\\nsuiteofmodelsinclude: (1)anencodermodel\\ntrainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\\ncorpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\\nstandingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\\ngeneral text embedding model trained using developedwiththegoalofimprovingaccuracyon\\na diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\\nple sources to address information retrieval\\nWuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1611}), Document(page_content='Wuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-\\ndisciplinaryfieldsrelatedtotheEarth,celestialbod-\\ntencyorresourceconstraints. Wealsocreated\\nies, the Sun, and planets within our solar system\\nthreenewscientificbenchmarkdatasetsnamely,\\nsuchasphysics,Earthscience,astrophysics,helio-\\nCLIMATE-CHANGE NER (entity-recognition),\\nNASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\\naccelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\\nfields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\\nelsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\\n(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2400}), Document(page_content='(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the\\nasexistingbenchmarktasksinthedomainsof\\ninterdisciplinarynatureofthesedomainsofinter-\\ninterest.\\nestisreflectedinavastbodyofliteraturescattered\\n1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\\na collection of encoder-based LLMs focused on\\nLarge language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\\namountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\\npabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\\nzationsandenterprisesworkinginthesefieldsby\\nContact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\\nmuthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\\n4202\\nyaM\\n02\\n]LC.sc[\\n2v52701.5042:viXra', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3209}), Document(page_content='Earth Science Data Pretraining\\n(Masked Finetuning Scientific Corpora\\nBioMedical Data Language Indus-Base (Contrastive\\nModelling) Learning) Open QA\\nAstrophysics Data\\nDuplicate Pairs\\nAstronomy Data\\nRepresentation Indus-Retriever-Base Citation Pairs\\nGeneral Science Distillation\\nData\\nFact Verification\\nGeneral English\\nData\\nGeneric Corpora\\nOutput\\nEncoder Training Corpus Indus-Small Distillation Embedding Training\\nCorpus\\nData Teacher (KD)\\nInitialization Output Indus-Retriever-\\nSmall\\nBC5-CHEM BC5-Disease NCBI-Disease BC2GM\\nNASA-QA TREC-Covid NFCorpus NQ HotpotQA\\nJNLPBA EBM-PICO ChemProt DDI\\nClimate FiQA Arguana Touche DBPedia NASA-IR\\nGAD HoC PubMedQA BioASQ Change NER\\nSciDocs FEVER C Fl Eim VEat Re SciFact\\nBIOSSES BLURB Benchmark BEIR Benchmark\\nNatural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='Natural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir\\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\\nNASA-QA,CLIMATE-CHANGENERandNASA-IR.\\nenablingthemininformeddecision-making. (Beltagy et al., 2019). We also show that the\\nSpecifically, we make the following contribu- knowledge-distilledmodelsachievedasignifi-\\ntions: cantboostinlatencywhilemaintainingstrong\\n1. Utilizing the byte-pair encoding algorithm, empiricalperformancecomparedtotheoriginal\\nwe constructed INDUSBPE, a customized to- modelsonmostofthebenchmarktasks.\\nkenizerfromthecuratedscientificcorpus.\\n2 Data\\n2. Wepretrainedmultipleencoder-onlyLLMsus-\\ning the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 747}), Document(page_content='ing the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than\\nsentence-embeddingmodelsbyfine-tuningthe\\ntheircounterpartstrainedonopen-domaincorpora.\\nencoder-only models with a contrastive learn-\\nWemeticulouslyidentifiedcorporaforeachofthe\\ningobjectivetolearn“universal”sentenceem-\\naforementioneddomains,andcreatedEnglish-only\\nbeddings (Gao et al., 2021) (§4). We also\\nmodels for the sake of containment. Specifically,\\ntrained smaller, more efficient versions of\\nforeachofthedomains,weusedopen-sourcedata\\nthesemodelsusingknowledge-distillationtech-\\nwhich has a permissive license, and further aug-\\nniques(§3.3,§4.2).\\nmentedthemwithspecificdatafromNASAandits\\n3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1540}), Document(page_content='3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in\\nrecognitiontask),NASA-QA(anextractiveques-\\nourtrainingcorpora. Webrieflydescribeeachdata\\ntionansweringtask)and NASA-IR (aretrieval\\nsourcebelow,andpresentstatisticsofthedatain\\ntask)(§5)tofurtheraccelerateresearchinthis\\nTable1.\\nmulti-disciplinaryfield.\\n• SAO/NASAAstrophysicsDataSystem(ADS)1:\\n4. Throughexperimentalresults,weshowstrong\\nADSisthebiggestsourceofdata,coveringpub-\\nperformancebyourmodelsonthesebenchmark\\nlications in the areas of astronomy and astro-\\ntasks as well as on existing domain-specific\\nphysics,physicsandgeneralscienceincluding\\nbenchmarks, outperforming general-purpose\\nallarXive-prints.\\nmodels like RoBERTa (Liu et al., 2019) as\\nwellasscientific-domainencoderslikeSCIBERT 1https://ui.adsabs.harvard.edu', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2318}), Document(page_content='Dataset Domain #Tokens Ratio Tokenizer ADS PMC Wikipedia\\nNASACMR EarthScience 0.3B 1% RoBERTa 12,867,439 7,549,075 15,859\\nAMSandAGUpapers EarthScience 2.8B 4% +lower_cased 12,862,227 7,557,868 16,901\\nEnglishWikipedia General 5.0B 8% INDUSBPE 12,309,023 6,920,659 16,056\\nPubMedAbstracts Biomedical 6.9B 10%\\nTable2: NumberoftokensproducedbyRoBERTaand\\nPMC Biomedical 18.5B 28%\\nINDUSBPEtokenizersappliedto1ksamplesfromeach\\nSAO/NASAADS Astronomy, 32.7B 49%\\ndataset. Fewer tokens lead to a smaller computation\\nAstrophysics,\\ncost.\\nPhysics,\\nGeneralScience\\nTotal 66.2B 100% ingdataset(§2)6. Forafaircomparison,wesetthe\\nvocabularysizeto50,265,whichisequaltothatof\\nTable1: Basicstatisticsofourpretrainingdataset.\\nthe RoBERTatokenizer(Liuetal.,2019)andused\\ntheuncasedvariationofboththetokenizers.\\n• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-\\nliterature maintained by National Library of BPE and the RoBERTa tokenizer. Out of 50,265\\nMedicineandNationalInstitutesofHealth. We tokens,22,355(44.5%)tokensarecommoninboth\\nusedtheportionofPMCthathasacommercial- thetokenizerswhiletheremaining27,910(55.5%)\\nfriendly license, along with the PubMed ab- tokensareincludedonlyineithertokenizer, indi-\\nstractsofallthearticlesinPMC. catingasignificantdistributionalshiftindomain.\\n• American Meteorological Society (AMS)3: Tofurther understandthe effect, weapplied both\\nWeusedfull-textjournaldocumentsspanning RoBERTaandINDUSBPEon1,000randomlysam-\\ntopicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 785}), Document(page_content='topicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts\\noceanography, atmospheric sciences, climate, tosinglesentences. AsshowninTable2, INDUS-\\nhydrometeorology, weather and forecasting, BPE tokenizer produced fewer tokens than the\\nandsocietalimpacts. RoBERTa tokenizer, leading to 8˜% drop in com-\\n• American Geophysical Union (AGU)4: The putationcostduringtraining.\\nAGUdatasetincludedjournaldocumentsacross Table3comparestheRoBERTatokenzierandIN-\\nthe topics of atmospheres, biogeosciences, DUSBPE tokenizer,illustratingthattheproposed\\nEarth surface, machine learning and compu- tokenizertreatedscientificterms(suchasbiomak-\\ntation, oceans, planets, solid earth, and space ers, phosphorylated, alzheimer) as single tokens\\nphysics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1546}), Document(page_content='physics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-\\n3.2 EncoderModel\\nquality,continuouslyevolvingmetadatasystem\\nthat catalogs all data and service metadata Wefirsttrainedanencoder-onlymodel,INDUS ,\\nBASE\\nrecords for NASA’s Earth Science Data and usingamaskedlanguagemodelingobjective. The\\nInformation System (ESDIS). It contains text modelarchitecturefollowsRoBERTa (Liuetal.,\\nBASE\\ndescriptions of the NASA Earth science data 2019),whichconsistsof12layersandhas125M\\nproducts. parameters. Weadoptedthedefaulthyperparame-\\nters7 butwithaneffectivebatchsizeof92,16. We\\n3 Methodology: EncoderModels trainedthemodelfor500Kstepsusing192V100\\nGPUs.\\n3.1 INDUSBPE Tokenizer\\n3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2388}), Document(page_content='3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL\\n2https://www.ncbi.nlm.nih.gov/pmc 38M parameters through knowledge distillation\\n3https://www.ametsoc.org/index.cfm/ams/publications/\\n4https://agupubs.onlinelibrary.wiley.com/ 6We used HF tokenizers, https://github.com/\\n5https://www.earthdata.nasa.gov/eosdis/science-system- huggingface/tokenizers\\ndescription/eosdis-components/cmr 7WereferreaderstoTable9in(Liuetal.,2019).', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3153}), Document(page_content='Inputtext\\nnoveltaubiomarkersphosphorylatedatt181,t217ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected\\nTokenizationbyRoBERTa\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTokenizationby INDUSBPE\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTable3: TokenizationcomparisonbetweenRoBERTaandourtokenizers. InputtextadaptedfromSuárez-Calvet\\netal.(2020).\\n(cid:88) (cid:88)\\ntechniquesbyusingINDUS\\nBASE\\nastheteacher. IN- Z\\ni\\n= es(qi,pj)+ es(qj,pi)\\nDUS follows a 4-layer architecture recom- j j\\nSMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='SMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the\\nwheres(q,p)isameasureoftemperature-scaled\\ndistillationobjectiveproposedinMiniLMv2(Wang\\ncosinesimilaritybetweentheembeddingsofquery\\netal.,2021)totransferfine-grainedself-attention\\nandapassagemeasuredby:\\nrelations,whichhasbeenshowntobethecurrent\\nstate-of-the-art(Udagawaetal.,2023). Usingthis 1 E(q)·E(p)\\ns(q,p) = (3)\\nobjective,wetrainedthemodelfor500Kstepswith τ ∥E(q)∥∥E(p)∥\\naneffectivebatchsizeof480on30V100GPUs.\\nTrainingData Similartopriorwork(Wangetal.,\\n2022; Li et al., 2023; Xiao et al., 2023), we em-\\n4 Methodology: SentenceEmbedding\\nployedastage-wisetrainingapproachforoursen-\\nModels\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 777}), Document(page_content='Models\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional\\noccurringpairscollectedfrominternetsources,\\nvectors,allowingforefficientuseindenseretrieval\\nsuch as Wikipedia, StackExchange, etc. We\\nsystems,whererelevantpassagesforagivenquery\\nalsoincludedscientificdatafromPubMed,PMC\\nareidentifiedonthebasisofthesimilaritybetween\\n(§2),Arxivand S2ORC (Loetal.,2020)asin-\\ntheirembeddings(Karpukhinetal.,2020).\\ndomaindataforourscience-orientedretriever\\nmodel. Furthermore, we created a domain-\\nContrastiveLearningObjective Sentenceem-\\nspecific dataset from the ADS data (§2) by in-\\nbeddingmodelstrainedusingacontrastivelearn-\\ncludingtitle-abstractpairs.\\ningobjective(Khoslaetal.,2020;Gaoetal.,2021)\\n2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1553}), Document(page_content='2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa\\n(Kwiatkowskietal.,2019),SQuAD(Rajpurkar\\nnon-relevantpassage.\\net al., 2016), SPECTER pairs (Cohan et al.,\\nInspired by recent work (Li et al., 2023), we\\n2020), etc. We included the aforementioned\\nusedanimprovedcontrastivelossbyintroducing\\nADS data and a sample of the S2ORC data in\\nanadditionalbidirectionalsignal. Specifically,for\\nthisstep,toboostdomain-specificsignals.\\na triple {q,p+,P−} of a query, a relevant (posi-\\nAppendixAcontainscomprehensivedetailsabout\\ntive)passage,andasetofnon-relevant(negative)\\nthe datasets used in training. For both training\\npassagesP− = {p−}m ,WedefinetheInfoNCE\\nj j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2374}), Document(page_content='j j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches\\n1 (cid:88)n es(qi,p+ i ) from each data source proportionately to its size,\\nL = − log (1)\\nIC\\nn Z i similartoLietal.(2023).\\ni=1', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3146}), Document(page_content='Model Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='extendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K\\ninthelargermodelbutimprovestheperformance\\nstepsfollowedbyanadditional100Kstepswiththe\\nofthesmallermodel.\\nsuperviseddata. Weusedalearningrateof2e−5\\nFor distilling the sentence embedding model,\\nduringboththesesteps.\\nwefoundthatastage-wisetrainingapproachdoes\\n4.2 KnowledgeDistillationforEmbedding not benefit performance as much as in the non-\\nModels distillation case (ablation presented in Appendix\\nB). We thus distilled in a single step with all the\\nTooptimizethelatencyforretrievalapplications,\\ndatadescribedin§4.1andAppendixAandadded\\nwe also created a small retriever model with the\\nlabelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 811}), Document(page_content='labelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE\\nmodel(INDUS SMALL),bydistillingtheteacher’sdis- ModelSpecifications Webuiltthesentenceem-\\ntributionofsimilarityscores. Furthermore,wefind beddingmodelbydistillinginto INDUS . This\\nSMALL\\nthatitisnecessarytomodifythetrainingstrategy isa4-layermodelwithanembeddingdimension\\nfordistillation,asdescribedbelow. of576. Werefertotheresultingretrievermodelas\\nDistillationLoss Weusedknowledgedistillation\\nINDUS-RETRIEVER SMALL. Itfollowsabi-encoder\\nframework,andherewefindthatusingthevector\\ntechniquesintroducedin(Xuetal.,2023). Specif-\\nrepresentationofthefirsttokenastheembedding\\nically, for a sentence x and its corresponding in-\\ni\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1573}), Document(page_content='i\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-\\ntionp ofsimilarityscoresbetweenpairsandthe\\nt TrainingDetails FortheRetro-MAEstylepre-\\nstudent’sdistribution, p . FollowingHintonetal.\\ns training(Xiaoetal.,2022),wetrainedon8A100\\n(2014), we also scaled the output distribution of\\nGPUs with an effective batch size of 128 for 2\\nbothteacherandstudentbyatemperature,τ :\\nKD epochswithalearningrateof2e−5. Forthestage-\\nn m wisedistillation,wetrainedon2A100GPUsfor\\n(cid:88)(cid:88)\\nL = − p (x ,x )logp (x ,x ) (4) 300K steps with an effective batch size of 2,048,\\nKD t i j s i j\\ni=1 j=1 andlearningrateof7e−4. Throughexperimenta-\\ntion,Wefoundthatτ = 4performedthebest.\\nKD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2369}), Document(page_content='KD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)\\nt i j (cid:80)m est(xi,x k)/τKD ingthelanguageunderstandingcapabilitiesmodels.\\nk=1\\nHowever,tothebestourknowledge,thereisano-\\nHere,s (x ,x )ands (x ,x )representthesim-\\ns i j t i j\\nticeableabsenceofdatasetstailoredforthediverse\\nilarityscoresbetweentwopairs{x ,x },definedin\\ni j\\nand multidisciplinary field under study. Thus, to\\nEquation3forthestudentandteacherrespectively.\\neffectively benchmark the proposed NLP models\\nTrainingData Wefirstconductedaembedding- and further accelerate research in this multidisci-\\noriented pretraining step, as presented in Retro- plinarydomain,Weintroducedthreenewdatasets,\\nMAE (Xiao et al., 2022), on English Wikipedia, anNERtask,aQAtask,andanIRtask,described\\nBooksCorpus,andStackExchangedata,totalling below.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3142}), Document(page_content='Train Validation Test spansoftheparagraphwhichanswerthequestions.\\nNum.Abstracts 382 77 75\\nWeused29paragraphs(with145QApairsintotal)\\nNum.Tokens 32,031 6,443 5,850\\nEntityLabels asthetrainingsetandtheremaining10paragraphs\\nclimate-nature,climate-greenhouse-gases,climate-assets,\\n(with 50 questions in total) as the evaluation set.\\nclimate-problem-origins,climate-mitigations,\\nclimate-properties,climate-impacts,climate-datasets, Thetrainingsetwasfurtheraugmentedwithpara-\\nclimate-organizations,climate-observations,\\ngraphsand QA pairsrelatedtoEarthsciencefrom\\nclimate-models,climate-hazards,climate-organisms\\nthe SQuADdataset(Rajpurkaretal.,2018). Specif-\\nTable4: CLIMATE-CHANGE NER statisticsandentity ically,thoserelatedtooxygen,Amazonrainforest\\nlabels\\nandgeologywereused. Thisresultedinapruned\\nSQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='SQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-\\nby augmenting these SQuAD pairs to the training\\nfer some assistance in exploring data related to\\ndatasourcedfromEarthsciencepapers,whilekeep-\\nclimatechange,thecomplexityofclimate-related\\ningtheevaluationsetintact.\\nqueries often requires more sophisticated natural\\nlanguageprocessingtechniques. Thisnecessityis 5.3 NASA-IR\\nunderscoredbytheextensivearrayofclimatemod-\\nWe introduced a domain-specific information re-\\nels, datasets, and organizations involved, which\\ntrieval benchmark, NASA-IR10, spanning almost\\ndemand meticulous curation and continuous up-\\n500question-answerpairsrelatedtotheEarthsci-\\ndates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 793}), Document(page_content='dates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-\\ndata,comprehensiveoverviewsofclimatemodels\\nically, we sampled a set of 166 paragraphs from\\nandimpactassessmentsarescarceandnoteasily\\nAGU, AMS, ADS, PMC and Pubmed (§2) and\\naccessible.\\nmanually annotated with 3 questions that are an-\\nInordertobridgethisgap,weintroducedacom-\\nswerablefromeachoftheseparagraphs,resulting\\nprehensive dataset for developing and evaluating\\nin498questions. Weused398ofthesequestions\\nNLP models tailored towards understanding and\\nas the training set and the remaining 100 as the\\naddressing climate-related topics across various\\nvalidation set. To comprehensively evaluate the\\ndomains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1589}), Document(page_content='domains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-\\nwhichthenamedentitiesofinterestoriginatefrom\\nstracts with these annotated paragraphs. On an\\ncomplextaxonomiesusedinclimate-relatedlitera-\\naverage, each query is 12 words long, and each\\nture. Thisdatasetcomprises534abstractssourced\\nparagraphis120wordslong. WeusedRecall@10\\nfromSemanticScholarAcademicGraph(Kinney\\nasevaluationmetricsinceeachquestionhasonly\\netal.,2023),collectedusingaseedsetofclimate-\\nonerelevantdocument.\\nrelated keywords such as wildfire or floods. The\\nabstractswereannotatedusingtheIOB(inside,out-\\n6 ExperimentalResults\\nside,beginning)taggingschemeandencompasses\\nadiversearrayofentitytypes,asshowninTable4. Baselines Wecompared INDUS modelsagainst\\nopensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2378}), Document(page_content='opensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.\\nsweringtaskfocusedontheEarthsciencedomain. • INDUS was compared to MINILM (6-\\nSMALL\\nFirst, 39 paragraphs from Earth science papers\\nlayer)13 andTINYBERT(4-layer)14.\\nwhichappearedinAGUandAMSjournals(§2)were\\n10https://huggingface.co/datasets/nasa-impact/nasa-smd-\\nsourced. Subject matter experts from NASA for- IR-benchmark\\nmulatedquestionsandmarkedthecorresponding 11https://huggingface.co/FacebookAI/roberta-base\\n12https://huggingface.co/allenai/scibert_scivocab_uncased\\n8https://huggingface.co/datasets/ibm/Climate-Change- 13https://huggingface.co/nreimers/MiniLM-L6-H384-\\nNER uncased\\n9https://huggingface.co/datasets/nasa-impact/nasa-smd- 14https://huggingface.co/huawei-\\nqa-benchmark noah/TinyBERT_General_4L_312D', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3221}), Document(page_content='Basemodel(125Mparams.) Smallmodel(∼30Mparams.)\\nTask Metric Dataset RoBERTa SCIBERT INDUSBASE TINYBERT MINILM INDUSSMALL\\nBC5-chem 90.3(0.2) 91.4(0.2) 93.3(0.2) 84.6(0.2) 86.1(0.3) 90.7(0.1)\\nBC5-disease 81.5(0.3) 83.7(0.3) 85.2(0.3) 74.0(0.4) 77.4(0.3) 81.3(0.3)\\nNER EntityF1 NCBI-disease 87.6(0.6) 87.6(0.4) 88.3(0.4) 81.2(0.4) 83.1(0.5) 85.6(0.6)\\nBC2GM 82.1(0.3) 82.3(0.2) 84.0(0.3) 74.7(0.4) 77.1(0.2) 79.7(0.3)\\nJNLPBA 79.1(0.2) 78.2(0.2) 80.3(0.2) 70.3(0.2) 73.4(0.3) 75.7(0.2)\\nPICO MacroF1 EBMPICO 72.3(0.3) 72.4(0.3) 73.1(0.2) 67.4(0.2) 70.3(0.1) 73.1(0.2)\\nChemProt 50.4(28.2) 73.9(0.7) 76.9(0.5) 56.2(3.2) 55.9(2.1) 71.7(0.9)\\nRelation\\nMicroF1 DDI 78.6(1.5) 80.1(1.0) 81.7(0.5) 39.3(5.3) 51.5(2.9) 69.0(1.2)\\nExtraction\\nGAD 80.0(1.1) 81.6(1.2) 79.4(5.6) 76.4(1.3) 77.3(1.0) 81.3(0.7)\\nDocument\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='Document\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy\\nAnswering BioASQ 69.1(4.8) 74.6(4.5) 69.6(5.8) 74.3(3.6) 66.7(2.3) 75.4(3.3)\\nSentence\\nSimilarity Pearson BIOSSES 79.8(6.3) 86.3(3.5) 72.2(9.5) 88.2(1.1) 26.6(8.7) 70.4(3.3)\\nMicroAverage - - 75.9(3.7) 79.2(1.3) 78.9(2.4) 67.6(1.9) 66.1(1.9) 76.2(1.0)\\nMacroAverage - - 74.9(3.7) 78.2(1.6) 76.4(3.2) 65.6(2.4) 60.6(3.0) 74.3(1.3)\\nTable5:EvaluationresultsonBLURB.Resultsreportedareaveragedon10randomseedswithstandarddeviationin\\nparenthesis. Microaverageisreportedacrossdatasetswhilemacroaverageiscomputedbyfirstaveragingscoreson\\neachtask(say,taskaverage),followedbyaveragingthetaskaverageacrosstasks. Resultsinboldindicatehighest\\nperformancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 787}), Document(page_content='performancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE\\nBGE 15 andaRoBERTa modelfinetuned RoBERTa 60.8(0.8)\\nBASE BASE\\nwiththesamemethodpresentedin§4.1. SCIBERT 61.8(0.7)\\n• INDUS-RETRIEVER\\nSMALL\\nwas compared to INDUS\\nBASE\\n64.0(1.0)\\nMINILM-V216 and BGE 17. TINYBERT 34.3(1.6)\\nSMALL\\nMINILM 44.7(1.3)\\n6.1 NaturalLanguageUnderstanding INDUS 54.8(0.8)\\nSMALL\\nBenchmarks\\nTable 6: CLIMATE-CHANGE NER benchmark results.\\nWe evaluated our models on BLURB (Gu et al.,\\nStandard deviation over 10 random seeds shown in\\n2021),abenchmarksuitefornaturallanguageun-\\nparenthesis. Resultsinboldandunderlineindicatehigh-\\nderstandingandreasoningtasksinthebiomedical estperformanceandsignificantdifferencefromsecond\\ndomain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1596}), Document(page_content='domain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.\\nTable 5 shows the evaluation results. Among\\nbase models, INDUS BASE significantly outper- WealsonoticedSCIBERTtendstoperformbetter\\nformsthegeneral-purposeRoBERTamodelonmi-\\nthan our model on paired input-text tasks, such\\ncro/macroaveragewhileachievingcompetitiveper-\\nas QA and semantic similarity tasks, although\\nformance to the bio-domain-specific counterpart,\\nthe results have relatively large standard devia-\\nSCIBERT.\\ntions. We hypothesized that the additional next\\nAsforsmallermodels,wenoticed INDUS SMALL sentence prediction objective during training in\\noutperformed the baselines, TINYBERT and BERT-stylemodels(suchasSCIBERT)incontrastto\\nMINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2418}), Document(page_content='MINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text\\ninNER,PICO,relationextraction,anddocument\\ntasks. Thistrendwasconsistentwiththeobserva-\\nclassification tasks. This demonstrates the effec-\\ntionsofTinnetal.(2023).\\ntivenessofknowledgedistillationfromourdomain-\\nspecificteachermodel,INDUS . 6.2 CLIMATE-CHANGE NER\\nBASE\\nAs shown in Table 6, our models clearly outper-\\n15https://huggingface.co/BAAI/bge-base-en-v1.5\\n16sentence-transformers/all-MiniLM-L6-v2 formedthecorrespondingbaselinemodelsonthe\\n17https://huggingface.co/BAAI/bge-small-en-v1.5 CLIMATE-CHANGE NERtask,suggestingtheeffec-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3215}), Document(page_content='tivenessoftrainingonlargedomain-specificdata. Model NASA-IR↑ BEIRAvg.↑ Retrieval\\nTime↓\\n6.3 NASA-QA BRo GB EE BAR ST Ea BASE 0 0. .6 66\\n7\\n0 0. .3 57\\n2\\n1 1. .2 10\\n8\\nAs mentioned in §5, we augmented the training\\nINDUS-RETRIEVERBASE 0.71 0.41 1.19\\nMINILM-V2 0.62 0.39 0.24\\nsetwithrelevantSQuADpairsforfine-tuning. All BGESMALL 0.66 0.51 0.42\\nmodelsarefinetunedfor15epochs,andtheresults INDUS-RETRIEVERSMALL 0.73 0.42 0.26\\nareshowninTable7. WeobservedthatINDUS\\nBASE Table 8: Evaluation results on NASA-IR and BEIR.\\noutperformedallmodelsofsimilarsizes,while IN-\\nNASA-IRshowedRecall@10whileBEIRreportedthe\\nDUS hadrelativelystrongperformancecom- averagenDCG@10acrossalltasks. Retrievaltimeper\\nSMALL\\nparedtoitscounterparts. queryontheNQtaskfromBEIR,reportedinseconds.\\nModel F1(SD)\\n7 Conclusions\\nRoBERTa 66.8(3.1)\\nSCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='SCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-\\nMINILM 59.2(3.9) enizer and in-domain data for training high qual-\\nINDUS 47.4(1.8) ityencodermodelsandsentenceembeddingmod-\\nSMALL\\nels. Further, we created smaller versions of the\\nTable7: NASA-QA benchmarkresults. Standarddevi- proposedmodelssuitableforapplicationswithla-\\nation over 3 random seeds shown in parenthesis. Re-\\ntencyorresourceconstraintsthroughstate-of-the-\\nsultsinboldandunderlineindicatehighestperformance\\nartknowledgedistillationtechniques. Fortheben-\\nandsignificantdifferencefromsecondhighestresultby\\nefit of the scientific community, we will release\\nmorethantwostandarddeviationsineachmodelsize,\\nthedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 804}), Document(page_content='thedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.\\nDoguAraci.2019. Finbert: Financialsentimentanaly-\\nsiswithpre-trainedlanguagemodels.\\n6.4 InformationRetrievalBenchmarks\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nWeevaluatedourmodelsonthe NASA-IR dataset\\nERT:Apretrainedlanguagemodelforscientifictext.\\naswellasBEIRBenchmark(Thakuretal.,2021), InProceedingsofthe2019ConferenceonEmpirical\\nwhichconsistsof12retrievaltasksspanningavari- Methods in Natural Language Processing and the\\n9thInternationalJointConferenceonNaturalLan-\\netyofdomains. TheBEIRbenchmarkusedtheNor-\\nguageProcessing(EMNLP-IJCNLP),pages3615–\\nmalized Cumulative Discount Gain (nDCG@10)\\n3620,HongKong,China.AssociationforComputa-\\n(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1610}), Document(page_content='(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\ntenceembeddingmodels,alongwithourbaselines.\\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\\nAs shown, both of our sentence embedding mod- Neelakantan,PranavShyam,GirishSastry,Amanda\\nelssignificantlyoutperformedthebaselinesonthe Askell, Sandhini Agarwal, Ariel Herbert-Voss,\\nNASA-IRtaskwhilestillmaintaininggoodperfor- Gretchen Krueger, Tom Henighan, Rewon Child,\\nAdityaRamesh,DanielZiegler,JeffreyWu,Clemens\\nmanceonseveraloftheBEIRtasks. (Wepresented\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nresultsforeachBEIRtaskinAppendixC).\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nWe also measured the average time per query Clark, ChristopherBerner, SamMcCandlish, Alec\\nforretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2447}), Document(page_content='forretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-\\nquestionssetofBEIR,onasingleA100GPU.This\\nvances in Neural Information Processing Systems,\\ntime includes the time to encode the query, cor-\\nvolume 33, pages 1877–1901. Curran Associates,\\npus,andtimetoretrieverelevantdocuments. No- Inc.\\ntably, INDUS-RETRIEVER outperformed IN-\\nSMALL Colin B. Clement, Matthew Bierbaum, Kevin P.\\nDUS-RETRIEVER BASE,onbothNASA-IRandBEIR,\\nO’Keeffe, and Alexander A. Alemi. 2019. On the\\nwhilebeingabout4.6xfaster. useofarxivasadataset.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3276}), Document(page_content='Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Sarna, Yonglong Tian, Phillip Isola, Aaron\\nDowney, and Daniel S. Weld. 2020. SPECTER: Maschinot, CeLiu, andDilipKrishnan.2020. Su-\\nDocument-level Representation Learning using pervisedcontrastivelearning. InAdvancesinNeural\\nCitation-informedTransformers. InACL. InformationProcessingSystems,volume33,pages\\n18661–18673.CurranAssociates,Inc.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of Rodney Kinney, Chloe Anastasiades, Russell Authur,\\ndeepbidirectionaltransformersforlanguageunder- Iz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\\nstanding. InProceedingsofthe2019Conferenceof ski,IsabelCachola,StefanCandra,YoganandChan-\\ntheNorthAmericanChapteroftheAssociationfor drasekhar, Arman Cohan, Miles Crawford, Doug\\nComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='ComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-\\n4171–4186,Minneapolis,Minnesota.Associationfor ham, Fangzhou Hu, Regan Huff, Daniel King, Se-\\nComputationalLinguistics. bastianKohlmeier,BaileyKuehl,MichaelLangan,\\nDaniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\\nMatthewDunn,LeventSagun,MikeHiggins,V.Ugur Kelsey MacMillan, Tyler Murray, Chris Newell,\\nGuney, Volkan Cirik, and Kyunghyun Cho. 2017. Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\\nSearchqa: Anewq&adatasetaugmentedwithcon- Shen,AmanpreetSingh,LucaSoldaini,Shivashankar\\ntextfromasearchengine. Subramanian,AmberTanaka,AlexD.Wade,Linda\\nWagner, Lucy Lu Wang, Chris Wilhelm, Caroline\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 804}), Document(page_content='Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic\\ntractedknowledgebases. InProceedingsofthe20th scholaropendataplatform.\\nACMSIGKDDInternationalConferenceonKnowl-\\nedge Discovery and Data Mining, KDD ’14, page TomKwiatkowski, JennimariaPalomaki, OliviaRed-\\n1156–1165, New York, NY, USA. Association for field,MichaelCollins,AnkurParikh,ChrisAlberti,\\nComputingMachinery. DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\\ntonLee,KristinaToutanova,LlionJones,Matthew\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nSimCSE:Simplecontrastivelearningofsentenceem- Uszkoreit,QuocLe,andSlavPetrov.2019. Natural\\nbeddings. In Proceedings of the 2021 Conference Questions: A Benchmark for Question Answering\\nonEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1574}), Document(page_content='onEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-\\nJinhyukLee,WonjinYoon,SungdongKim,Donghyeon\\nminican Republic. Association for Computational\\nKim,SunkyuKim,ChanHoSo,andJaewooKang.\\nLinguistics.\\n2019. BioBERT:apre-trainedbiomedicallanguage\\nrepresentation model for biomedical text mining.\\nYuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto\\nBioinformatics,36(4):1234–1240.\\nUsuyama,XiaodongLiu,TristanNaumann,Jianfeng\\nGao,andHoifungPoon.2021. Domain-specificlan-\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nguagemodelpretrainingforbiomedicalnaturallan-\\nGhazvininejad,AbdelrahmanMohamed,OmerLevy,\\nguageprocessing. ACMTrans.Comput.Healthcare,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\n3(1).\\nBART:Denoisingsequence-to-sequencepre-training\\nfornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2447}), Document(page_content='fornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In\\ningoftheAssociationforComputationalLinguistics,\\nNeurIPSDeepLearningWorksop.\\npages7871–7880,Online.AssociationforComputa-\\ntionalLinguistics.\\nZhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon\\nDuede,KyleChard,andIanFoster.2023. Thedimin- PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-\\nishingreturnsofmaskedlanguagemodelstoscience. ervini,HeinrichKüttler,AleksandraPiktus,Pontus\\nStenetorp,andSebastianRiedel.2021. PAQ:65Mil-\\nShuHuangandJacquelineMCole.2022. Batterybert:\\nlionProbably-AskedQuestionsandWhatYouCan\\nA pretrained language model for battery database\\nDoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3244}), Document(page_content='DoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,\\nVladimirKarpukhin,BarlasOguz,SewonMin,Patrick Pengjun Xie, and Meishan Zhang. 2023. Towards\\nLewis,LedellWu,SergeyEdunov,DanqiChen,and generaltextembeddingswithmulti-stagecontrastive\\nWen-tauYih.2020. Densepassageretrievalforopen- learning.\\ndomainquestionanswering. InProceedingsofthe\\n2020ConferenceonEmpiricalMethodsinNatural YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-\\nLanguageProcessing(EMNLP),pages6769–6781, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nOnline.AssociationforComputationalLinguistics. Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron proach. arXivpreprintarXiv:1907.11692.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3997}), Document(page_content='KyleLo,LucyLuWang,MarkNeumann,RodneyKin- and VERification. In Proceedings of the 2018\\nney,andDanielWeld.2020. S2ORC:Thesemantic Conference of the North American Chapter of\\nscholaropenresearchcorpus. InProceedingsofthe the Association for Computational Linguistics:\\n58thAnnualMeetingoftheAssociationforCompu- Human Language Technologies, Volume 1 (Long\\ntationalLinguistics,pages4969–4983,Online.Asso- Papers), pages 809–819, New Orleans, Louisiana.\\nciationforComputationalLinguistics. AssociationforComputationalLinguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, RobertTinn,HaoCheng,YuGu,NaotoUsuyama,Xi-\\nDarioAmodei,andIlyaSutskever.2019. Language aodong Liu, Tristan Naumann, Jianfeng Gao, and\\nmodelsareunsupervisedmultitasklearners. HoifungPoon.2023. Fine-tuninglargeneurallan-\\nguagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='guagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe HugoTouvron,ThibautLavril,GautierIzacard,Xavier\\nlimitsoftransferlearningwithaunifiedtext-to-text Martinet,Marie-AnneLachaux,TimothéeLacroix,\\ntransformer. JournalofMachineLearningResearch, BaptisteRozière,NamanGoyal,EricHambro,Faisal\\n21(1). Azhar,AurelienRodriguez,ArmandJoulin,Edouard\\nGrave,andGuillaumeLample.2023. Llama: Open\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nandefficientfoundationlanguagemodels.\\nKnowwhatyoudon’tknow:Unanswerablequestions\\nforsquad. CoRR,abs/1806.03822.\\nAashka Trivedi, Takuma Udagawa, Michele Merler,\\nRameswarPanda,YousefEl-Kurdi,andBishwaran-\\nPranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 790}), Document(page_content='PranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in\\nMachineComprehensionofText. InEMNLP.\\nlanguagemodels. arXivpreprintarXiv:2303.09639.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nTakumaUdagawa,AashkaTrivedi,MicheleMerler,and\\nBERT:SentenceembeddingsusingSiameseBERT-\\nBishwaranjanBhattacharjee.2023. Acomparative\\nnetworks. InProceedingsofthe2019Conferenceon\\nanalysisoftask-agnosticdistillationmethodsforcom-\\nEmpiricalMethodsinNaturalLanguageProcessing\\npressingtransformerlanguagemodels. InProceed-\\nandthe9thInternationalJointConferenceonNatu-\\ningsofthe2023ConferenceonEmpiricalMethodsin\\nralLanguageProcessing(EMNLP-IJCNLP),pages\\nNaturalLanguageProcessing: IndustryTrack,pages\\n3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1581}), Document(page_content='3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2401}), Document(page_content='Beteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria\\nEmilio,CarlesFalcon,SherezadeFuentes,LauraHer- Nicholas Walker, Amalie Trewartha, Haoyan Huo,\\nnandez, Gema Huesa, Jordi Huguet, Paula Marne, SanghoonLee,KevinCruse,JohnDagdelen,Alexan-\\nTaniaMenchón,GrégoryOperto,AlbinaPolo,San- der Dunn, Kristin Persson, Gerbrand Ceder, and\\ndra Pradas, Anna Soteras, Marc Vilanova, and Na- AnubhavJain.2021. Theimpactofdomain-specific\\ntalia Vilor-Tejedor. 2020. Novel tau biomarkers pre-trainingonnamedentityrecognitiontasksinma-\\nphosphorylatedatt181,t217ort231riseintheini- terialsscience. AvailableatSSRN3950755.\\ntial stages of the preclinical alzheimer&#x2019;s\\n<i>continuum</i> when only subtle changes in Liang Wang, Nan Yang, Xiaolong Huang, Binxing\\na&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3230}), Document(page_content='a&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-\\nsupervisedcontrastivepre-training.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishekSrivastava,andIrynaGurevych.2021. Beir: WenhuiWang,HangboBao,ShaohanHuang,LiDong,\\nAheterogenousbenchmarkforzero-shotevaluation and Furu Wei. 2021. MiniLMv2: Multi-head self-\\nofinformationretrievalmodels. attention relation distillation for compressing pre-\\ntrainedtransformers. InFindingsoftheAssociation\\nJames Thorne, Andreas Vlachos, Christos forComputationalLinguistics: ACL-IJCNLP2021,\\nChristodoulopoulos, and Arpit Mittal. 2018. pages2140–2151,Online.AssociationforComputa-\\nFEVER: a large-scale dataset for fact extraction tionalLinguistics.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 4056}), Document(page_content='Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and Model Training NASA-IR BEIRAvg.\\nTie-Yan Liu. 2013. A theoretical analysis of ndcg INDUS-RETRIEVERSMALL One-Stage 0.73 0.42\\ntyperankingmeasures. InProceedingsofthe26th INDUS-RETRIEVERSMALL Stagewise 0.72 0.41\\nAnnualConferenceonLearningTheory,volume30\\nofProceedingsofMachineLearningResearch,pages\\nTable9: AblationStudy: EvaluationresultsonNASA-\\n25–54,Princeton,NJ,USA.PMLR. QAandBEIR.NASA-QAshowedRecall10whileBEIR\\nreportednDCG10.\\nShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,\\nMarkDredze,SebastianGehrmann,PrabhanjanKam-\\nbadur, David Rosenberg, and Gideon Mann. 2023. C CompleteResultsonBEIRBenchmark\\nBloomberggpt: Alargelanguagemodelforfinance.\\nTable11showstheper-datasetresultsontheBEIR\\nShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao.\\ntasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='tasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 779}), Document(page_content='In Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.\\nA SentenceEmbeddingTrainingData\\nTable 10 shows the various data sources used for\\ntrainingembeddingmodels. Alldataispresented\\nin the form of text-pairs, where each item in the\\npairmaybeasentenceoraparagraph. Inthetable,\\nDataFormatdenotess2pforsentence-to-paragraph\\nmappings,s2sforsentence-to-sentencemappings,\\nandp2pforparagraph-to-paragraphmappings. We\\nusedabout360millionpairsfortrainingandused\\nin-batchnegatives.\\nB AblationStudy: Stage-wiseDistillation\\nforEmbeddingModel\\nFor the distilled embedding models, we find that\\nstage-wisedistillationdoesnotbenefitperformance\\nasmuchasaone-stepprocess, combiningallthe\\nsupervised and unsupervised data. As shown in\\nTable9,thestage-wiseapproachunderperformed\\ntheone-stageapproachby1percentagepointfor\\nbothNASA-QAandonBEIR.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1608}), Document(page_content='Dataset Num. Pairs DataCategory DataFormat\\nStackOverflow† 18562443 Title-Body s2p\\nStackExchangeMath† 2201906 Title-Body s2p\\nS2ORC[title-abstract](Loetal.,2020) 41769185 Title-Body s2p\\nS2ORCCitationPairs[Abstracts](Loetal.,2020) 52603982 Title-Body p2p\\nStackExchange[title-body]† 5415570 Title-Body s2p\\nWikipedia(Faderetal.,2014) 6458670 Title-Body s2p\\nArxiv(Clementetal.,2019) 2358545 Title-Body s2p\\nNASAADS[title-abstract](§2) 2633240 Title-Body s2p\\nPubMed[title-abstract](§2) 24001387 Title-Body s2p\\nPMC[title-abstract](§2) 2585537 Title-Body s2p\\nStackExchangeDuplicateQuestions[title-body-title-body]† 250460 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[body-body]† 250519 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[title-title]† 304525 DuplicateQuestions s2s\\nWikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='WikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s\\nSQuAD(Rajpurkaretal.,2016) 87599 QuestionAnswers s2p\\nNQ(Kwiatkowskietal.,2019) 100231 QuestionAnswers s2p\\nSearchQA(Dunnetal.,2017) 582261 QuestionAnswers s2p\\nStackExchange[title-answer]† 4067139 QuestionAnswers s2p\\nStackExchange[title-body-answer]† 187195 QuestionAnswers p2p\\nPAQ(Lewisetal.,2021) 64371441 QuestionAnswers s2p\\nFEVER(Thorneetal.,2018)∗ 109810 FactVerification s2p\\nHotpotQA(Yangetal.,2018)∗ 85000 QuestionAnswering s2p\\nTable10:TrainingDataforEmbeddingModels. Thetrainingdatatotalstoaround360Mpairs. DataFormatdenotes\\ns2pforsentence-to-paragraphmappings,s2sforsentence-to-sentencemappings,andp2pforparagraph-to-paragraph\\nmappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 785}), Document(page_content='mappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval\\nTREC- NFCorpus NQ HotPotQA FiQA ArguaAna Touche DBPedia Scidocs FEVER Climate SciFact AVG.\\nCovid FEVER BEIR\\nRoBERTa\\nBASE\\n0.47 0.30 0.54 0.34 0.38 0.52 0.18 0.25 0.22 0.46 0.14 0.67 0.37\\nBGEBASE 0.78 0.37 0.54 0.73 0.41 0.64 0.26 0.41 0.22 0.86 0.31 0.74 0.52\\nINDUS-RETRIEVERBASE 0.56 0.32 0.54 0.49 0.36 0.54 0.17 0.31 0.21 0.56 0.14 0.74 0.41\\nMINILM-V2 0.47 0.32 0.44 0.47 0.35 0.50 0.17 0.32 0.22 0.52 0.25 0.65 0.39\\nBGESMALL 0.76 0.34 0.50 0.70 0.40 0.60 0.26 0.40 0.21 0.87 0.32 0.71 0.51\\nINDUS-RETRIEVERSMALL 0.55 0.31 0.53 0.48 0.29 0.50 0.21 0.33 0.23 0.61 0.23 0.71 0.42\\nTable11: EvaluationresultsBEIR.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1607})]\n"
     ]
    }
   ],
   "source": [
    "# total 57 chunks\n",
    "print(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c57698-c4fa-48a0-b435-547ed5bcc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=doc_splits, embedding=CohereEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb1d1d91-b3f5-45b1-b470-024183915e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatCohere(model='command-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abef0bb1-6499-4576-9d17-a3ca04ab90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6114a8aa-e5f7-4647-a3d2-82a4686071d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f908a3ae-2bfd-4376-a34d-6a3cf5a08f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INDUS: Effective and Efficient Language Models for Scientific Applications\\nBishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\\nMuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\\nBharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\\nElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\\nThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\\nDavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\\nPanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\\nArminMehrabian9,TsendgarLee7\\n1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\\n7 NASAHQ,8 JPL,9 NASAGSFC\\nAbstract generation tasks. Most popular LLMs rely on the\\ntransformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-\\nlike Wikipedia or CommonCrawl (Devlin et al.,\\nsults on natural language processing (NLP)\\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\\ntasks. However, previous research demon-\\net al., 2020; Brown et al., 2020; Touvron et al.,\\nstratedLLMstrainedusingdomain-focusedcor-\\npora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\\nspired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\\nINDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\\nfortheEarthscience,biology,physics,helio- domain-specific natural language understanding\\nphysics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific\\ncorporadrawnfromdiversedatasources. The\\nLLMs such as SCIBERT (Beltagy et al., 2019),\\nsuiteofmodelsinclude: (1)anencodermodel\\ntrainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\\ncorpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\\nstandingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\\ngeneral text embedding model trained using developedwiththegoalofimprovingaccuracyon\\na diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\\nple sources to address information retrieval\\nWuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-\\ndisciplinaryfieldsrelatedtotheEarth,celestialbod-\\ntencyorresourceconstraints. Wealsocreated\\nies, the Sun, and planets within our solar system\\nthreenewscientificbenchmarkdatasetsnamely,\\nsuchasphysics,Earthscience,astrophysics,helio-\\nCLIMATE-CHANGE NER (entity-recognition),\\nNASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\\naccelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\\nfields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\\nelsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\\n(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the\\nasexistingbenchmarktasksinthedomainsof\\ninterdisciplinarynatureofthesedomainsofinter-\\ninterest.\\nestisreflectedinavastbodyofliteraturescattered\\n1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\\na collection of encoder-based LLMs focused on\\nLarge language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\\namountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\\npabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\\nzationsandenterprisesworkinginthesefieldsby\\nContact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\\nmuthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\\n4202\\nyaM\\n02\\n]LC.sc[\\n2v52701.5042:viXra\\n\\n\\nEarth Science Data Pretraining\\n(Masked Finetuning Scientific Corpora\\nBioMedical Data Language Indus-Base (Contrastive\\nModelling) Learning) Open QA\\nAstrophysics Data\\nDuplicate Pairs\\nAstronomy Data\\nRepresentation Indus-Retriever-Base Citation Pairs\\nGeneral Science Distillation\\nData\\nFact Verification\\nGeneral English\\nData\\nGeneric Corpora\\nOutput\\nEncoder Training Corpus Indus-Small Distillation Embedding Training\\nCorpus\\nData Teacher (KD)\\nInitialization Output Indus-Retriever-\\nSmall\\nBC5-CHEM BC5-Disease NCBI-Disease BC2GM\\nNASA-QA TREC-Covid NFCorpus NQ HotpotQA\\nJNLPBA EBM-PICO ChemProt DDI\\nClimate FiQA Arguana Touche DBPedia NASA-IR\\nGAD HoC PubMedQA BioASQ Change NER\\nSciDocs FEVER C Fl Eim VEat Re SciFact\\nBIOSSES BLURB Benchmark BEIR Benchmark\\nNatural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir\\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\\nNASA-QA,CLIMATE-CHANGENERandNASA-IR.\\nenablingthemininformeddecision-making. (Beltagy et al., 2019). We also show that the\\nSpecifically, we make the following contribu- knowledge-distilledmodelsachievedasignifi-\\ntions: cantboostinlatencywhilemaintainingstrong\\n1. Utilizing the byte-pair encoding algorithm, empiricalperformancecomparedtotheoriginal\\nwe constructed INDUSBPE, a customized to- modelsonmostofthebenchmarktasks.\\nkenizerfromthecuratedscientificcorpus.\\n2 Data\\n2. Wepretrainedmultipleencoder-onlyLLMsus-\\ning the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than\\nsentence-embeddingmodelsbyfine-tuningthe\\ntheircounterpartstrainedonopen-domaincorpora.\\nencoder-only models with a contrastive learn-\\nWemeticulouslyidentifiedcorporaforeachofthe\\ningobjectivetolearn“universal”sentenceem-\\naforementioneddomains,andcreatedEnglish-only\\nbeddings (Gao et al., 2021) (§4). We also\\nmodels for the sake of containment. Specifically,\\ntrained smaller, more efficient versions of\\nforeachofthedomains,weusedopen-sourcedata\\nthesemodelsusingknowledge-distillationtech-\\nwhich has a permissive license, and further aug-\\nniques(§3.3,§4.2).\\nmentedthemwithspecificdatafromNASAandits\\n3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in\\nrecognitiontask),NASA-QA(anextractiveques-\\nourtrainingcorpora. Webrieflydescribeeachdata\\ntionansweringtask)and NASA-IR (aretrieval\\nsourcebelow,andpresentstatisticsofthedatain\\ntask)(§5)tofurtheraccelerateresearchinthis\\nTable1.\\nmulti-disciplinaryfield.\\n• SAO/NASAAstrophysicsDataSystem(ADS)1:\\n4. Throughexperimentalresults,weshowstrong\\nADSisthebiggestsourceofdata,coveringpub-\\nperformancebyourmodelsonthesebenchmark\\nlications in the areas of astronomy and astro-\\ntasks as well as on existing domain-specific\\nphysics,physicsandgeneralscienceincluding\\nbenchmarks, outperforming general-purpose\\nallarXive-prints.\\nmodels like RoBERTa (Liu et al., 2019) as\\nwellasscientific-domainencoderslikeSCIBERT 1https://ui.adsabs.harvard.edu\\n\\n\\nDataset Domain #Tokens Ratio Tokenizer ADS PMC Wikipedia\\nNASACMR EarthScience 0.3B 1% RoBERTa 12,867,439 7,549,075 15,859\\nAMSandAGUpapers EarthScience 2.8B 4% +lower_cased 12,862,227 7,557,868 16,901\\nEnglishWikipedia General 5.0B 8% INDUSBPE 12,309,023 6,920,659 16,056\\nPubMedAbstracts Biomedical 6.9B 10%\\nTable2: NumberoftokensproducedbyRoBERTaand\\nPMC Biomedical 18.5B 28%\\nINDUSBPEtokenizersappliedto1ksamplesfromeach\\nSAO/NASAADS Astronomy, 32.7B 49%\\ndataset. Fewer tokens lead to a smaller computation\\nAstrophysics,\\ncost.\\nPhysics,\\nGeneralScience\\nTotal 66.2B 100% ingdataset(§2)6. Forafaircomparison,wesetthe\\nvocabularysizeto50,265,whichisequaltothatof\\nTable1: Basicstatisticsofourpretrainingdataset.\\nthe RoBERTatokenizer(Liuetal.,2019)andused\\ntheuncasedvariationofboththetokenizers.\\n• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-\\nliterature maintained by National Library of BPE and the RoBERTa tokenizer. Out of 50,265\\nMedicineandNationalInstitutesofHealth. We tokens,22,355(44.5%)tokensarecommoninboth\\nusedtheportionofPMCthathasacommercial- thetokenizerswhiletheremaining27,910(55.5%)\\nfriendly license, along with the PubMed ab- tokensareincludedonlyineithertokenizer, indi-\\nstractsofallthearticlesinPMC. catingasignificantdistributionalshiftindomain.\\n• American Meteorological Society (AMS)3: Tofurther understandthe effect, weapplied both\\nWeusedfull-textjournaldocumentsspanning RoBERTaandINDUSBPEon1,000randomlysam-\\ntopicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts\\noceanography, atmospheric sciences, climate, tosinglesentences. AsshowninTable2, INDUS-\\nhydrometeorology, weather and forecasting, BPE tokenizer produced fewer tokens than the\\nandsocietalimpacts. RoBERTa tokenizer, leading to 8˜% drop in com-\\n• American Geophysical Union (AGU)4: The putationcostduringtraining.\\nAGUdatasetincludedjournaldocumentsacross Table3comparestheRoBERTatokenzierandIN-\\nthe topics of atmospheres, biogeosciences, DUSBPE tokenizer,illustratingthattheproposed\\nEarth surface, machine learning and compu- tokenizertreatedscientificterms(suchasbiomak-\\ntation, oceans, planets, solid earth, and space ers, phosphorylated, alzheimer) as single tokens\\nphysics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-\\n3.2 EncoderModel\\nquality,continuouslyevolvingmetadatasystem\\nthat catalogs all data and service metadata Wefirsttrainedanencoder-onlymodel,INDUS ,\\nBASE\\nrecords for NASA’s Earth Science Data and usingamaskedlanguagemodelingobjective. The\\nInformation System (ESDIS). It contains text modelarchitecturefollowsRoBERTa (Liuetal.,\\nBASE\\ndescriptions of the NASA Earth science data 2019),whichconsistsof12layersandhas125M\\nproducts. parameters. Weadoptedthedefaulthyperparame-\\nters7 butwithaneffectivebatchsizeof92,16. We\\n3 Methodology: EncoderModels trainedthemodelfor500Kstepsusing192V100\\nGPUs.\\n3.1 INDUSBPE Tokenizer\\n3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL\\n2https://www.ncbi.nlm.nih.gov/pmc 38M parameters through knowledge distillation\\n3https://www.ametsoc.org/index.cfm/ams/publications/\\n4https://agupubs.onlinelibrary.wiley.com/ 6We used HF tokenizers, https://github.com/\\n5https://www.earthdata.nasa.gov/eosdis/science-system- huggingface/tokenizers\\ndescription/eosdis-components/cmr 7WereferreaderstoTable9in(Liuetal.,2019).\\n\\n\\nInputtext\\nnoveltaubiomarkersphosphorylatedatt181,t217ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected\\nTokenizationbyRoBERTa\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTokenizationby INDUSBPE\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTable3: TokenizationcomparisonbetweenRoBERTaandourtokenizers. InputtextadaptedfromSuárez-Calvet\\netal.(2020).\\n(cid:88) (cid:88)\\ntechniquesbyusingINDUS\\nBASE\\nastheteacher. IN- Z\\ni\\n= es(qi,pj)+ es(qj,pi)\\nDUS follows a 4-layer architecture recom- j j\\nSMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the\\nwheres(q,p)isameasureoftemperature-scaled\\ndistillationobjectiveproposedinMiniLMv2(Wang\\ncosinesimilaritybetweentheembeddingsofquery\\netal.,2021)totransferfine-grainedself-attention\\nandapassagemeasuredby:\\nrelations,whichhasbeenshowntobethecurrent\\nstate-of-the-art(Udagawaetal.,2023). Usingthis 1 E(q)·E(p)\\ns(q,p) = (3)\\nobjective,wetrainedthemodelfor500Kstepswith τ ∥E(q)∥∥E(p)∥\\naneffectivebatchsizeof480on30V100GPUs.\\nTrainingData Similartopriorwork(Wangetal.,\\n2022; Li et al., 2023; Xiao et al., 2023), we em-\\n4 Methodology: SentenceEmbedding\\nployedastage-wisetrainingapproachforoursen-\\nModels\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional\\noccurringpairscollectedfrominternetsources,\\nvectors,allowingforefficientuseindenseretrieval\\nsuch as Wikipedia, StackExchange, etc. We\\nsystems,whererelevantpassagesforagivenquery\\nalsoincludedscientificdatafromPubMed,PMC\\nareidentifiedonthebasisofthesimilaritybetween\\n(§2),Arxivand S2ORC (Loetal.,2020)asin-\\ntheirembeddings(Karpukhinetal.,2020).\\ndomaindataforourscience-orientedretriever\\nmodel. Furthermore, we created a domain-\\nContrastiveLearningObjective Sentenceem-\\nspecific dataset from the ADS data (§2) by in-\\nbeddingmodelstrainedusingacontrastivelearn-\\ncludingtitle-abstractpairs.\\ningobjective(Khoslaetal.,2020;Gaoetal.,2021)\\n2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa\\n(Kwiatkowskietal.,2019),SQuAD(Rajpurkar\\nnon-relevantpassage.\\net al., 2016), SPECTER pairs (Cohan et al.,\\nInspired by recent work (Li et al., 2023), we\\n2020), etc. We included the aforementioned\\nusedanimprovedcontrastivelossbyintroducing\\nADS data and a sample of the S2ORC data in\\nanadditionalbidirectionalsignal. Specifically,for\\nthisstep,toboostdomain-specificsignals.\\na triple {q,p+,P−} of a query, a relevant (posi-\\nAppendixAcontainscomprehensivedetailsabout\\ntive)passage,andasetofnon-relevant(negative)\\nthe datasets used in training. For both training\\npassagesP− = {p−}m ,WedefinetheInfoNCE\\nj j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches\\n1 (cid:88)n es(qi,p+ i ) from each data source proportionately to its size,\\nL = − log (1)\\nIC\\nn Z i similartoLietal.(2023).\\ni=1\\n\\n\\nModel Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K\\ninthelargermodelbutimprovestheperformance\\nstepsfollowedbyanadditional100Kstepswiththe\\nofthesmallermodel.\\nsuperviseddata. Weusedalearningrateof2e−5\\nFor distilling the sentence embedding model,\\nduringboththesesteps.\\nwefoundthatastage-wisetrainingapproachdoes\\n4.2 KnowledgeDistillationforEmbedding not benefit performance as much as in the non-\\nModels distillation case (ablation presented in Appendix\\nB). We thus distilled in a single step with all the\\nTooptimizethelatencyforretrievalapplications,\\ndatadescribedin§4.1andAppendixAandadded\\nwe also created a small retriever model with the\\nlabelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE\\nmodel(INDUS SMALL),bydistillingtheteacher’sdis- ModelSpecifications Webuiltthesentenceem-\\ntributionofsimilarityscores. Furthermore,wefind beddingmodelbydistillinginto INDUS . This\\nSMALL\\nthatitisnecessarytomodifythetrainingstrategy isa4-layermodelwithanembeddingdimension\\nfordistillation,asdescribedbelow. of576. Werefertotheresultingretrievermodelas\\nDistillationLoss Weusedknowledgedistillation\\nINDUS-RETRIEVER SMALL. Itfollowsabi-encoder\\nframework,andherewefindthatusingthevector\\ntechniquesintroducedin(Xuetal.,2023). Specif-\\nrepresentationofthefirsttokenastheembedding\\nically, for a sentence x and its corresponding in-\\ni\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-\\ntionp ofsimilarityscoresbetweenpairsandthe\\nt TrainingDetails FortheRetro-MAEstylepre-\\nstudent’sdistribution, p . FollowingHintonetal.\\ns training(Xiaoetal.,2022),wetrainedon8A100\\n(2014), we also scaled the output distribution of\\nGPUs with an effective batch size of 128 for 2\\nbothteacherandstudentbyatemperature,τ :\\nKD epochswithalearningrateof2e−5. Forthestage-\\nn m wisedistillation,wetrainedon2A100GPUsfor\\n(cid:88)(cid:88)\\nL = − p (x ,x )logp (x ,x ) (4) 300K steps with an effective batch size of 2,048,\\nKD t i j s i j\\ni=1 j=1 andlearningrateof7e−4. Throughexperimenta-\\ntion,Wefoundthatτ = 4performedthebest.\\nKD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)\\nt i j (cid:80)m est(xi,x k)/τKD ingthelanguageunderstandingcapabilitiesmodels.\\nk=1\\nHowever,tothebestourknowledge,thereisano-\\nHere,s (x ,x )ands (x ,x )representthesim-\\ns i j t i j\\nticeableabsenceofdatasetstailoredforthediverse\\nilarityscoresbetweentwopairs{x ,x },definedin\\ni j\\nand multidisciplinary field under study. Thus, to\\nEquation3forthestudentandteacherrespectively.\\neffectively benchmark the proposed NLP models\\nTrainingData Wefirstconductedaembedding- and further accelerate research in this multidisci-\\noriented pretraining step, as presented in Retro- plinarydomain,Weintroducedthreenewdatasets,\\nMAE (Xiao et al., 2022), on English Wikipedia, anNERtask,aQAtask,andanIRtask,described\\nBooksCorpus,andStackExchangedata,totalling below.\\n\\n\\nTrain Validation Test spansoftheparagraphwhichanswerthequestions.\\nNum.Abstracts 382 77 75\\nWeused29paragraphs(with145QApairsintotal)\\nNum.Tokens 32,031 6,443 5,850\\nEntityLabels asthetrainingsetandtheremaining10paragraphs\\nclimate-nature,climate-greenhouse-gases,climate-assets,\\n(with 50 questions in total) as the evaluation set.\\nclimate-problem-origins,climate-mitigations,\\nclimate-properties,climate-impacts,climate-datasets, Thetrainingsetwasfurtheraugmentedwithpara-\\nclimate-organizations,climate-observations,\\ngraphsand QA pairsrelatedtoEarthsciencefrom\\nclimate-models,climate-hazards,climate-organisms\\nthe SQuADdataset(Rajpurkaretal.,2018). Specif-\\nTable4: CLIMATE-CHANGE NER statisticsandentity ically,thoserelatedtooxygen,Amazonrainforest\\nlabels\\nandgeologywereused. Thisresultedinapruned\\nSQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-\\nby augmenting these SQuAD pairs to the training\\nfer some assistance in exploring data related to\\ndatasourcedfromEarthsciencepapers,whilekeep-\\nclimatechange,thecomplexityofclimate-related\\ningtheevaluationsetintact.\\nqueries often requires more sophisticated natural\\nlanguageprocessingtechniques. Thisnecessityis 5.3 NASA-IR\\nunderscoredbytheextensivearrayofclimatemod-\\nWe introduced a domain-specific information re-\\nels, datasets, and organizations involved, which\\ntrieval benchmark, NASA-IR10, spanning almost\\ndemand meticulous curation and continuous up-\\n500question-answerpairsrelatedtotheEarthsci-\\ndates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-\\ndata,comprehensiveoverviewsofclimatemodels\\nically, we sampled a set of 166 paragraphs from\\nandimpactassessmentsarescarceandnoteasily\\nAGU, AMS, ADS, PMC and Pubmed (§2) and\\naccessible.\\nmanually annotated with 3 questions that are an-\\nInordertobridgethisgap,weintroducedacom-\\nswerablefromeachoftheseparagraphs,resulting\\nprehensive dataset for developing and evaluating\\nin498questions. Weused398ofthesequestions\\nNLP models tailored towards understanding and\\nas the training set and the remaining 100 as the\\naddressing climate-related topics across various\\nvalidation set. To comprehensively evaluate the\\ndomains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-\\nwhichthenamedentitiesofinterestoriginatefrom\\nstracts with these annotated paragraphs. On an\\ncomplextaxonomiesusedinclimate-relatedlitera-\\naverage, each query is 12 words long, and each\\nture. Thisdatasetcomprises534abstractssourced\\nparagraphis120wordslong. WeusedRecall@10\\nfromSemanticScholarAcademicGraph(Kinney\\nasevaluationmetricsinceeachquestionhasonly\\netal.,2023),collectedusingaseedsetofclimate-\\nonerelevantdocument.\\nrelated keywords such as wildfire or floods. The\\nabstractswereannotatedusingtheIOB(inside,out-\\n6 ExperimentalResults\\nside,beginning)taggingschemeandencompasses\\nadiversearrayofentitytypes,asshowninTable4. Baselines Wecompared INDUS modelsagainst\\nopensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.\\nsweringtaskfocusedontheEarthsciencedomain. • INDUS was compared to MINILM (6-\\nSMALL\\nFirst, 39 paragraphs from Earth science papers\\nlayer)13 andTINYBERT(4-layer)14.\\nwhichappearedinAGUandAMSjournals(§2)were\\n10https://huggingface.co/datasets/nasa-impact/nasa-smd-\\nsourced. Subject matter experts from NASA for- IR-benchmark\\nmulatedquestionsandmarkedthecorresponding 11https://huggingface.co/FacebookAI/roberta-base\\n12https://huggingface.co/allenai/scibert_scivocab_uncased\\n8https://huggingface.co/datasets/ibm/Climate-Change- 13https://huggingface.co/nreimers/MiniLM-L6-H384-\\nNER uncased\\n9https://huggingface.co/datasets/nasa-impact/nasa-smd- 14https://huggingface.co/huawei-\\nqa-benchmark noah/TinyBERT_General_4L_312D\\n\\n\\nBasemodel(125Mparams.) Smallmodel(∼30Mparams.)\\nTask Metric Dataset RoBERTa SCIBERT INDUSBASE TINYBERT MINILM INDUSSMALL\\nBC5-chem 90.3(0.2) 91.4(0.2) 93.3(0.2) 84.6(0.2) 86.1(0.3) 90.7(0.1)\\nBC5-disease 81.5(0.3) 83.7(0.3) 85.2(0.3) 74.0(0.4) 77.4(0.3) 81.3(0.3)\\nNER EntityF1 NCBI-disease 87.6(0.6) 87.6(0.4) 88.3(0.4) 81.2(0.4) 83.1(0.5) 85.6(0.6)\\nBC2GM 82.1(0.3) 82.3(0.2) 84.0(0.3) 74.7(0.4) 77.1(0.2) 79.7(0.3)\\nJNLPBA 79.1(0.2) 78.2(0.2) 80.3(0.2) 70.3(0.2) 73.4(0.3) 75.7(0.2)\\nPICO MacroF1 EBMPICO 72.3(0.3) 72.4(0.3) 73.1(0.2) 67.4(0.2) 70.3(0.1) 73.1(0.2)\\nChemProt 50.4(28.2) 73.9(0.7) 76.9(0.5) 56.2(3.2) 55.9(2.1) 71.7(0.9)\\nRelation\\nMicroF1 DDI 78.6(1.5) 80.1(1.0) 81.7(0.5) 39.3(5.3) 51.5(2.9) 69.0(1.2)\\nExtraction\\nGAD 80.0(1.1) 81.6(1.2) 79.4(5.6) 76.4(1.3) 77.3(1.0) 81.3(0.7)\\nDocument\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy\\nAnswering BioASQ 69.1(4.8) 74.6(4.5) 69.6(5.8) 74.3(3.6) 66.7(2.3) 75.4(3.3)\\nSentence\\nSimilarity Pearson BIOSSES 79.8(6.3) 86.3(3.5) 72.2(9.5) 88.2(1.1) 26.6(8.7) 70.4(3.3)\\nMicroAverage - - 75.9(3.7) 79.2(1.3) 78.9(2.4) 67.6(1.9) 66.1(1.9) 76.2(1.0)\\nMacroAverage - - 74.9(3.7) 78.2(1.6) 76.4(3.2) 65.6(2.4) 60.6(3.0) 74.3(1.3)\\nTable5:EvaluationresultsonBLURB.Resultsreportedareaveragedon10randomseedswithstandarddeviationin\\nparenthesis. Microaverageisreportedacrossdatasetswhilemacroaverageiscomputedbyfirstaveragingscoreson\\neachtask(say,taskaverage),followedbyaveragingthetaskaverageacrosstasks. Resultsinboldindicatehighest\\nperformancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE\\nBGE 15 andaRoBERTa modelfinetuned RoBERTa 60.8(0.8)\\nBASE BASE\\nwiththesamemethodpresentedin§4.1. SCIBERT 61.8(0.7)\\n• INDUS-RETRIEVER\\nSMALL\\nwas compared to INDUS\\nBASE\\n64.0(1.0)\\nMINILM-V216 and BGE 17. TINYBERT 34.3(1.6)\\nSMALL\\nMINILM 44.7(1.3)\\n6.1 NaturalLanguageUnderstanding INDUS 54.8(0.8)\\nSMALL\\nBenchmarks\\nTable 6: CLIMATE-CHANGE NER benchmark results.\\nWe evaluated our models on BLURB (Gu et al.,\\nStandard deviation over 10 random seeds shown in\\n2021),abenchmarksuitefornaturallanguageun-\\nparenthesis. Resultsinboldandunderlineindicatehigh-\\nderstandingandreasoningtasksinthebiomedical estperformanceandsignificantdifferencefromsecond\\ndomain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.\\nTable 5 shows the evaluation results. Among\\nbase models, INDUS BASE significantly outper- WealsonoticedSCIBERTtendstoperformbetter\\nformsthegeneral-purposeRoBERTamodelonmi-\\nthan our model on paired input-text tasks, such\\ncro/macroaveragewhileachievingcompetitiveper-\\nas QA and semantic similarity tasks, although\\nformance to the bio-domain-specific counterpart,\\nthe results have relatively large standard devia-\\nSCIBERT.\\ntions. We hypothesized that the additional next\\nAsforsmallermodels,wenoticed INDUS SMALL sentence prediction objective during training in\\noutperformed the baselines, TINYBERT and BERT-stylemodels(suchasSCIBERT)incontrastto\\nMINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text\\ninNER,PICO,relationextraction,anddocument\\ntasks. Thistrendwasconsistentwiththeobserva-\\nclassification tasks. This demonstrates the effec-\\ntionsofTinnetal.(2023).\\ntivenessofknowledgedistillationfromourdomain-\\nspecificteachermodel,INDUS . 6.2 CLIMATE-CHANGE NER\\nBASE\\nAs shown in Table 6, our models clearly outper-\\n15https://huggingface.co/BAAI/bge-base-en-v1.5\\n16sentence-transformers/all-MiniLM-L6-v2 formedthecorrespondingbaselinemodelsonthe\\n17https://huggingface.co/BAAI/bge-small-en-v1.5 CLIMATE-CHANGE NERtask,suggestingtheeffec-\\n\\n\\ntivenessoftrainingonlargedomain-specificdata. Model NASA-IR↑ BEIRAvg.↑ Retrieval\\nTime↓\\n6.3 NASA-QA BRo GB EE BAR ST Ea BASE 0 0. .6 66\\n7\\n0 0. .3 57\\n2\\n1 1. .2 10\\n8\\nAs mentioned in §5, we augmented the training\\nINDUS-RETRIEVERBASE 0.71 0.41 1.19\\nMINILM-V2 0.62 0.39 0.24\\nsetwithrelevantSQuADpairsforfine-tuning. All BGESMALL 0.66 0.51 0.42\\nmodelsarefinetunedfor15epochs,andtheresults INDUS-RETRIEVERSMALL 0.73 0.42 0.26\\nareshowninTable7. WeobservedthatINDUS\\nBASE Table 8: Evaluation results on NASA-IR and BEIR.\\noutperformedallmodelsofsimilarsizes,while IN-\\nNASA-IRshowedRecall@10whileBEIRreportedthe\\nDUS hadrelativelystrongperformancecom- averagenDCG@10acrossalltasks. Retrievaltimeper\\nSMALL\\nparedtoitscounterparts. queryontheNQtaskfromBEIR,reportedinseconds.\\nModel F1(SD)\\n7 Conclusions\\nRoBERTa 66.8(3.1)\\nSCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-\\nMINILM 59.2(3.9) enizer and in-domain data for training high qual-\\nINDUS 47.4(1.8) ityencodermodelsandsentenceembeddingmod-\\nSMALL\\nels. Further, we created smaller versions of the\\nTable7: NASA-QA benchmarkresults. Standarddevi- proposedmodelssuitableforapplicationswithla-\\nation over 3 random seeds shown in parenthesis. Re-\\ntencyorresourceconstraintsthroughstate-of-the-\\nsultsinboldandunderlineindicatehighestperformance\\nartknowledgedistillationtechniques. Fortheben-\\nandsignificantdifferencefromsecondhighestresultby\\nefit of the scientific community, we will release\\nmorethantwostandarddeviationsineachmodelsize,\\nthedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.\\nDoguAraci.2019. Finbert: Financialsentimentanaly-\\nsiswithpre-trainedlanguagemodels.\\n6.4 InformationRetrievalBenchmarks\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nWeevaluatedourmodelsonthe NASA-IR dataset\\nERT:Apretrainedlanguagemodelforscientifictext.\\naswellasBEIRBenchmark(Thakuretal.,2021), InProceedingsofthe2019ConferenceonEmpirical\\nwhichconsistsof12retrievaltasksspanningavari- Methods in Natural Language Processing and the\\n9thInternationalJointConferenceonNaturalLan-\\netyofdomains. TheBEIRbenchmarkusedtheNor-\\nguageProcessing(EMNLP-IJCNLP),pages3615–\\nmalized Cumulative Discount Gain (nDCG@10)\\n3620,HongKong,China.AssociationforComputa-\\n(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\ntenceembeddingmodels,alongwithourbaselines.\\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\\nAs shown, both of our sentence embedding mod- Neelakantan,PranavShyam,GirishSastry,Amanda\\nelssignificantlyoutperformedthebaselinesonthe Askell, Sandhini Agarwal, Ariel Herbert-Voss,\\nNASA-IRtaskwhilestillmaintaininggoodperfor- Gretchen Krueger, Tom Henighan, Rewon Child,\\nAdityaRamesh,DanielZiegler,JeffreyWu,Clemens\\nmanceonseveraloftheBEIRtasks. (Wepresented\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nresultsforeachBEIRtaskinAppendixC).\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nWe also measured the average time per query Clark, ChristopherBerner, SamMcCandlish, Alec\\nforretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-\\nquestionssetofBEIR,onasingleA100GPU.This\\nvances in Neural Information Processing Systems,\\ntime includes the time to encode the query, cor-\\nvolume 33, pages 1877–1901. Curran Associates,\\npus,andtimetoretrieverelevantdocuments. No- Inc.\\ntably, INDUS-RETRIEVER outperformed IN-\\nSMALL Colin B. Clement, Matthew Bierbaum, Kevin P.\\nDUS-RETRIEVER BASE,onbothNASA-IRandBEIR,\\nO’Keeffe, and Alexander A. Alemi. 2019. On the\\nwhilebeingabout4.6xfaster. useofarxivasadataset.\\n\\n\\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug Sarna, Yonglong Tian, Phillip Isola, Aaron\\nDowney, and Daniel S. Weld. 2020. SPECTER: Maschinot, CeLiu, andDilipKrishnan.2020. Su-\\nDocument-level Representation Learning using pervisedcontrastivelearning. InAdvancesinNeural\\nCitation-informedTransformers. InACL. InformationProcessingSystems,volume33,pages\\n18661–18673.CurranAssociates,Inc.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of Rodney Kinney, Chloe Anastasiades, Russell Authur,\\ndeepbidirectionaltransformersforlanguageunder- Iz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\\nstanding. InProceedingsofthe2019Conferenceof ski,IsabelCachola,StefanCandra,YoganandChan-\\ntheNorthAmericanChapteroftheAssociationfor drasekhar, Arman Cohan, Miles Crawford, Doug\\nComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-\\n4171–4186,Minneapolis,Minnesota.Associationfor ham, Fangzhou Hu, Regan Huff, Daniel King, Se-\\nComputationalLinguistics. bastianKohlmeier,BaileyKuehl,MichaelLangan,\\nDaniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\\nMatthewDunn,LeventSagun,MikeHiggins,V.Ugur Kelsey MacMillan, Tyler Murray, Chris Newell,\\nGuney, Volkan Cirik, and Kyunghyun Cho. 2017. Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\\nSearchqa: Anewq&adatasetaugmentedwithcon- Shen,AmanpreetSingh,LucaSoldaini,Shivashankar\\ntextfromasearchengine. Subramanian,AmberTanaka,AlexD.Wade,Linda\\nWagner, Lucy Lu Wang, Chris Wilhelm, Caroline\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic\\ntractedknowledgebases. InProceedingsofthe20th scholaropendataplatform.\\nACMSIGKDDInternationalConferenceonKnowl-\\nedge Discovery and Data Mining, KDD ’14, page TomKwiatkowski, JennimariaPalomaki, OliviaRed-\\n1156–1165, New York, NY, USA. Association for field,MichaelCollins,AnkurParikh,ChrisAlberti,\\nComputingMachinery. DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\\ntonLee,KristinaToutanova,LlionJones,Matthew\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nSimCSE:Simplecontrastivelearningofsentenceem- Uszkoreit,QuocLe,andSlavPetrov.2019. Natural\\nbeddings. In Proceedings of the 2021 Conference Questions: A Benchmark for Question Answering\\nonEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-\\nJinhyukLee,WonjinYoon,SungdongKim,Donghyeon\\nminican Republic. Association for Computational\\nKim,SunkyuKim,ChanHoSo,andJaewooKang.\\nLinguistics.\\n2019. BioBERT:apre-trainedbiomedicallanguage\\nrepresentation model for biomedical text mining.\\nYuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto\\nBioinformatics,36(4):1234–1240.\\nUsuyama,XiaodongLiu,TristanNaumann,Jianfeng\\nGao,andHoifungPoon.2021. Domain-specificlan-\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nguagemodelpretrainingforbiomedicalnaturallan-\\nGhazvininejad,AbdelrahmanMohamed,OmerLevy,\\nguageprocessing. ACMTrans.Comput.Healthcare,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\n3(1).\\nBART:Denoisingsequence-to-sequencepre-training\\nfornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In\\ningoftheAssociationforComputationalLinguistics,\\nNeurIPSDeepLearningWorksop.\\npages7871–7880,Online.AssociationforComputa-\\ntionalLinguistics.\\nZhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon\\nDuede,KyleChard,andIanFoster.2023. Thedimin- PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-\\nishingreturnsofmaskedlanguagemodelstoscience. ervini,HeinrichKüttler,AleksandraPiktus,Pontus\\nStenetorp,andSebastianRiedel.2021. PAQ:65Mil-\\nShuHuangandJacquelineMCole.2022. Batterybert:\\nlionProbably-AskedQuestionsandWhatYouCan\\nA pretrained language model for battery database\\nDoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,\\nVladimirKarpukhin,BarlasOguz,SewonMin,Patrick Pengjun Xie, and Meishan Zhang. 2023. Towards\\nLewis,LedellWu,SergeyEdunov,DanqiChen,and generaltextembeddingswithmulti-stagecontrastive\\nWen-tauYih.2020. Densepassageretrievalforopen- learning.\\ndomainquestionanswering. InProceedingsofthe\\n2020ConferenceonEmpiricalMethodsinNatural YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-\\nLanguageProcessing(EMNLP),pages6769–6781, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nOnline.AssociationforComputationalLinguistics. Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron proach. arXivpreprintarXiv:1907.11692.\\n\\n\\nKyleLo,LucyLuWang,MarkNeumann,RodneyKin- and VERification. In Proceedings of the 2018\\nney,andDanielWeld.2020. S2ORC:Thesemantic Conference of the North American Chapter of\\nscholaropenresearchcorpus. InProceedingsofthe the Association for Computational Linguistics:\\n58thAnnualMeetingoftheAssociationforCompu- Human Language Technologies, Volume 1 (Long\\ntationalLinguistics,pages4969–4983,Online.Asso- Papers), pages 809–819, New Orleans, Louisiana.\\nciationforComputationalLinguistics. AssociationforComputationalLinguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, RobertTinn,HaoCheng,YuGu,NaotoUsuyama,Xi-\\nDarioAmodei,andIlyaSutskever.2019. Language aodong Liu, Tristan Naumann, Jianfeng Gao, and\\nmodelsareunsupervisedmultitasklearners. HoifungPoon.2023. Fine-tuninglargeneurallan-\\nguagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe HugoTouvron,ThibautLavril,GautierIzacard,Xavier\\nlimitsoftransferlearningwithaunifiedtext-to-text Martinet,Marie-AnneLachaux,TimothéeLacroix,\\ntransformer. JournalofMachineLearningResearch, BaptisteRozière,NamanGoyal,EricHambro,Faisal\\n21(1). Azhar,AurelienRodriguez,ArmandJoulin,Edouard\\nGrave,andGuillaumeLample.2023. Llama: Open\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nandefficientfoundationlanguagemodels.\\nKnowwhatyoudon’tknow:Unanswerablequestions\\nforsquad. CoRR,abs/1806.03822.\\nAashka Trivedi, Takuma Udagawa, Michele Merler,\\nRameswarPanda,YousefEl-Kurdi,andBishwaran-\\nPranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in\\nMachineComprehensionofText. InEMNLP.\\nlanguagemodels. arXivpreprintarXiv:2303.09639.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nTakumaUdagawa,AashkaTrivedi,MicheleMerler,and\\nBERT:SentenceembeddingsusingSiameseBERT-\\nBishwaranjanBhattacharjee.2023. Acomparative\\nnetworks. InProceedingsofthe2019Conferenceon\\nanalysisoftask-agnosticdistillationmethodsforcom-\\nEmpiricalMethodsinNaturalLanguageProcessing\\npressingtransformerlanguagemodels. InProceed-\\nandthe9thInternationalJointConferenceonNatu-\\ningsofthe2023ConferenceonEmpiricalMethodsin\\nralLanguageProcessing(EMNLP-IJCNLP),pages\\nNaturalLanguageProcessing: IndustryTrack,pages\\n3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria\\nEmilio,CarlesFalcon,SherezadeFuentes,LauraHer- Nicholas Walker, Amalie Trewartha, Haoyan Huo,\\nnandez, Gema Huesa, Jordi Huguet, Paula Marne, SanghoonLee,KevinCruse,JohnDagdelen,Alexan-\\nTaniaMenchón,GrégoryOperto,AlbinaPolo,San- der Dunn, Kristin Persson, Gerbrand Ceder, and\\ndra Pradas, Anna Soteras, Marc Vilanova, and Na- AnubhavJain.2021. Theimpactofdomain-specific\\ntalia Vilor-Tejedor. 2020. Novel tau biomarkers pre-trainingonnamedentityrecognitiontasksinma-\\nphosphorylatedatt181,t217ort231riseintheini- terialsscience. AvailableatSSRN3950755.\\ntial stages of the preclinical alzheimer&#x2019;s\\n<i>continuum</i> when only subtle changes in Liang Wang, Nan Yang, Xiaolong Huang, Binxing\\na&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-\\nsupervisedcontrastivepre-training.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishekSrivastava,andIrynaGurevych.2021. Beir: WenhuiWang,HangboBao,ShaohanHuang,LiDong,\\nAheterogenousbenchmarkforzero-shotevaluation and Furu Wei. 2021. MiniLMv2: Multi-head self-\\nofinformationretrievalmodels. attention relation distillation for compressing pre-\\ntrainedtransformers. InFindingsoftheAssociation\\nJames Thorne, Andreas Vlachos, Christos forComputationalLinguistics: ACL-IJCNLP2021,\\nChristodoulopoulos, and Arpit Mittal. 2018. pages2140–2151,Online.AssociationforComputa-\\nFEVER: a large-scale dataset for fact extraction tionalLinguistics.\\n\\n\\nYining Wang, Liwei Wang, Yuanzhi Li, Di He, and Model Training NASA-IR BEIRAvg.\\nTie-Yan Liu. 2013. A theoretical analysis of ndcg INDUS-RETRIEVERSMALL One-Stage 0.73 0.42\\ntyperankingmeasures. InProceedingsofthe26th INDUS-RETRIEVERSMALL Stagewise 0.72 0.41\\nAnnualConferenceonLearningTheory,volume30\\nofProceedingsofMachineLearningResearch,pages\\nTable9: AblationStudy: EvaluationresultsonNASA-\\n25–54,Princeton,NJ,USA.PMLR. QAandBEIR.NASA-QAshowedRecall10whileBEIR\\nreportednDCG10.\\nShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,\\nMarkDredze,SebastianGehrmann,PrabhanjanKam-\\nbadur, David Rosenberg, and Gideon Mann. 2023. C CompleteResultsonBEIRBenchmark\\nBloomberggpt: Alargelanguagemodelforfinance.\\nTable11showstheper-datasetresultsontheBEIR\\nShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao.\\ntasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.\\nA SentenceEmbeddingTrainingData\\nTable 10 shows the various data sources used for\\ntrainingembeddingmodels. Alldataispresented\\nin the form of text-pairs, where each item in the\\npairmaybeasentenceoraparagraph. Inthetable,\\nDataFormatdenotess2pforsentence-to-paragraph\\nmappings,s2sforsentence-to-sentencemappings,\\nandp2pforparagraph-to-paragraphmappings. We\\nusedabout360millionpairsfortrainingandused\\nin-batchnegatives.\\nB AblationStudy: Stage-wiseDistillation\\nforEmbeddingModel\\nFor the distilled embedding models, we find that\\nstage-wisedistillationdoesnotbenefitperformance\\nasmuchasaone-stepprocess, combiningallthe\\nsupervised and unsupervised data. As shown in\\nTable9,thestage-wiseapproachunderperformed\\ntheone-stageapproachby1percentagepointfor\\nbothNASA-QAandonBEIR.\\n\\n\\nDataset Num. Pairs DataCategory DataFormat\\nStackOverflow† 18562443 Title-Body s2p\\nStackExchangeMath† 2201906 Title-Body s2p\\nS2ORC[title-abstract](Loetal.,2020) 41769185 Title-Body s2p\\nS2ORCCitationPairs[Abstracts](Loetal.,2020) 52603982 Title-Body p2p\\nStackExchange[title-body]† 5415570 Title-Body s2p\\nWikipedia(Faderetal.,2014) 6458670 Title-Body s2p\\nArxiv(Clementetal.,2019) 2358545 Title-Body s2p\\nNASAADS[title-abstract](§2) 2633240 Title-Body s2p\\nPubMed[title-abstract](§2) 24001387 Title-Body s2p\\nPMC[title-abstract](§2) 2585537 Title-Body s2p\\nStackExchangeDuplicateQuestions[title-body-title-body]† 250460 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[body-body]† 250519 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[title-title]† 304525 DuplicateQuestions s2s\\nWikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s\\nSQuAD(Rajpurkaretal.,2016) 87599 QuestionAnswers s2p\\nNQ(Kwiatkowskietal.,2019) 100231 QuestionAnswers s2p\\nSearchQA(Dunnetal.,2017) 582261 QuestionAnswers s2p\\nStackExchange[title-answer]† 4067139 QuestionAnswers s2p\\nStackExchange[title-body-answer]† 187195 QuestionAnswers p2p\\nPAQ(Lewisetal.,2021) 64371441 QuestionAnswers s2p\\nFEVER(Thorneetal.,2018)∗ 109810 FactVerification s2p\\nHotpotQA(Yangetal.,2018)∗ 85000 QuestionAnswering s2p\\nTable10:TrainingDataforEmbeddingModels. Thetrainingdatatotalstoaround360Mpairs. DataFormatdenotes\\ns2pforsentence-to-paragraphmappings,s2sforsentence-to-sentencemappings,andp2pforparagraph-to-paragraph\\nmappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval\\nTREC- NFCorpus NQ HotPotQA FiQA ArguaAna Touche DBPedia Scidocs FEVER Climate SciFact AVG.\\nCovid FEVER BEIR\\nRoBERTa\\nBASE\\n0.47 0.30 0.54 0.34 0.38 0.52 0.18 0.25 0.22 0.46 0.14 0.67 0.37\\nBGEBASE 0.78 0.37 0.54 0.73 0.41 0.64 0.26 0.41 0.22 0.86 0.31 0.74 0.52\\nINDUS-RETRIEVERBASE 0.56 0.32 0.54 0.49 0.36 0.54 0.17 0.31 0.21 0.56 0.14 0.74 0.41\\nMINILM-V2 0.47 0.32 0.44 0.47 0.35 0.50 0.17 0.32 0.22 0.52 0.25 0.65 0.39\\nBGESMALL 0.76 0.34 0.50 0.70 0.40 0.60 0.26 0.40 0.21 0.87 0.32 0.71 0.51\\nINDUS-RETRIEVERSMALL 0.55 0.31 0.53 0.48 0.29 0.50 0.21 0.33 0.23 0.61 0.23 0.71 0.42\\nTable11: EvaluationresultsBEIR.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "format_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34a383ee-ae87-45a8-bd5a-7cf593787a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {'context' : retriever | format_docs, 'question' : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "865852f7-878d-47aa-9691-d064a77c7b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'CohereEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x140161210>)\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
       "| ChatCohere(client=<cohere.client.Client object at 0x131129450>, async_client=<cohere.client.AsyncClient object at 0x140610350>, model='command-r', cohere_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a308ad-9505-415d-b91d-8fa180144ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The research focuses on developing INDUS, a series of encoder-based models designed to aid understanding in specific domains, namely Earth science. These models are trained using specialized corpora. According to the research, INDUS outperforms comparable models, and its smaller version, INDUS SMALL, also performs relatively well.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('Can you brief me the summary of this research paper in a very plain english which is easy to understand ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2f82a-9ae5-4573-a9bc-dadd43f17e19",
   "metadata": {},
   "source": [
    "Now, I have a follow-up question to understand about encoder models, LLM, everything about INDUS. How did it prove to be better than the other models?\n",
    "\n",
    "But for this, it would be ideal if the model knows about the conversation history so it can boil down the exact question very precisely and become time efficient.\n",
    "\n",
    "Thus let's add a chat history to this RAG Chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abc030-3cd0-4277-8986-4e5e95570bb9",
   "metadata": {},
   "source": [
    "##### 2 things : \n",
    "##### a) contextualize input question based on chat history\n",
    "##### b) add chat history to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c791b2-aa8a-43cb-8070-891135d2d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"Based on the given chat history and the most recent user question which\\\n",
    "could reference context in the chat history, create a standalone question which could be understood without\\\n",
    "the chat history. Do not answer the question. Just reformulate it if needed otherwise return it as it is.\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder('chat_history'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ab39dd-5798-4b19-82fe-87b7ac6c87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"You are an assistant for question answering tasks. Use the retrieved context to answer\\\n",
    "the user input. If you don't know the answer, just say you do not know.\\\n",
    "{context}\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', qa_system_prompt),\n",
    "    MessagesPlaceholder('chat_history'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "775e0377-521d-4eb9-ac16-94248466ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f2c2454-db48-4d6b-aa51-e79162b1ed1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
       " AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "chat_history.extend([\n",
    "    HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
    "    AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.')\n",
    "])\n",
    "\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "670d08e6-4816-4ddd-93cd-a95dde385487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
       " AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.'),\n",
       " HumanMessage(content='what are encoder models?'),\n",
       " AIMessage(content='Encoder models are a type of neural network architecture commonly used in natural language processing (NLP) and speech processing tasks. They are called \"encoders\" because they transform input data, such as a sentence or a sound clip, into a smaller set of values, known as an encoding. This encoding captures the most important aspects of the original data.\\n\\nIn the context of NLP, encoders read and comprehend text, then summarize or encode the content into fixed-length vectors, which can be thought of as a summary or fingerprint of the input. These vectors can then be used for various tasks like text classification, language translation, or as input to other models.\\n\\nEncoders can be used on their own or in combination with other network components, such as decoders, which take the encoding and expand it back into a more complex output, like generating text or translating to another language. The Transformer architecture, which has become popular for NLP tasks, relies on encoders to process the input sequences.\\n\\nIn the paper you provided, the researchers created and trained their own encoder models, called INDUS, on a large corpus of scientific text. These models are designed to capture the context and semantics of the text, and can be used for various NLP tasks in the science domain. They experiment with different versions of the encoder, including a larger one called INDUS BASE and a smaller, more efficient one called INDUS SMALL.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what are encoder models?\"\n",
    "response = rag_chain.invoke({'input' : question, 'chat_history' : chat_history})\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=response['answer'])\n",
    "])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd7c8d5-38b6-481e-b75d-79eb76b77696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
       " AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.'),\n",
       " HumanMessage(content='what are encoder models?'),\n",
       " AIMessage(content='Encoder models are a type of neural network architecture commonly used in natural language processing (NLP) and speech processing tasks. They are called \"encoders\" because they transform input data, such as a sentence or a sound clip, into a smaller set of values, known as an encoding. This encoding captures the most important aspects of the original data.\\n\\nIn the context of NLP, encoders read and comprehend text, then summarize or encode the content into fixed-length vectors, which can be thought of as a summary or fingerprint of the input. These vectors can then be used for various tasks like text classification, language translation, or as input to other models.\\n\\nEncoders can be used on their own or in combination with other network components, such as decoders, which take the encoding and expand it back into a more complex output, like generating text or translating to another language. The Transformer architecture, which has become popular for NLP tasks, relies on encoders to process the input sequences.\\n\\nIn the paper you provided, the researchers created and trained their own encoder models, called INDUS, on a large corpus of scientific text. These models are designed to capture the context and semantics of the text, and can be used for various NLP tasks in the science domain. They experiment with different versions of the encoder, including a larger one called INDUS BASE and a smaller, more efficient one called INDUS SMALL.'),\n",
       " HumanMessage(content='what is an encoder based LLM? Tell me about its architecture.'),\n",
       " AIMessage(content='An encoder-based Language Large Model (LLM) is a type of neural network model that uses an encoder architecture to process and understand natural language. These models are designed to handle sequential data, such as text, and are particularly effective at tasks like text generation, summarization, and information retrieval.\\n\\nThe architecture of an encoder-based LLM typically consists of multiple layers, or blocks. Here\\'s a simplified breakdown:\\n\\n1. Input Layer: The model receives a sequence of words, which could be a sentence or a part of a sentence. Each word is usually represented by its word embedding, a numerical vector that captures the meaning of the word.\\n\\n2. Encoder Layers: These are the core of the model. Each encoder layer consists of two main components:\\n    - Self-Attention Mechanism: This allows the model to weigh the importance of different words in the sentence differently. It calculates attention weights, which represent how much each word should be focused on relative to others.\\n\\n    - Feed Forward Neural Network (FFNN): This part of the layer transforms the output from the self-attention mechanism into a new representation. It typically consists of one or more hidden layers.\\n\\n3. Pooling Layer: After multiple encoder layers, there\\'s often a pooling layer that aggregates the outputs into a fixed-length vector. This vector represents the encoding of the entire input sequence and captures its most important features.\\n\\n4. Decoder Layers (sometimes): In some architectures, especially for tasks like machine translation, a set of decoder layers follows the encoder. The decoder uses the encoding to generate an output sequence, such as translating a sentence into another language.\\n\\n5. Output Layer: This layer produces the final output based on the decoder\\'s output or the pooled encoding, depending on the task. For example, it might generate a probability distribution over a vocabulary of words, allowing the model to select the most appropriate words to continue a sentence or summarize the input.\\n\\nDuring training, these models often use a technique called \"masked language modeling,\" where a certain percentage of the input words are hidden (masked), and the model\\'s goal is to predict them based on the surrounding context. This helps the model learn to understand the context and semantics of sentences.\\n\\nEncoder-based LLMs can be quite complex, with many layers and millions of parameters. They are often trained on vast amounts of text data to learn general language patterns and can then be fine-tuned on specific tasks like sentiment analysis or question answering. The Transformer architecture, proposed by Vaswani et al. in 2017, is a prominent example of an encoder-based LLM and has been instrumental in advancing the field of NLP. Models like BERT, GPT-2, and GPT-3 are based on this architecture.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is an encoder based LLM? Tell me about its architecture.\"\n",
    "response = rag_chain.invoke({'input' : question, 'chat_history' : chat_history})\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=response['answer'])\n",
    "])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac69a2d-fbf1-42b7-bee6-8f2a54f5bc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Model Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 0, 'total_pages': 12}),\n",
       " Document(page_content='3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 2401, 'total_pages': 12}),\n",
       " Document(page_content='tasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 779, 'total_pages': 12}),\n",
       " Document(page_content='KyleLo,LucyLuWang,MarkNeumann,RodneyKin- and VERification. In Proceedings of the 2018\\nney,andDanielWeld.2020. S2ORC:Thesemantic Conference of the North American Chapter of\\nscholaropenresearchcorpus. InProceedingsofthe the Association for Computational Linguistics:\\n58thAnnualMeetingoftheAssociationforCompu- Human Language Technologies, Volume 1 (Long\\ntationalLinguistics,pages4969–4983,Online.Asso- Papers), pages 809–819, New Orleans, Louisiana.\\nciationforComputationalLinguistics. AssociationforComputationalLinguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, RobertTinn,HaoCheng,YuGu,NaotoUsuyama,Xi-\\nDarioAmodei,andIlyaSutskever.2019. Language aodong Liu, Tristan Naumann, Jianfeng Gao, and\\nmodelsareunsupervisedmultitasklearners. HoifungPoon.2023. Fine-tuninglargeneurallan-\\nguagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 0, 'total_pages': 12})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12248edf-291b-4a63-89c9-dd08d09f0be4",
   "metadata": {},
   "source": [
    "Now we can see that it's easy to ask follow-up questions to our assistant by attaching a conversational chat history for it to refer before giving an answer\n",
    "\n",
    "But one problem with this system is the effort needed to attach each question-answer response to this chat history after each invocation call. \n",
    "\n",
    "So to counter this, we need to modify it in a way that chat history gets updated after every invocation automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5433e2-4612-41af-a29b-51f3848f8bf0",
   "metadata": {},
   "source": [
    "##### Adding message history using RunnableWithMessageHistory to store messages based on session\n",
    "##### For now, storing it in-memory using a dictionary with session id and an instance of BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51b6f377-4096-4983-84cc-248ed4f9ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='input',\n",
    "    history_messages_key='chat_history',\n",
    "    output_messages_key='answer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca6d4e0d-57a5-40cf-90a6-a7e50b25a070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'RunnableWithChatHistoryInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'input': {'title': 'Input',\n",
       "   'anyOf': [{'type': 'string'},\n",
       "    {'$ref': '#/definitions/BaseMessage'},\n",
       "    {'type': 'array', 'items': {'$ref': '#/definitions/BaseMessage'}}]}},\n",
       " 'required': ['input'],\n",
       " 'definitions': {'BaseMessage': {'title': 'BaseMessage',\n",
       "   'description': 'Base abstract Message class.\\n\\nMessages are the inputs and outputs of ChatModels.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type', 'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'type']}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27199cb1-00bc-49ed-83c0-c7c470dc1c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'RunnableSequenceOutput',\n",
       " 'type': 'object',\n",
       " 'properties': {'chat_history': {'title': 'Chat History',\n",
       "   'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'},\n",
       "     {'$ref': '#/definitions/ToolMessage'}]}},\n",
       "  'input': {'title': 'Input', 'type': 'string'},\n",
       "  'answer': {'title': 'Answer', 'type': 'string'}},\n",
       " 'definitions': {'ToolCall': {'title': 'ToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id']},\n",
       "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'error': {'title': 'Error', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
       "    'tool_calls': {'title': 'Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/ToolCall'}},\n",
       "    'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/InvalidToolCall'}}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c7bf38d-e154-4dc9-bad4-f0ee4673090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {'input': 'Summarize the research paper for me.'},\n",
    "    config={\n",
    "        'configurable': {\n",
    "            'session_id': 'chat1'\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de3adeaf-c553-4133-8fc1-c464befcc941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Summarize the research paper for me.',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='domains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-\\nwhichthenamedentitiesofinterestoriginatefrom\\nstracts with these annotated paragraphs. On an\\ncomplextaxonomiesusedinclimate-relatedlitera-\\naverage, each query is 12 words long, and each\\nture. Thisdatasetcomprises534abstractssourced\\nparagraphis120wordslong. WeusedRecall@10\\nfromSemanticScholarAcademicGraph(Kinney\\nasevaluationmetricsinceeachquestionhasonly\\netal.,2023),collectedusingaseedsetofclimate-\\nonerelevantdocument.\\nrelated keywords such as wildfire or floods. The\\nabstractswereannotatedusingtheIOB(inside,out-\\n6 ExperimentalResults\\nside,beginning)taggingschemeandencompasses\\nadiversearrayofentitytypes,asshowninTable4. Baselines Wecompared INDUS modelsagainst\\nopensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 2378, 'total_pages': 12}),\n",
       "  Document(page_content='dates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-\\ndata,comprehensiveoverviewsofclimatemodels\\nically, we sampled a set of 166 paragraphs from\\nandimpactassessmentsarescarceandnoteasily\\nAGU, AMS, ADS, PMC and Pubmed (§2) and\\naccessible.\\nmanually annotated with 3 questions that are an-\\nInordertobridgethisgap,weintroducedacom-\\nswerablefromeachoftheseparagraphs,resulting\\nprehensive dataset for developing and evaluating\\nin498questions. Weused398ofthesequestions\\nNLP models tailored towards understanding and\\nas the training set and the remaining 100 as the\\naddressing climate-related topics across various\\nvalidation set. To comprehensively evaluate the\\ndomains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 1589, 'total_pages': 12}),\n",
       "  Document(page_content='MINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text\\ninNER,PICO,relationextraction,anddocument\\ntasks. Thistrendwasconsistentwiththeobserva-\\nclassification tasks. This demonstrates the effec-\\ntionsofTinnetal.(2023).\\ntivenessofknowledgedistillationfromourdomain-\\nspecificteachermodel,INDUS . 6.2 CLIMATE-CHANGE NER\\nBASE\\nAs shown in Table 6, our models clearly outper-\\n15https://huggingface.co/BAAI/bge-base-en-v1.5\\n16sentence-transformers/all-MiniLM-L6-v2 formedthecorrespondingbaselinemodelsonthe\\n17https://huggingface.co/BAAI/bge-small-en-v1.5 CLIMATE-CHANGE NERtask,suggestingtheeffec-', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 3215, 'total_pages': 12}),\n",
       "  Document(page_content='opensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.\\nsweringtaskfocusedontheEarthsciencedomain. • INDUS was compared to MINILM (6-\\nSMALL\\nFirst, 39 paragraphs from Earth science papers\\nlayer)13 andTINYBERT(4-layer)14.\\nwhichappearedinAGUandAMSjournals(§2)were\\n10https://huggingface.co/datasets/nasa-impact/nasa-smd-\\nsourced. Subject matter experts from NASA for- IR-benchmark\\nmulatedquestionsandmarkedthecorresponding 11https://huggingface.co/FacebookAI/roberta-base\\n12https://huggingface.co/allenai/scibert_scivocab_uncased\\n8https://huggingface.co/datasets/ibm/Climate-Change- 13https://huggingface.co/nreimers/MiniLM-L6-H384-\\nNER uncased\\n9https://huggingface.co/datasets/nasa-impact/nasa-smd- 14https://huggingface.co/huawei-\\nqa-benchmark noah/TinyBERT_General_4L_312D', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 3221, 'total_pages': 12})],\n",
       " 'answer': 'The research paper introduces NASA-QA9, a new dataset for developing and evaluating NLP models tailored to understanding climate-related topics. This dataset is manually annotated with questions that can be answered from each paragraph, making it useful for information retrieval and question-answering tasks. The paper also introduces a comprehensive climate change dataset with complex taxonomies, which is useful for improving climate literature analysis. \\n\\nTo evaluate the datasets, the researchers compared the performance of various open-source models, including RoBERTa, SCIBERT, and MINILM, using metrics like Recall@10. The results show that a domain-specific teacher model, INDUS, outperforms the baseline models in most cases, suggesting the effectiveness of knowledge distillation in this domain. \\n\\nOverall, this work aims to bridge the gap in climate-related NLP research by providing new datasets and evaluation methods for developing better models to address climate change-related questions and analyze relevant literature.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ca20e69-8714-45f2-b6e5-1fc748b802cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper introduces NASA-QA9, a new dataset for developing and evaluating NLP models tailored to understanding climate-related topics. This dataset is manually annotated with questions that can be answered from each paragraph, making it useful for information retrieval and question-answering tasks. The paper also introduces a comprehensive climate change dataset with complex taxonomies, which is useful for improving climate literature analysis. \n",
      "\n",
      "To evaluate the datasets, the researchers compared the performance of various open-source models, including RoBERTa, SCIBERT, and MINILM, using metrics like Recall@10. The results show that a domain-specific teacher model, INDUS, outperforms the baseline models in most cases, suggesting the effectiveness of knowledge distillation in this domain. \n",
      "\n",
      "Overall, this work aims to bridge the gap in climate-related NLP research by providing new datasets and evaluation methods for developing better models to address climate change-related questions and analyze relevant literature.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3648d21e-3e44-4895-8342-df4dacce35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {'input': 'What is NASA-QA9? Give me all the details about this and how it is being used in the research paper?'},\n",
    "    config={\n",
    "        'configurable': {\n",
    "            'session_id': 'chat1'\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc43338d-8476-4e3a-b330-5cb871363102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is NASA-QA9? Give me all the details about this and how it is being used in the research paper?',\n",
       " \"NASA-QA9 is a large-scale dataset introduced in the research paper as a benchmark for climate-related question answering. It is designed to develop and evaluate extractive question-answering models that can comprehend and provide answers from climate change-related paragraphs. This dataset is a modification and enhancement of the previous NASA-QA dataset. \\n\\nHere are the key details about NASA-QA9 and its usage in the research:\\n1. Dataset Creation: NASA-QA9 is created by sampling paragraphs from various sources, including AGU, AMS, ADS, PMC, and PubMed. These paragraphs are manually annotated with questions that can be answered based on the information within each paragraph. \\n2. Sample Size: The dataset contains a total of 686 paragraphs, which are divided into a training set (398 paragraphs) and a validation set (288 paragraphs). Each paragraph is, on average, 120 words long. The questions associated with these paragraphs total 5,081, including 2,817 answerable questions and 2,264 unanswerable ones. \\n3. Question-Answering Task: The research aims to train extractive question-answering models that can provide concise answers to the posed questions based on the given paragraphs. The models should identify the relevant information within the text and extract the exact answer. \\n4. Evaluation: The performance of the models is evaluated using Recall@10, which measures whether the correct answer is present in the top 10 predictions of the model. This metric is chosen because each question in the dataset has only one relevant document. \\n5. Comparison with Baselines: The researchers compared the performance of their proposed INDUS models with other open-source baseline models, such as RoBERTa BASE, SCIBERT BASE, and MINILM, on the NASA-QA9 dataset. They used various techniques like knowledge distillation and contrastive learning to enhance the models' climate-related information retrieval and answering abilities. \\n6. Results: The results show that the INDUS models, especially the one trained with knowledge distillation, outperform the baseline models on the NASA-QA9 dataset. This indicates the effectiveness of domain-specific training and knowledge transfer in improving the models' performance on climate-related questions.\\n\\nOverall, NASA-QA9 serves as a valuable resource for training and evaluating climate change question-answering systems. By using this dataset, the researchers aim to advance the development of NLP models that can assist in understanding and analyzing climate change literature, ultimately contributing to the real-world data retrieval and analysis related to this complex topic.\")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(response['input'], response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1668d9ff-5f5d-4b34-9479-b5b84888f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {'input': 'What is the INDUS model? Cover all the details about it from its initiation phase, to architecture and all its specifications.'},\n",
    "    config={\n",
    "        'configurable': {\n",
    "            'session_id': 'chat1'\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8656957f-3a73-4f05-a5a2-c95af33b05df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the INDUS model? Cover all the details about it from its initiation phase, to architecture and all its specifications.',\n",
       " 'The INDUS model is a family of transformer-based language models developed specifically for scientific applications. The name \"INDUS\" stands for \"Effective and Efficient Language models for Scientific Applications.\" These models are designed to address the challenges of understanding and extracting information from scientific texts. \\n\\nHere\\'s a comprehensive overview of the INDUS model:\\n1. Initiation: The researchers started by curating a large corpus of scientific text, which included papers from various domains like computer science, biology, medicine, and climate change. This corpus was used to train the INDUS models, focusing on creating domain-specific representations. \\n2. Data Processing: The scientific corpus underwent pre-processing using a customized byte-pair encoding (BPE) tokenizer called INDUSBPE. This tokenizer is trained on the scientific corpus to handle the unique vocabulary and terminology present in scientific literature. \\n3. Model Training: The researchers trained multiple encoder-only language models using the processed scientific data. These models are based on the transformer architecture and are pretrained using masked language modeling (MLM) objectives. \\n4. Model Variants: Three variants of the INDUS model were introduced with different sizes:\\n   - INDUS BASE: This is the largest model, consisting of 110M parameters. It has 12 encoder layers, 768 hidden states, and 12 attention heads.\\n   - INDUS SMALL: A smaller version of the base model with 33M parameters, 6 encoder layers, 512 hidden states, and 8 attention heads.\\n   - INDUS TINY: The lightest version, featuring 6M parameters, 4 encoder layers, 384 hidden states, and 6 attention heads.\\n\\n5. Knowledge Distillation: To improve the efficiency of the models, the researchers employed knowledge distillation techniques. They used a teacher-student approach, where the larger INDUS BASE model acts as a teacher to guide the smaller variants (SMALL and TINY). This process involves transferring the knowledge and information gained by the larger model to the smaller ones, enabling them to perform better despite their reduced size. \\n6. Retrieval Model: In addition to the encoder-only models, the researchers also developed a retrieval-based model called INDUS-RETRIEVER. This model is designed to retrieve relevant information from scientific texts based on user queries. It uses the BASE encoder as a backbone and incorporates a separate component for retrieval.\\n\\n7. Evaluation Benchmarks: The INDUS models were evaluated on various benchmarks, including NASA-QA9, which we discussed earlier. Other benchmarks used for evaluation include CLIMATE-CHANGENER and NASA-IR. The models\\' performance was measured using metrics like Recall@10, F1 score, and latency. \\n8. Results: The INDUS models, especially the BASE variant, demonstrated superior performance on the scientific benchmarks compared to baseline models of similar sizes. The knowledge-distilled versions of SMALL and TINY models also showed relatively strong performance, maintaining high quality while being more lightweight and efficient. \\n9. Release of Models and Datasets: To benefit the scientific community, the researchers announced the release of the developed models and benchmark datasets on HuggingFace, making them accessible to other researchers and developers.\\n\\nOverall, the INDUS models are designed to excel at understanding and processing scientific literature. The research paper highlights the models\\' potential to bridge the gap between NLP tools and scientific applications, making them a valuable resource for tasks like question answering, information retrieval, and knowledge distillation in the domain of climate change and other scientific fields.')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(response['input'], response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "484c15ce-8475-48ba-ab23-16182c05253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The INDUS model is a pivotal component of the research, designed specifically for scientific domain tasks. Let's delve into the details of its development and architecture.\n",
      "\n",
      "Initiation Phase:\n",
      "INDUS was developed with a focus on creating a domain-specific language model tailored to scientific literature. The model's training leveraged a large corpus of scientific papers and articles, ensuring its exposure to the specific vocabulary, terminology, and linguistic nuances prevalent in this domain. \n",
      "\n",
      "This curated scientific corpus, used for training INDUS, is compiled from multiple sources, including arXiv, a repository of electronic preprints, and domain-specific datasets like PubMed and PubMed Central, which contain a vast collection of biomedical research articles.\n",
      "\n",
      "Architecture and Specifications:\n",
      "INDUS is built upon the foundation of the transformer-based architecture, sharing similarities with other state-of-the-art models like RoBERTa and BERT. It employs the encoder component of the transformer, comprising several layers. \n",
      "\n",
      "The model uses the byte-pair encoding (BPE) algorithm to handle the complex vocabulary of scientific literature. This algorithm breaks words down into subword units, enabling the model to capture a rich variety of scientific terms, even those absent during training. This flexibility is particularly advantageous when dealing with the rapidly evolving terminology in scientific domains.\n",
      "\n",
      "The researchers also introduced a customized tokenizer, INDUSBPE, tailored for the scientific language, which further enhances the model's performance. This tokenizer is trained using the large scientific corpus, ensuring that it captures the specificities of scientific text. \n",
      "\n",
      "During training, INDUS leverages a combination of two objectives:\n",
      "\n",
      "1. Masked Language Modeling (MLM): This is a standard objective for pre-training transformer models, where some tokens in the input are masked, and the model predicts them.\n",
      "\n",
      "2. Next Sentence Prediction (NSP): This objective functions to determine whether a given pair of sentences appear consecutively in the training data.\n",
      "\n",
      "To distil the acquired knowledge into smaller models suitable for resource-constrained applications, the researchers employed knowledge distillation techniques. This process created distilled versions of INDUS, namely INDUS SMALL and TINYBERT, which maintain strong performance while reducing latency.\n",
      "\n",
      "Evaluation and Benchmarking:\n",
      "The performance of INDUS is thoroughly evaluated using various benchmarks, including NASA-QA and CLIMATE-CHANGE NER, which we discussed earlier. These benchmarks assess the model's ability in named entity recognition, question answering, relation extraction, and document classification. \n",
      "\n",
      "The research paper reports that INDUS outperforms general-purpose models like RoBERTa and domain-specific models like SCIBERT on these benchmarks, highlighting its effectiveness in scientific language processing. \n",
      "\n",
      "Overall, the INDUS model is a domain-specific variation of the transformer architecture, trained and tailored for scientific text. Its unique design, incorporating the scientific corpus and domain-specific tokenizer, enables it to excel in understanding and processing scientific literature. The model's success in benchmarking against other state-of-the-art models underscores its potential in facilitating climate-related and other scientific NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bfe13-b974-43e1-8f8d-554930ab7305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
