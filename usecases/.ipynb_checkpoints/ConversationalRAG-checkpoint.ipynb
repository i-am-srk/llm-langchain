{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12ddb519-7769-4c2c-adfa-6ebc85046af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2035f07-5aca-4662-ba6f-ada9372d1ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['COHERE_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dea3f2-57e9-4171-9d54-f147967517ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_api_key = os.environ['COHERE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cbbc74-fd73-4ad0-b830-1e8458dc02a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6930464-fc8a-49d8-8436-d40db2436ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFPlumberLoader('https://arxiv.org/pdf/2405.10725')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513b3510-3669-4eb5-a52c-2e43e2145b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='INDUS: Effective and Efficient Language Models for Scientific Applications\\nBishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\\nMuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\\nBharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\\nElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\\nThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\\nDavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\\nPanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\\nArminMehrabian9,TsendgarLee7\\n1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\\n7 NASAHQ,8 JPL,9 NASAGSFC\\nAbstract generation tasks. Most popular LLMs rely on the\\ntransformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-\\nlike Wikipedia or CommonCrawl (Devlin et al.,\\nsults on natural language processing (NLP)\\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\\ntasks. However, previous research demon-\\net al., 2020; Brown et al., 2020; Touvron et al.,\\nstratedLLMstrainedusingdomain-focusedcor-\\npora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\\nspired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\\nINDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\\nfortheEarthscience,biology,physics,helio- domain-specific natural language understanding\\nphysics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific\\ncorporadrawnfromdiversedatasources. The\\nLLMs such as SCIBERT (Beltagy et al., 2019),\\nsuiteofmodelsinclude: (1)anencodermodel\\ntrainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\\ncorpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\\nstandingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\\ngeneral text embedding model trained using developedwiththegoalofimprovingaccuracyon\\na diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\\nple sources to address information retrieval\\nWuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-\\ndisciplinaryfieldsrelatedtotheEarth,celestialbod-\\ntencyorresourceconstraints. Wealsocreated\\nies, the Sun, and planets within our solar system\\nthreenewscientificbenchmarkdatasetsnamely,\\nsuchasphysics,Earthscience,astrophysics,helio-\\nCLIMATE-CHANGE NER (entity-recognition),\\nNASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\\naccelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\\nfields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\\nelsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\\n(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the\\nasexistingbenchmarktasksinthedomainsof\\ninterdisciplinarynatureofthesedomainsofinter-\\ninterest.\\nestisreflectedinavastbodyofliteraturescattered\\n1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\\na collection of encoder-based LLMs focused on\\nLarge language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\\namountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\\npabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\\nzationsandenterprisesworkinginthesefieldsby\\nContact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\\nmuthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\\n4202\\nyaM\\n02\\n]LC.sc[\\n2v52701.5042:viXra\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Earth Science Data Pretraining\\n(Masked Finetuning Scientific Corpora\\nBioMedical Data Language Indus-Base (Contrastive\\nModelling) Learning) Open QA\\nAstrophysics Data\\nDuplicate Pairs\\nAstronomy Data\\nRepresentation Indus-Retriever-Base Citation Pairs\\nGeneral Science Distillation\\nData\\nFact Verification\\nGeneral English\\nData\\nGeneric Corpora\\nOutput\\nEncoder Training Corpus Indus-Small Distillation Embedding Training\\nCorpus\\nData Teacher (KD)\\nInitialization Output Indus-Retriever-\\nSmall\\nBC5-CHEM BC5-Disease NCBI-Disease BC2GM\\nNASA-QA TREC-Covid NFCorpus NQ HotpotQA\\nJNLPBA EBM-PICO ChemProt DDI\\nClimate FiQA Arguana Touche DBPedia NASA-IR\\nGAD HoC PubMedQA BioASQ Change NER\\nSciDocs FEVER C Fl Eim VEat Re SciFact\\nBIOSSES BLURB Benchmark BEIR Benchmark\\nNatural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir\\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\\nNASA-QA,CLIMATE-CHANGENERandNASA-IR.\\nenablingthemininformeddecision-making. (Beltagy et al., 2019). We also show that the\\nSpecifically, we make the following contribu- knowledge-distilledmodelsachievedasignifi-\\ntions: cantboostinlatencywhilemaintainingstrong\\n1. Utilizing the byte-pair encoding algorithm, empiricalperformancecomparedtotheoriginal\\nwe constructed INDUSBPE, a customized to- modelsonmostofthebenchmarktasks.\\nkenizerfromthecuratedscientificcorpus.\\n2 Data\\n2. Wepretrainedmultipleencoder-onlyLLMsus-\\ning the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than\\nsentence-embeddingmodelsbyfine-tuningthe\\ntheircounterpartstrainedonopen-domaincorpora.\\nencoder-only models with a contrastive learn-\\nWemeticulouslyidentifiedcorporaforeachofthe\\ningobjectivetolearn“universal”sentenceem-\\naforementioneddomains,andcreatedEnglish-only\\nbeddings (Gao et al., 2021) (§4). We also\\nmodels for the sake of containment. Specifically,\\ntrained smaller, more efficient versions of\\nforeachofthedomains,weusedopen-sourcedata\\nthesemodelsusingknowledge-distillationtech-\\nwhich has a permissive license, and further aug-\\nniques(§3.3,§4.2).\\nmentedthemwithspecificdatafromNASAandits\\n3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in\\nrecognitiontask),NASA-QA(anextractiveques-\\nourtrainingcorpora. Webrieflydescribeeachdata\\ntionansweringtask)and NASA-IR (aretrieval\\nsourcebelow,andpresentstatisticsofthedatain\\ntask)(§5)tofurtheraccelerateresearchinthis\\nTable1.\\nmulti-disciplinaryfield.\\n• SAO/NASAAstrophysicsDataSystem(ADS)1:\\n4. Throughexperimentalresults,weshowstrong\\nADSisthebiggestsourceofdata,coveringpub-\\nperformancebyourmodelsonthesebenchmark\\nlications in the areas of astronomy and astro-\\ntasks as well as on existing domain-specific\\nphysics,physicsandgeneralscienceincluding\\nbenchmarks, outperforming general-purpose\\nallarXive-prints.\\nmodels like RoBERTa (Liu et al., 2019) as\\nwellasscientific-domainencoderslikeSCIBERT 1https://ui.adsabs.harvard.edu\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Dataset Domain #Tokens Ratio Tokenizer ADS PMC Wikipedia\\nNASACMR EarthScience 0.3B 1% RoBERTa 12,867,439 7,549,075 15,859\\nAMSandAGUpapers EarthScience 2.8B 4% +lower_cased 12,862,227 7,557,868 16,901\\nEnglishWikipedia General 5.0B 8% INDUSBPE 12,309,023 6,920,659 16,056\\nPubMedAbstracts Biomedical 6.9B 10%\\nTable2: NumberoftokensproducedbyRoBERTaand\\nPMC Biomedical 18.5B 28%\\nINDUSBPEtokenizersappliedto1ksamplesfromeach\\nSAO/NASAADS Astronomy, 32.7B 49%\\ndataset. Fewer tokens lead to a smaller computation\\nAstrophysics,\\ncost.\\nPhysics,\\nGeneralScience\\nTotal 66.2B 100% ingdataset(§2)6. Forafaircomparison,wesetthe\\nvocabularysizeto50,265,whichisequaltothatof\\nTable1: Basicstatisticsofourpretrainingdataset.\\nthe RoBERTatokenizer(Liuetal.,2019)andused\\ntheuncasedvariationofboththetokenizers.\\n• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-\\nliterature maintained by National Library of BPE and the RoBERTa tokenizer. Out of 50,265\\nMedicineandNationalInstitutesofHealth. We tokens,22,355(44.5%)tokensarecommoninboth\\nusedtheportionofPMCthathasacommercial- thetokenizerswhiletheremaining27,910(55.5%)\\nfriendly license, along with the PubMed ab- tokensareincludedonlyineithertokenizer, indi-\\nstractsofallthearticlesinPMC. catingasignificantdistributionalshiftindomain.\\n• American Meteorological Society (AMS)3: Tofurther understandthe effect, weapplied both\\nWeusedfull-textjournaldocumentsspanning RoBERTaandINDUSBPEon1,000randomlysam-\\ntopicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts\\noceanography, atmospheric sciences, climate, tosinglesentences. AsshowninTable2, INDUS-\\nhydrometeorology, weather and forecasting, BPE tokenizer produced fewer tokens than the\\nandsocietalimpacts. RoBERTa tokenizer, leading to 8˜% drop in com-\\n• American Geophysical Union (AGU)4: The putationcostduringtraining.\\nAGUdatasetincludedjournaldocumentsacross Table3comparestheRoBERTatokenzierandIN-\\nthe topics of atmospheres, biogeosciences, DUSBPE tokenizer,illustratingthattheproposed\\nEarth surface, machine learning and compu- tokenizertreatedscientificterms(suchasbiomak-\\ntation, oceans, planets, solid earth, and space ers, phosphorylated, alzheimer) as single tokens\\nphysics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-\\n3.2 EncoderModel\\nquality,continuouslyevolvingmetadatasystem\\nthat catalogs all data and service metadata Wefirsttrainedanencoder-onlymodel,INDUS ,\\nBASE\\nrecords for NASA’s Earth Science Data and usingamaskedlanguagemodelingobjective. The\\nInformation System (ESDIS). It contains text modelarchitecturefollowsRoBERTa (Liuetal.,\\nBASE\\ndescriptions of the NASA Earth science data 2019),whichconsistsof12layersandhas125M\\nproducts. parameters. Weadoptedthedefaulthyperparame-\\nters7 butwithaneffectivebatchsizeof92,16. We\\n3 Methodology: EncoderModels trainedthemodelfor500Kstepsusing192V100\\nGPUs.\\n3.1 INDUSBPE Tokenizer\\n3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL\\n2https://www.ncbi.nlm.nih.gov/pmc 38M parameters through knowledge distillation\\n3https://www.ametsoc.org/index.cfm/ams/publications/\\n4https://agupubs.onlinelibrary.wiley.com/ 6We used HF tokenizers, https://github.com/\\n5https://www.earthdata.nasa.gov/eosdis/science-system- huggingface/tokenizers\\ndescription/eosdis-components/cmr 7WereferreaderstoTable9in(Liuetal.,2019).\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Inputtext\\nnoveltaubiomarkersphosphorylatedatt181,t217ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected\\nTokenizationbyRoBERTa\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTokenizationby INDUSBPE\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTable3: TokenizationcomparisonbetweenRoBERTaandourtokenizers. InputtextadaptedfromSuárez-Calvet\\netal.(2020).\\n(cid:88) (cid:88)\\ntechniquesbyusingINDUS\\nBASE\\nastheteacher. IN- Z\\ni\\n= es(qi,pj)+ es(qj,pi)\\nDUS follows a 4-layer architecture recom- j j\\nSMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the\\nwheres(q,p)isameasureoftemperature-scaled\\ndistillationobjectiveproposedinMiniLMv2(Wang\\ncosinesimilaritybetweentheembeddingsofquery\\netal.,2021)totransferfine-grainedself-attention\\nandapassagemeasuredby:\\nrelations,whichhasbeenshowntobethecurrent\\nstate-of-the-art(Udagawaetal.,2023). Usingthis 1 E(q)·E(p)\\ns(q,p) = (3)\\nobjective,wetrainedthemodelfor500Kstepswith τ ∥E(q)∥∥E(p)∥\\naneffectivebatchsizeof480on30V100GPUs.\\nTrainingData Similartopriorwork(Wangetal.,\\n2022; Li et al., 2023; Xiao et al., 2023), we em-\\n4 Methodology: SentenceEmbedding\\nployedastage-wisetrainingapproachforoursen-\\nModels\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional\\noccurringpairscollectedfrominternetsources,\\nvectors,allowingforefficientuseindenseretrieval\\nsuch as Wikipedia, StackExchange, etc. We\\nsystems,whererelevantpassagesforagivenquery\\nalsoincludedscientificdatafromPubMed,PMC\\nareidentifiedonthebasisofthesimilaritybetween\\n(§2),Arxivand S2ORC (Loetal.,2020)asin-\\ntheirembeddings(Karpukhinetal.,2020).\\ndomaindataforourscience-orientedretriever\\nmodel. Furthermore, we created a domain-\\nContrastiveLearningObjective Sentenceem-\\nspecific dataset from the ADS data (§2) by in-\\nbeddingmodelstrainedusingacontrastivelearn-\\ncludingtitle-abstractpairs.\\ningobjective(Khoslaetal.,2020;Gaoetal.,2021)\\n2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa\\n(Kwiatkowskietal.,2019),SQuAD(Rajpurkar\\nnon-relevantpassage.\\net al., 2016), SPECTER pairs (Cohan et al.,\\nInspired by recent work (Li et al., 2023), we\\n2020), etc. We included the aforementioned\\nusedanimprovedcontrastivelossbyintroducing\\nADS data and a sample of the S2ORC data in\\nanadditionalbidirectionalsignal. Specifically,for\\nthisstep,toboostdomain-specificsignals.\\na triple {q,p+,P−} of a query, a relevant (posi-\\nAppendixAcontainscomprehensivedetailsabout\\ntive)passage,andasetofnon-relevant(negative)\\nthe datasets used in training. For both training\\npassagesP− = {p−}m ,WedefinetheInfoNCE\\nj j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches\\n1 (cid:88)n es(qi,p+ i ) from each data source proportionately to its size,\\nL = − log (1)\\nIC\\nn Z i similartoLietal.(2023).\\ni=1\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Model Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K\\ninthelargermodelbutimprovestheperformance\\nstepsfollowedbyanadditional100Kstepswiththe\\nofthesmallermodel.\\nsuperviseddata. Weusedalearningrateof2e−5\\nFor distilling the sentence embedding model,\\nduringboththesesteps.\\nwefoundthatastage-wisetrainingapproachdoes\\n4.2 KnowledgeDistillationforEmbedding not benefit performance as much as in the non-\\nModels distillation case (ablation presented in Appendix\\nB). We thus distilled in a single step with all the\\nTooptimizethelatencyforretrievalapplications,\\ndatadescribedin§4.1andAppendixAandadded\\nwe also created a small retriever model with the\\nlabelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE\\nmodel(INDUS SMALL),bydistillingtheteacher’sdis- ModelSpecifications Webuiltthesentenceem-\\ntributionofsimilarityscores. Furthermore,wefind beddingmodelbydistillinginto INDUS . This\\nSMALL\\nthatitisnecessarytomodifythetrainingstrategy isa4-layermodelwithanembeddingdimension\\nfordistillation,asdescribedbelow. of576. Werefertotheresultingretrievermodelas\\nDistillationLoss Weusedknowledgedistillation\\nINDUS-RETRIEVER SMALL. Itfollowsabi-encoder\\nframework,andherewefindthatusingthevector\\ntechniquesintroducedin(Xuetal.,2023). Specif-\\nrepresentationofthefirsttokenastheembedding\\nically, for a sentence x and its corresponding in-\\ni\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-\\ntionp ofsimilarityscoresbetweenpairsandthe\\nt TrainingDetails FortheRetro-MAEstylepre-\\nstudent’sdistribution, p . FollowingHintonetal.\\ns training(Xiaoetal.,2022),wetrainedon8A100\\n(2014), we also scaled the output distribution of\\nGPUs with an effective batch size of 128 for 2\\nbothteacherandstudentbyatemperature,τ :\\nKD epochswithalearningrateof2e−5. Forthestage-\\nn m wisedistillation,wetrainedon2A100GPUsfor\\n(cid:88)(cid:88)\\nL = − p (x ,x )logp (x ,x ) (4) 300K steps with an effective batch size of 2,048,\\nKD t i j s i j\\ni=1 j=1 andlearningrateof7e−4. Throughexperimenta-\\ntion,Wefoundthatτ = 4performedthebest.\\nKD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)\\nt i j (cid:80)m est(xi,x k)/τKD ingthelanguageunderstandingcapabilitiesmodels.\\nk=1\\nHowever,tothebestourknowledge,thereisano-\\nHere,s (x ,x )ands (x ,x )representthesim-\\ns i j t i j\\nticeableabsenceofdatasetstailoredforthediverse\\nilarityscoresbetweentwopairs{x ,x },definedin\\ni j\\nand multidisciplinary field under study. Thus, to\\nEquation3forthestudentandteacherrespectively.\\neffectively benchmark the proposed NLP models\\nTrainingData Wefirstconductedaembedding- and further accelerate research in this multidisci-\\noriented pretraining step, as presented in Retro- plinarydomain,Weintroducedthreenewdatasets,\\nMAE (Xiao et al., 2022), on English Wikipedia, anNERtask,aQAtask,andanIRtask,described\\nBooksCorpus,andStackExchangedata,totalling below.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Train Validation Test spansoftheparagraphwhichanswerthequestions.\\nNum.Abstracts 382 77 75\\nWeused29paragraphs(with145QApairsintotal)\\nNum.Tokens 32,031 6,443 5,850\\nEntityLabels asthetrainingsetandtheremaining10paragraphs\\nclimate-nature,climate-greenhouse-gases,climate-assets,\\n(with 50 questions in total) as the evaluation set.\\nclimate-problem-origins,climate-mitigations,\\nclimate-properties,climate-impacts,climate-datasets, Thetrainingsetwasfurtheraugmentedwithpara-\\nclimate-organizations,climate-observations,\\ngraphsand QA pairsrelatedtoEarthsciencefrom\\nclimate-models,climate-hazards,climate-organisms\\nthe SQuADdataset(Rajpurkaretal.,2018). Specif-\\nTable4: CLIMATE-CHANGE NER statisticsandentity ically,thoserelatedtooxygen,Amazonrainforest\\nlabels\\nandgeologywereused. Thisresultedinapruned\\nSQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-\\nby augmenting these SQuAD pairs to the training\\nfer some assistance in exploring data related to\\ndatasourcedfromEarthsciencepapers,whilekeep-\\nclimatechange,thecomplexityofclimate-related\\ningtheevaluationsetintact.\\nqueries often requires more sophisticated natural\\nlanguageprocessingtechniques. Thisnecessityis 5.3 NASA-IR\\nunderscoredbytheextensivearrayofclimatemod-\\nWe introduced a domain-specific information re-\\nels, datasets, and organizations involved, which\\ntrieval benchmark, NASA-IR10, spanning almost\\ndemand meticulous curation and continuous up-\\n500question-answerpairsrelatedtotheEarthsci-\\ndates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-\\ndata,comprehensiveoverviewsofclimatemodels\\nically, we sampled a set of 166 paragraphs from\\nandimpactassessmentsarescarceandnoteasily\\nAGU, AMS, ADS, PMC and Pubmed (§2) and\\naccessible.\\nmanually annotated with 3 questions that are an-\\nInordertobridgethisgap,weintroducedacom-\\nswerablefromeachoftheseparagraphs,resulting\\nprehensive dataset for developing and evaluating\\nin498questions. Weused398ofthesequestions\\nNLP models tailored towards understanding and\\nas the training set and the remaining 100 as the\\naddressing climate-related topics across various\\nvalidation set. To comprehensively evaluate the\\ndomains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-\\nwhichthenamedentitiesofinterestoriginatefrom\\nstracts with these annotated paragraphs. On an\\ncomplextaxonomiesusedinclimate-relatedlitera-\\naverage, each query is 12 words long, and each\\nture. Thisdatasetcomprises534abstractssourced\\nparagraphis120wordslong. WeusedRecall@10\\nfromSemanticScholarAcademicGraph(Kinney\\nasevaluationmetricsinceeachquestionhasonly\\netal.,2023),collectedusingaseedsetofclimate-\\nonerelevantdocument.\\nrelated keywords such as wildfire or floods. The\\nabstractswereannotatedusingtheIOB(inside,out-\\n6 ExperimentalResults\\nside,beginning)taggingschemeandencompasses\\nadiversearrayofentitytypes,asshowninTable4. Baselines Wecompared INDUS modelsagainst\\nopensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.\\nsweringtaskfocusedontheEarthsciencedomain. • INDUS was compared to MINILM (6-\\nSMALL\\nFirst, 39 paragraphs from Earth science papers\\nlayer)13 andTINYBERT(4-layer)14.\\nwhichappearedinAGUandAMSjournals(§2)were\\n10https://huggingface.co/datasets/nasa-impact/nasa-smd-\\nsourced. Subject matter experts from NASA for- IR-benchmark\\nmulatedquestionsandmarkedthecorresponding 11https://huggingface.co/FacebookAI/roberta-base\\n12https://huggingface.co/allenai/scibert_scivocab_uncased\\n8https://huggingface.co/datasets/ibm/Climate-Change- 13https://huggingface.co/nreimers/MiniLM-L6-H384-\\nNER uncased\\n9https://huggingface.co/datasets/nasa-impact/nasa-smd- 14https://huggingface.co/huawei-\\nqa-benchmark noah/TinyBERT_General_4L_312D\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Basemodel(125Mparams.) Smallmodel(∼30Mparams.)\\nTask Metric Dataset RoBERTa SCIBERT INDUSBASE TINYBERT MINILM INDUSSMALL\\nBC5-chem 90.3(0.2) 91.4(0.2) 93.3(0.2) 84.6(0.2) 86.1(0.3) 90.7(0.1)\\nBC5-disease 81.5(0.3) 83.7(0.3) 85.2(0.3) 74.0(0.4) 77.4(0.3) 81.3(0.3)\\nNER EntityF1 NCBI-disease 87.6(0.6) 87.6(0.4) 88.3(0.4) 81.2(0.4) 83.1(0.5) 85.6(0.6)\\nBC2GM 82.1(0.3) 82.3(0.2) 84.0(0.3) 74.7(0.4) 77.1(0.2) 79.7(0.3)\\nJNLPBA 79.1(0.2) 78.2(0.2) 80.3(0.2) 70.3(0.2) 73.4(0.3) 75.7(0.2)\\nPICO MacroF1 EBMPICO 72.3(0.3) 72.4(0.3) 73.1(0.2) 67.4(0.2) 70.3(0.1) 73.1(0.2)\\nChemProt 50.4(28.2) 73.9(0.7) 76.9(0.5) 56.2(3.2) 55.9(2.1) 71.7(0.9)\\nRelation\\nMicroF1 DDI 78.6(1.5) 80.1(1.0) 81.7(0.5) 39.3(5.3) 51.5(2.9) 69.0(1.2)\\nExtraction\\nGAD 80.0(1.1) 81.6(1.2) 79.4(5.6) 76.4(1.3) 77.3(1.0) 81.3(0.7)\\nDocument\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy\\nAnswering BioASQ 69.1(4.8) 74.6(4.5) 69.6(5.8) 74.3(3.6) 66.7(2.3) 75.4(3.3)\\nSentence\\nSimilarity Pearson BIOSSES 79.8(6.3) 86.3(3.5) 72.2(9.5) 88.2(1.1) 26.6(8.7) 70.4(3.3)\\nMicroAverage - - 75.9(3.7) 79.2(1.3) 78.9(2.4) 67.6(1.9) 66.1(1.9) 76.2(1.0)\\nMacroAverage - - 74.9(3.7) 78.2(1.6) 76.4(3.2) 65.6(2.4) 60.6(3.0) 74.3(1.3)\\nTable5:EvaluationresultsonBLURB.Resultsreportedareaveragedon10randomseedswithstandarddeviationin\\nparenthesis. Microaverageisreportedacrossdatasetswhilemacroaverageiscomputedbyfirstaveragingscoreson\\neachtask(say,taskaverage),followedbyaveragingthetaskaverageacrosstasks. Resultsinboldindicatehighest\\nperformancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE\\nBGE 15 andaRoBERTa modelfinetuned RoBERTa 60.8(0.8)\\nBASE BASE\\nwiththesamemethodpresentedin§4.1. SCIBERT 61.8(0.7)\\n• INDUS-RETRIEVER\\nSMALL\\nwas compared to INDUS\\nBASE\\n64.0(1.0)\\nMINILM-V216 and BGE 17. TINYBERT 34.3(1.6)\\nSMALL\\nMINILM 44.7(1.3)\\n6.1 NaturalLanguageUnderstanding INDUS 54.8(0.8)\\nSMALL\\nBenchmarks\\nTable 6: CLIMATE-CHANGE NER benchmark results.\\nWe evaluated our models on BLURB (Gu et al.,\\nStandard deviation over 10 random seeds shown in\\n2021),abenchmarksuitefornaturallanguageun-\\nparenthesis. Resultsinboldandunderlineindicatehigh-\\nderstandingandreasoningtasksinthebiomedical estperformanceandsignificantdifferencefromsecond\\ndomain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.\\nTable 5 shows the evaluation results. Among\\nbase models, INDUS BASE significantly outper- WealsonoticedSCIBERTtendstoperformbetter\\nformsthegeneral-purposeRoBERTamodelonmi-\\nthan our model on paired input-text tasks, such\\ncro/macroaveragewhileachievingcompetitiveper-\\nas QA and semantic similarity tasks, although\\nformance to the bio-domain-specific counterpart,\\nthe results have relatively large standard devia-\\nSCIBERT.\\ntions. We hypothesized that the additional next\\nAsforsmallermodels,wenoticed INDUS SMALL sentence prediction objective during training in\\noutperformed the baselines, TINYBERT and BERT-stylemodels(suchasSCIBERT)incontrastto\\nMINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text\\ninNER,PICO,relationextraction,anddocument\\ntasks. Thistrendwasconsistentwiththeobserva-\\nclassification tasks. This demonstrates the effec-\\ntionsofTinnetal.(2023).\\ntivenessofknowledgedistillationfromourdomain-\\nspecificteachermodel,INDUS . 6.2 CLIMATE-CHANGE NER\\nBASE\\nAs shown in Table 6, our models clearly outper-\\n15https://huggingface.co/BAAI/bge-base-en-v1.5\\n16sentence-transformers/all-MiniLM-L6-v2 formedthecorrespondingbaselinemodelsonthe\\n17https://huggingface.co/BAAI/bge-small-en-v1.5 CLIMATE-CHANGE NERtask,suggestingtheeffec-\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='tivenessoftrainingonlargedomain-specificdata. Model NASA-IR↑ BEIRAvg.↑ Retrieval\\nTime↓\\n6.3 NASA-QA BRo GB EE BAR ST Ea BASE 0 0. .6 66\\n7\\n0 0. .3 57\\n2\\n1 1. .2 10\\n8\\nAs mentioned in §5, we augmented the training\\nINDUS-RETRIEVERBASE 0.71 0.41 1.19\\nMINILM-V2 0.62 0.39 0.24\\nsetwithrelevantSQuADpairsforfine-tuning. All BGESMALL 0.66 0.51 0.42\\nmodelsarefinetunedfor15epochs,andtheresults INDUS-RETRIEVERSMALL 0.73 0.42 0.26\\nareshowninTable7. WeobservedthatINDUS\\nBASE Table 8: Evaluation results on NASA-IR and BEIR.\\noutperformedallmodelsofsimilarsizes,while IN-\\nNASA-IRshowedRecall@10whileBEIRreportedthe\\nDUS hadrelativelystrongperformancecom- averagenDCG@10acrossalltasks. Retrievaltimeper\\nSMALL\\nparedtoitscounterparts. queryontheNQtaskfromBEIR,reportedinseconds.\\nModel F1(SD)\\n7 Conclusions\\nRoBERTa 66.8(3.1)\\nSCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-\\nMINILM 59.2(3.9) enizer and in-domain data for training high qual-\\nINDUS 47.4(1.8) ityencodermodelsandsentenceembeddingmod-\\nSMALL\\nels. Further, we created smaller versions of the\\nTable7: NASA-QA benchmarkresults. Standarddevi- proposedmodelssuitableforapplicationswithla-\\nation over 3 random seeds shown in parenthesis. Re-\\ntencyorresourceconstraintsthroughstate-of-the-\\nsultsinboldandunderlineindicatehighestperformance\\nartknowledgedistillationtechniques. Fortheben-\\nandsignificantdifferencefromsecondhighestresultby\\nefit of the scientific community, we will release\\nmorethantwostandarddeviationsineachmodelsize,\\nthedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.\\nDoguAraci.2019. Finbert: Financialsentimentanaly-\\nsiswithpre-trainedlanguagemodels.\\n6.4 InformationRetrievalBenchmarks\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nWeevaluatedourmodelsonthe NASA-IR dataset\\nERT:Apretrainedlanguagemodelforscientifictext.\\naswellasBEIRBenchmark(Thakuretal.,2021), InProceedingsofthe2019ConferenceonEmpirical\\nwhichconsistsof12retrievaltasksspanningavari- Methods in Natural Language Processing and the\\n9thInternationalJointConferenceonNaturalLan-\\netyofdomains. TheBEIRbenchmarkusedtheNor-\\nguageProcessing(EMNLP-IJCNLP),pages3615–\\nmalized Cumulative Discount Gain (nDCG@10)\\n3620,HongKong,China.AssociationforComputa-\\n(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\ntenceembeddingmodels,alongwithourbaselines.\\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\\nAs shown, both of our sentence embedding mod- Neelakantan,PranavShyam,GirishSastry,Amanda\\nelssignificantlyoutperformedthebaselinesonthe Askell, Sandhini Agarwal, Ariel Herbert-Voss,\\nNASA-IRtaskwhilestillmaintaininggoodperfor- Gretchen Krueger, Tom Henighan, Rewon Child,\\nAdityaRamesh,DanielZiegler,JeffreyWu,Clemens\\nmanceonseveraloftheBEIRtasks. (Wepresented\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nresultsforeachBEIRtaskinAppendixC).\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nWe also measured the average time per query Clark, ChristopherBerner, SamMcCandlish, Alec\\nforretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-\\nquestionssetofBEIR,onasingleA100GPU.This\\nvances in Neural Information Processing Systems,\\ntime includes the time to encode the query, cor-\\nvolume 33, pages 1877–1901. Curran Associates,\\npus,andtimetoretrieverelevantdocuments. No- Inc.\\ntably, INDUS-RETRIEVER outperformed IN-\\nSMALL Colin B. Clement, Matthew Bierbaum, Kevin P.\\nDUS-RETRIEVER BASE,onbothNASA-IRandBEIR,\\nO’Keeffe, and Alexander A. Alemi. 2019. On the\\nwhilebeingabout4.6xfaster. useofarxivasadataset.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Sarna, Yonglong Tian, Phillip Isola, Aaron\\nDowney, and Daniel S. Weld. 2020. SPECTER: Maschinot, CeLiu, andDilipKrishnan.2020. Su-\\nDocument-level Representation Learning using pervisedcontrastivelearning. InAdvancesinNeural\\nCitation-informedTransformers. InACL. InformationProcessingSystems,volume33,pages\\n18661–18673.CurranAssociates,Inc.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of Rodney Kinney, Chloe Anastasiades, Russell Authur,\\ndeepbidirectionaltransformersforlanguageunder- Iz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\\nstanding. InProceedingsofthe2019Conferenceof ski,IsabelCachola,StefanCandra,YoganandChan-\\ntheNorthAmericanChapteroftheAssociationfor drasekhar, Arman Cohan, Miles Crawford, Doug\\nComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-\\n4171–4186,Minneapolis,Minnesota.Associationfor ham, Fangzhou Hu, Regan Huff, Daniel King, Se-\\nComputationalLinguistics. bastianKohlmeier,BaileyKuehl,MichaelLangan,\\nDaniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\\nMatthewDunn,LeventSagun,MikeHiggins,V.Ugur Kelsey MacMillan, Tyler Murray, Chris Newell,\\nGuney, Volkan Cirik, and Kyunghyun Cho. 2017. Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\\nSearchqa: Anewq&adatasetaugmentedwithcon- Shen,AmanpreetSingh,LucaSoldaini,Shivashankar\\ntextfromasearchengine. Subramanian,AmberTanaka,AlexD.Wade,Linda\\nWagner, Lucy Lu Wang, Chris Wilhelm, Caroline\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic\\ntractedknowledgebases. InProceedingsofthe20th scholaropendataplatform.\\nACMSIGKDDInternationalConferenceonKnowl-\\nedge Discovery and Data Mining, KDD ’14, page TomKwiatkowski, JennimariaPalomaki, OliviaRed-\\n1156–1165, New York, NY, USA. Association for field,MichaelCollins,AnkurParikh,ChrisAlberti,\\nComputingMachinery. DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\\ntonLee,KristinaToutanova,LlionJones,Matthew\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nSimCSE:Simplecontrastivelearningofsentenceem- Uszkoreit,QuocLe,andSlavPetrov.2019. Natural\\nbeddings. In Proceedings of the 2021 Conference Questions: A Benchmark for Question Answering\\nonEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-\\nJinhyukLee,WonjinYoon,SungdongKim,Donghyeon\\nminican Republic. Association for Computational\\nKim,SunkyuKim,ChanHoSo,andJaewooKang.\\nLinguistics.\\n2019. BioBERT:apre-trainedbiomedicallanguage\\nrepresentation model for biomedical text mining.\\nYuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto\\nBioinformatics,36(4):1234–1240.\\nUsuyama,XiaodongLiu,TristanNaumann,Jianfeng\\nGao,andHoifungPoon.2021. Domain-specificlan-\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nguagemodelpretrainingforbiomedicalnaturallan-\\nGhazvininejad,AbdelrahmanMohamed,OmerLevy,\\nguageprocessing. ACMTrans.Comput.Healthcare,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\n3(1).\\nBART:Denoisingsequence-to-sequencepre-training\\nfornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In\\ningoftheAssociationforComputationalLinguistics,\\nNeurIPSDeepLearningWorksop.\\npages7871–7880,Online.AssociationforComputa-\\ntionalLinguistics.\\nZhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon\\nDuede,KyleChard,andIanFoster.2023. Thedimin- PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-\\nishingreturnsofmaskedlanguagemodelstoscience. ervini,HeinrichKüttler,AleksandraPiktus,Pontus\\nStenetorp,andSebastianRiedel.2021. PAQ:65Mil-\\nShuHuangandJacquelineMCole.2022. Batterybert:\\nlionProbably-AskedQuestionsandWhatYouCan\\nA pretrained language model for battery database\\nDoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,\\nVladimirKarpukhin,BarlasOguz,SewonMin,Patrick Pengjun Xie, and Meishan Zhang. 2023. Towards\\nLewis,LedellWu,SergeyEdunov,DanqiChen,and generaltextembeddingswithmulti-stagecontrastive\\nWen-tauYih.2020. Densepassageretrievalforopen- learning.\\ndomainquestionanswering. InProceedingsofthe\\n2020ConferenceonEmpiricalMethodsinNatural YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-\\nLanguageProcessing(EMNLP),pages6769–6781, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nOnline.AssociationforComputationalLinguistics. Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron proach. arXivpreprintarXiv:1907.11692.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='KyleLo,LucyLuWang,MarkNeumann,RodneyKin- and VERification. In Proceedings of the 2018\\nney,andDanielWeld.2020. S2ORC:Thesemantic Conference of the North American Chapter of\\nscholaropenresearchcorpus. InProceedingsofthe the Association for Computational Linguistics:\\n58thAnnualMeetingoftheAssociationforCompu- Human Language Technologies, Volume 1 (Long\\ntationalLinguistics,pages4969–4983,Online.Asso- Papers), pages 809–819, New Orleans, Louisiana.\\nciationforComputationalLinguistics. AssociationforComputationalLinguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, RobertTinn,HaoCheng,YuGu,NaotoUsuyama,Xi-\\nDarioAmodei,andIlyaSutskever.2019. Language aodong Liu, Tristan Naumann, Jianfeng Gao, and\\nmodelsareunsupervisedmultitasklearners. HoifungPoon.2023. Fine-tuninglargeneurallan-\\nguagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe HugoTouvron,ThibautLavril,GautierIzacard,Xavier\\nlimitsoftransferlearningwithaunifiedtext-to-text Martinet,Marie-AnneLachaux,TimothéeLacroix,\\ntransformer. JournalofMachineLearningResearch, BaptisteRozière,NamanGoyal,EricHambro,Faisal\\n21(1). Azhar,AurelienRodriguez,ArmandJoulin,Edouard\\nGrave,andGuillaumeLample.2023. Llama: Open\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nandefficientfoundationlanguagemodels.\\nKnowwhatyoudon’tknow:Unanswerablequestions\\nforsquad. CoRR,abs/1806.03822.\\nAashka Trivedi, Takuma Udagawa, Michele Merler,\\nRameswarPanda,YousefEl-Kurdi,andBishwaran-\\nPranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in\\nMachineComprehensionofText. InEMNLP.\\nlanguagemodels. arXivpreprintarXiv:2303.09639.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nTakumaUdagawa,AashkaTrivedi,MicheleMerler,and\\nBERT:SentenceembeddingsusingSiameseBERT-\\nBishwaranjanBhattacharjee.2023. Acomparative\\nnetworks. InProceedingsofthe2019Conferenceon\\nanalysisoftask-agnosticdistillationmethodsforcom-\\nEmpiricalMethodsinNaturalLanguageProcessing\\npressingtransformerlanguagemodels. InProceed-\\nandthe9thInternationalJointConferenceonNatu-\\ningsofthe2023ConferenceonEmpiricalMethodsin\\nralLanguageProcessing(EMNLP-IJCNLP),pages\\nNaturalLanguageProcessing: IndustryTrack,pages\\n3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria\\nEmilio,CarlesFalcon,SherezadeFuentes,LauraHer- Nicholas Walker, Amalie Trewartha, Haoyan Huo,\\nnandez, Gema Huesa, Jordi Huguet, Paula Marne, SanghoonLee,KevinCruse,JohnDagdelen,Alexan-\\nTaniaMenchón,GrégoryOperto,AlbinaPolo,San- der Dunn, Kristin Persson, Gerbrand Ceder, and\\ndra Pradas, Anna Soteras, Marc Vilanova, and Na- AnubhavJain.2021. Theimpactofdomain-specific\\ntalia Vilor-Tejedor. 2020. Novel tau biomarkers pre-trainingonnamedentityrecognitiontasksinma-\\nphosphorylatedatt181,t217ort231riseintheini- terialsscience. AvailableatSSRN3950755.\\ntial stages of the preclinical alzheimer&#x2019;s\\n<i>continuum</i> when only subtle changes in Liang Wang, Nan Yang, Xiaolong Huang, Binxing\\na&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-\\nsupervisedcontrastivepre-training.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishekSrivastava,andIrynaGurevych.2021. Beir: WenhuiWang,HangboBao,ShaohanHuang,LiDong,\\nAheterogenousbenchmarkforzero-shotevaluation and Furu Wei. 2021. MiniLMv2: Multi-head self-\\nofinformationretrievalmodels. attention relation distillation for compressing pre-\\ntrainedtransformers. InFindingsoftheAssociation\\nJames Thorne, Andreas Vlachos, Christos forComputationalLinguistics: ACL-IJCNLP2021,\\nChristodoulopoulos, and Arpit Mittal. 2018. pages2140–2151,Online.AssociationforComputa-\\nFEVER: a large-scale dataset for fact extraction tionalLinguistics.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and Model Training NASA-IR BEIRAvg.\\nTie-Yan Liu. 2013. A theoretical analysis of ndcg INDUS-RETRIEVERSMALL One-Stage 0.73 0.42\\ntyperankingmeasures. InProceedingsofthe26th INDUS-RETRIEVERSMALL Stagewise 0.72 0.41\\nAnnualConferenceonLearningTheory,volume30\\nofProceedingsofMachineLearningResearch,pages\\nTable9: AblationStudy: EvaluationresultsonNASA-\\n25–54,Princeton,NJ,USA.PMLR. QAandBEIR.NASA-QAshowedRecall10whileBEIR\\nreportednDCG10.\\nShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,\\nMarkDredze,SebastianGehrmann,PrabhanjanKam-\\nbadur, David Rosenberg, and Gideon Mann. 2023. C CompleteResultsonBEIRBenchmark\\nBloomberggpt: Alargelanguagemodelforfinance.\\nTable11showstheper-datasetresultsontheBEIR\\nShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao.\\ntasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.\\nA SentenceEmbeddingTrainingData\\nTable 10 shows the various data sources used for\\ntrainingembeddingmodels. Alldataispresented\\nin the form of text-pairs, where each item in the\\npairmaybeasentenceoraparagraph. Inthetable,\\nDataFormatdenotess2pforsentence-to-paragraph\\nmappings,s2sforsentence-to-sentencemappings,\\nandp2pforparagraph-to-paragraphmappings. We\\nusedabout360millionpairsfortrainingandused\\nin-batchnegatives.\\nB AblationStudy: Stage-wiseDistillation\\nforEmbeddingModel\\nFor the distilled embedding models, we find that\\nstage-wisedistillationdoesnotbenefitperformance\\nasmuchasaone-stepprocess, combiningallthe\\nsupervised and unsupervised data. As shown in\\nTable9,thestage-wiseapproachunderperformed\\ntheone-stageapproachby1percentagepointfor\\nbothNASA-QAandonBEIR.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}), Document(page_content='Dataset Num. Pairs DataCategory DataFormat\\nStackOverflow† 18562443 Title-Body s2p\\nStackExchangeMath† 2201906 Title-Body s2p\\nS2ORC[title-abstract](Loetal.,2020) 41769185 Title-Body s2p\\nS2ORCCitationPairs[Abstracts](Loetal.,2020) 52603982 Title-Body p2p\\nStackExchange[title-body]† 5415570 Title-Body s2p\\nWikipedia(Faderetal.,2014) 6458670 Title-Body s2p\\nArxiv(Clementetal.,2019) 2358545 Title-Body s2p\\nNASAADS[title-abstract](§2) 2633240 Title-Body s2p\\nPubMed[title-abstract](§2) 24001387 Title-Body s2p\\nPMC[title-abstract](§2) 2585537 Title-Body s2p\\nStackExchangeDuplicateQuestions[title-body-title-body]† 250460 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[body-body]† 250519 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[title-title]† 304525 DuplicateQuestions s2s\\nWikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s\\nSQuAD(Rajpurkaretal.,2016) 87599 QuestionAnswers s2p\\nNQ(Kwiatkowskietal.,2019) 100231 QuestionAnswers s2p\\nSearchQA(Dunnetal.,2017) 582261 QuestionAnswers s2p\\nStackExchange[title-answer]† 4067139 QuestionAnswers s2p\\nStackExchange[title-body-answer]† 187195 QuestionAnswers p2p\\nPAQ(Lewisetal.,2021) 64371441 QuestionAnswers s2p\\nFEVER(Thorneetal.,2018)∗ 109810 FactVerification s2p\\nHotpotQA(Yangetal.,2018)∗ 85000 QuestionAnswering s2p\\nTable10:TrainingDataforEmbeddingModels. Thetrainingdatatotalstoaround360Mpairs. DataFormatdenotes\\ns2pforsentence-to-paragraphmappings,s2sforsentence-to-sentencemappings,andp2pforparagraph-to-paragraph\\nmappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval\\nTREC- NFCorpus NQ HotPotQA FiQA ArguaAna Touche DBPedia Scidocs FEVER Climate SciFact AVG.\\nCovid FEVER BEIR\\nRoBERTa\\nBASE\\n0.47 0.30 0.54 0.34 0.38 0.52 0.18 0.25 0.22 0.46 0.14 0.67 0.37\\nBGEBASE 0.78 0.37 0.54 0.73 0.41 0.64 0.26 0.41 0.22 0.86 0.31 0.74 0.52\\nINDUS-RETRIEVERBASE 0.56 0.32 0.54 0.49 0.36 0.54 0.17 0.31 0.21 0.56 0.14 0.74 0.41\\nMINILM-V2 0.47 0.32 0.44 0.47 0.35 0.50 0.17 0.32 0.22 0.52 0.25 0.65 0.39\\nBGESMALL 0.76 0.34 0.50 0.70 0.40 0.60 0.26 0.40 0.21 0.87 0.32 0.71 0.51\\nINDUS-RETRIEVERSMALL 0.55 0.31 0.53 0.48 0.29 0.50 0.21 0.33 0.23 0.61 0.23 0.71 0.42\\nTable11: EvaluationresultsBEIR.\\n', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'})]\n"
     ]
    }
   ],
   "source": [
    "# total 12 documents\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6edc065-ec1f-4b2e-a9f8-0a29dbd59052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUS: Effective and Efficient Language Models for Scientific Applications\n",
      "BishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\n",
      "MuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\n",
      "BharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\n",
      "ElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\n",
      "ThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\n",
      "DavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\n",
      "PanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\n",
      "ArminMehrabian9,TsendgarLee7\n",
      "1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\n",
      "7 NASAHQ,8 JPL,9 NASAGSFC\n",
      "Abstract generation tasks. Most popular LLMs rely on the\n",
      "transformer architecture (Vaswani et al., 2017)\n",
      "Largelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\n",
      "eral domain corpora showed remarkable re-\n",
      "like Wikipedia or CommonCrawl (Devlin et al.,\n",
      "sults on natural language processing (NLP)\n",
      "2019; Liu et al., 2019; Lewis et al., 2020; Raffel\n",
      "tasks. However, previous research demon-\n",
      "et al., 2020; Brown et al., 2020; Touvron et al.,\n",
      "stratedLLMstrainedusingdomain-focusedcor-\n",
      "pora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\n",
      "spired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\n",
      "INDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\n",
      "fortheEarthscience,biology,physics,helio- domain-specific natural language understanding\n",
      "physics, planetary sciences and astrophysics\n",
      "and generation tasks (Beltagy et al., 2019). Fol-\n",
      "domains and trained using curated scientific\n",
      "lowing this observation, several domain-specific\n",
      "corporadrawnfromdiversedatasources. The\n",
      "LLMs such as SCIBERT (Beltagy et al., 2019),\n",
      "suiteofmodelsinclude: (1)anencodermodel\n",
      "trainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\n",
      "corpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\n",
      "standingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\n",
      "general text embedding model trained using developedwiththegoalofimprovingaccuracyon\n",
      "a diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\n",
      "ple sources to address information retrieval\n",
      "Wuetal.,2023).\n",
      "tasks and (3) smaller versions of these mod-\n",
      "els created using knowledge distillation tech-\n",
      "Inthisresearch,wespecificallyfocusedoninter-\n",
      "niquestoaddressapplicationswhichhavela-\n",
      "disciplinaryfieldsrelatedtotheEarth,celestialbod-\n",
      "tencyorresourceconstraints. Wealsocreated\n",
      "ies, the Sun, and planets within our solar system\n",
      "threenewscientificbenchmarkdatasetsnamely,\n",
      "suchasphysics,Earthscience,astrophysics,helio-\n",
      "CLIMATE-CHANGE NER (entity-recognition),\n",
      "NASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\n",
      "accelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\n",
      "fields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\n",
      "elsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\n",
      "(RoBERTa) and existing domain-specific en-\n",
      "rentlynospecificmodelavailablethatencompasses\n",
      "coders(SCIBERT)onthesenewtasksaswell\n",
      "allofthefieldsofinterestcollectively. Further,the\n",
      "asexistingbenchmarktasksinthedomainsof\n",
      "interdisciplinarynatureofthesedomainsofinter-\n",
      "interest.\n",
      "estisreflectedinavastbodyofliteraturescattered\n",
      "1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\n",
      "a collection of encoder-based LLMs focused on\n",
      "Large language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\n",
      "amountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\n",
      "pabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\n",
      "zationsandenterprisesworkinginthesefieldsby\n",
      "Contact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\n",
      "muthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\n",
      "4202\n",
      "yaM\n",
      "02\n",
      "]LC.sc[\n",
      "2v52701.5042:viXra\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddea242-5b07-44df-83be-6158eb1b4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b4e216e-69de-42fc-844a-69ece0f9fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200, add_start_index = True)\n",
    "doc_splits = text_splitter.split_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bced56a-84bc-4b38-9211-b466cb2aed40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='INDUS: Effective and Efficient Language Models for Scientific Applications\\nBishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\\nMuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\\nBharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\\nElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\\nThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\\nDavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\\nPanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\\nArminMehrabian9,TsendgarLee7\\n1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\\n7 NASAHQ,8 JPL,9 NASAGSFC\\nAbstract generation tasks. Most popular LLMs rely on the\\ntransformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='transformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-\\nlike Wikipedia or CommonCrawl (Devlin et al.,\\nsults on natural language processing (NLP)\\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\\ntasks. However, previous research demon-\\net al., 2020; Brown et al., 2020; Touvron et al.,\\nstratedLLMstrainedusingdomain-focusedcor-\\npora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\\nspired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\\nINDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\\nfortheEarthscience,biology,physics,helio- domain-specific natural language understanding\\nphysics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 808}), Document(page_content='physics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific\\ncorporadrawnfromdiversedatasources. The\\nLLMs such as SCIBERT (Beltagy et al., 2019),\\nsuiteofmodelsinclude: (1)anencodermodel\\ntrainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\\ncorpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\\nstandingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\\ngeneral text embedding model trained using developedwiththegoalofimprovingaccuracyon\\na diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\\nple sources to address information retrieval\\nWuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1611}), Document(page_content='Wuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-\\ndisciplinaryfieldsrelatedtotheEarth,celestialbod-\\ntencyorresourceconstraints. Wealsocreated\\nies, the Sun, and planets within our solar system\\nthreenewscientificbenchmarkdatasetsnamely,\\nsuchasphysics,Earthscience,astrophysics,helio-\\nCLIMATE-CHANGE NER (entity-recognition),\\nNASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\\naccelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\\nfields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\\nelsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\\n(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2400}), Document(page_content='(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the\\nasexistingbenchmarktasksinthedomainsof\\ninterdisciplinarynatureofthesedomainsofinter-\\ninterest.\\nestisreflectedinavastbodyofliteraturescattered\\n1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\\na collection of encoder-based LLMs focused on\\nLarge language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\\namountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\\npabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\\nzationsandenterprisesworkinginthesefieldsby\\nContact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\\nmuthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\\n4202\\nyaM\\n02\\n]LC.sc[\\n2v52701.5042:viXra', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3209}), Document(page_content='Earth Science Data Pretraining\\n(Masked Finetuning Scientific Corpora\\nBioMedical Data Language Indus-Base (Contrastive\\nModelling) Learning) Open QA\\nAstrophysics Data\\nDuplicate Pairs\\nAstronomy Data\\nRepresentation Indus-Retriever-Base Citation Pairs\\nGeneral Science Distillation\\nData\\nFact Verification\\nGeneral English\\nData\\nGeneric Corpora\\nOutput\\nEncoder Training Corpus Indus-Small Distillation Embedding Training\\nCorpus\\nData Teacher (KD)\\nInitialization Output Indus-Retriever-\\nSmall\\nBC5-CHEM BC5-Disease NCBI-Disease BC2GM\\nNASA-QA TREC-Covid NFCorpus NQ HotpotQA\\nJNLPBA EBM-PICO ChemProt DDI\\nClimate FiQA Arguana Touche DBPedia NASA-IR\\nGAD HoC PubMedQA BioASQ Change NER\\nSciDocs FEVER C Fl Eim VEat Re SciFact\\nBIOSSES BLURB Benchmark BEIR Benchmark\\nNatural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='Natural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir\\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\\nNASA-QA,CLIMATE-CHANGENERandNASA-IR.\\nenablingthemininformeddecision-making. (Beltagy et al., 2019). We also show that the\\nSpecifically, we make the following contribu- knowledge-distilledmodelsachievedasignifi-\\ntions: cantboostinlatencywhilemaintainingstrong\\n1. Utilizing the byte-pair encoding algorithm, empiricalperformancecomparedtotheoriginal\\nwe constructed INDUSBPE, a customized to- modelsonmostofthebenchmarktasks.\\nkenizerfromthecuratedscientificcorpus.\\n2 Data\\n2. Wepretrainedmultipleencoder-onlyLLMsus-\\ning the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 747}), Document(page_content='ing the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than\\nsentence-embeddingmodelsbyfine-tuningthe\\ntheircounterpartstrainedonopen-domaincorpora.\\nencoder-only models with a contrastive learn-\\nWemeticulouslyidentifiedcorporaforeachofthe\\ningobjectivetolearn“universal”sentenceem-\\naforementioneddomains,andcreatedEnglish-only\\nbeddings (Gao et al., 2021) (§4). We also\\nmodels for the sake of containment. Specifically,\\ntrained smaller, more efficient versions of\\nforeachofthedomains,weusedopen-sourcedata\\nthesemodelsusingknowledge-distillationtech-\\nwhich has a permissive license, and further aug-\\nniques(§3.3,§4.2).\\nmentedthemwithspecificdatafromNASAandits\\n3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1540}), Document(page_content='3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in\\nrecognitiontask),NASA-QA(anextractiveques-\\nourtrainingcorpora. Webrieflydescribeeachdata\\ntionansweringtask)and NASA-IR (aretrieval\\nsourcebelow,andpresentstatisticsofthedatain\\ntask)(§5)tofurtheraccelerateresearchinthis\\nTable1.\\nmulti-disciplinaryfield.\\n• SAO/NASAAstrophysicsDataSystem(ADS)1:\\n4. Throughexperimentalresults,weshowstrong\\nADSisthebiggestsourceofdata,coveringpub-\\nperformancebyourmodelsonthesebenchmark\\nlications in the areas of astronomy and astro-\\ntasks as well as on existing domain-specific\\nphysics,physicsandgeneralscienceincluding\\nbenchmarks, outperforming general-purpose\\nallarXive-prints.\\nmodels like RoBERTa (Liu et al., 2019) as\\nwellasscientific-domainencoderslikeSCIBERT 1https://ui.adsabs.harvard.edu', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2318}), Document(page_content='Dataset Domain #Tokens Ratio Tokenizer ADS PMC Wikipedia\\nNASACMR EarthScience 0.3B 1% RoBERTa 12,867,439 7,549,075 15,859\\nAMSandAGUpapers EarthScience 2.8B 4% +lower_cased 12,862,227 7,557,868 16,901\\nEnglishWikipedia General 5.0B 8% INDUSBPE 12,309,023 6,920,659 16,056\\nPubMedAbstracts Biomedical 6.9B 10%\\nTable2: NumberoftokensproducedbyRoBERTaand\\nPMC Biomedical 18.5B 28%\\nINDUSBPEtokenizersappliedto1ksamplesfromeach\\nSAO/NASAADS Astronomy, 32.7B 49%\\ndataset. Fewer tokens lead to a smaller computation\\nAstrophysics,\\ncost.\\nPhysics,\\nGeneralScience\\nTotal 66.2B 100% ingdataset(§2)6. Forafaircomparison,wesetthe\\nvocabularysizeto50,265,whichisequaltothatof\\nTable1: Basicstatisticsofourpretrainingdataset.\\nthe RoBERTatokenizer(Liuetal.,2019)andused\\ntheuncasedvariationofboththetokenizers.\\n• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-\\nliterature maintained by National Library of BPE and the RoBERTa tokenizer. Out of 50,265\\nMedicineandNationalInstitutesofHealth. We tokens,22,355(44.5%)tokensarecommoninboth\\nusedtheportionofPMCthathasacommercial- thetokenizerswhiletheremaining27,910(55.5%)\\nfriendly license, along with the PubMed ab- tokensareincludedonlyineithertokenizer, indi-\\nstractsofallthearticlesinPMC. catingasignificantdistributionalshiftindomain.\\n• American Meteorological Society (AMS)3: Tofurther understandthe effect, weapplied both\\nWeusedfull-textjournaldocumentsspanning RoBERTaandINDUSBPEon1,000randomlysam-\\ntopicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 785}), Document(page_content='topicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts\\noceanography, atmospheric sciences, climate, tosinglesentences. AsshowninTable2, INDUS-\\nhydrometeorology, weather and forecasting, BPE tokenizer produced fewer tokens than the\\nandsocietalimpacts. RoBERTa tokenizer, leading to 8˜% drop in com-\\n• American Geophysical Union (AGU)4: The putationcostduringtraining.\\nAGUdatasetincludedjournaldocumentsacross Table3comparestheRoBERTatokenzierandIN-\\nthe topics of atmospheres, biogeosciences, DUSBPE tokenizer,illustratingthattheproposed\\nEarth surface, machine learning and compu- tokenizertreatedscientificterms(suchasbiomak-\\ntation, oceans, planets, solid earth, and space ers, phosphorylated, alzheimer) as single tokens\\nphysics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1546}), Document(page_content='physics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-\\n3.2 EncoderModel\\nquality,continuouslyevolvingmetadatasystem\\nthat catalogs all data and service metadata Wefirsttrainedanencoder-onlymodel,INDUS ,\\nBASE\\nrecords for NASA’s Earth Science Data and usingamaskedlanguagemodelingobjective. The\\nInformation System (ESDIS). It contains text modelarchitecturefollowsRoBERTa (Liuetal.,\\nBASE\\ndescriptions of the NASA Earth science data 2019),whichconsistsof12layersandhas125M\\nproducts. parameters. Weadoptedthedefaulthyperparame-\\nters7 butwithaneffectivebatchsizeof92,16. We\\n3 Methodology: EncoderModels trainedthemodelfor500Kstepsusing192V100\\nGPUs.\\n3.1 INDUSBPE Tokenizer\\n3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2388}), Document(page_content='3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL\\n2https://www.ncbi.nlm.nih.gov/pmc 38M parameters through knowledge distillation\\n3https://www.ametsoc.org/index.cfm/ams/publications/\\n4https://agupubs.onlinelibrary.wiley.com/ 6We used HF tokenizers, https://github.com/\\n5https://www.earthdata.nasa.gov/eosdis/science-system- huggingface/tokenizers\\ndescription/eosdis-components/cmr 7WereferreaderstoTable9in(Liuetal.,2019).', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3153}), Document(page_content='Inputtext\\nnoveltaubiomarkersphosphorylatedatt181,t217ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected\\nTokenizationbyRoBERTa\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTokenizationby INDUSBPE\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTable3: TokenizationcomparisonbetweenRoBERTaandourtokenizers. InputtextadaptedfromSuárez-Calvet\\netal.(2020).\\n(cid:88) (cid:88)\\ntechniquesbyusingINDUS\\nBASE\\nastheteacher. IN- Z\\ni\\n= es(qi,pj)+ es(qj,pi)\\nDUS follows a 4-layer architecture recom- j j\\nSMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='SMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the\\nwheres(q,p)isameasureoftemperature-scaled\\ndistillationobjectiveproposedinMiniLMv2(Wang\\ncosinesimilaritybetweentheembeddingsofquery\\netal.,2021)totransferfine-grainedself-attention\\nandapassagemeasuredby:\\nrelations,whichhasbeenshowntobethecurrent\\nstate-of-the-art(Udagawaetal.,2023). Usingthis 1 E(q)·E(p)\\ns(q,p) = (3)\\nobjective,wetrainedthemodelfor500Kstepswith τ ∥E(q)∥∥E(p)∥\\naneffectivebatchsizeof480on30V100GPUs.\\nTrainingData Similartopriorwork(Wangetal.,\\n2022; Li et al., 2023; Xiao et al., 2023), we em-\\n4 Methodology: SentenceEmbedding\\nployedastage-wisetrainingapproachforoursen-\\nModels\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 777}), Document(page_content='Models\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional\\noccurringpairscollectedfrominternetsources,\\nvectors,allowingforefficientuseindenseretrieval\\nsuch as Wikipedia, StackExchange, etc. We\\nsystems,whererelevantpassagesforagivenquery\\nalsoincludedscientificdatafromPubMed,PMC\\nareidentifiedonthebasisofthesimilaritybetween\\n(§2),Arxivand S2ORC (Loetal.,2020)asin-\\ntheirembeddings(Karpukhinetal.,2020).\\ndomaindataforourscience-orientedretriever\\nmodel. Furthermore, we created a domain-\\nContrastiveLearningObjective Sentenceem-\\nspecific dataset from the ADS data (§2) by in-\\nbeddingmodelstrainedusingacontrastivelearn-\\ncludingtitle-abstractpairs.\\ningobjective(Khoslaetal.,2020;Gaoetal.,2021)\\n2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1553}), Document(page_content='2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa\\n(Kwiatkowskietal.,2019),SQuAD(Rajpurkar\\nnon-relevantpassage.\\net al., 2016), SPECTER pairs (Cohan et al.,\\nInspired by recent work (Li et al., 2023), we\\n2020), etc. We included the aforementioned\\nusedanimprovedcontrastivelossbyintroducing\\nADS data and a sample of the S2ORC data in\\nanadditionalbidirectionalsignal. Specifically,for\\nthisstep,toboostdomain-specificsignals.\\na triple {q,p+,P−} of a query, a relevant (posi-\\nAppendixAcontainscomprehensivedetailsabout\\ntive)passage,andasetofnon-relevant(negative)\\nthe datasets used in training. For both training\\npassagesP− = {p−}m ,WedefinetheInfoNCE\\nj j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2374}), Document(page_content='j j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches\\n1 (cid:88)n es(qi,p+ i ) from each data source proportionately to its size,\\nL = − log (1)\\nIC\\nn Z i similartoLietal.(2023).\\ni=1', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3146}), Document(page_content='Model Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='extendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K\\ninthelargermodelbutimprovestheperformance\\nstepsfollowedbyanadditional100Kstepswiththe\\nofthesmallermodel.\\nsuperviseddata. Weusedalearningrateof2e−5\\nFor distilling the sentence embedding model,\\nduringboththesesteps.\\nwefoundthatastage-wisetrainingapproachdoes\\n4.2 KnowledgeDistillationforEmbedding not benefit performance as much as in the non-\\nModels distillation case (ablation presented in Appendix\\nB). We thus distilled in a single step with all the\\nTooptimizethelatencyforretrievalapplications,\\ndatadescribedin§4.1andAppendixAandadded\\nwe also created a small retriever model with the\\nlabelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 811}), Document(page_content='labelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE\\nmodel(INDUS SMALL),bydistillingtheteacher’sdis- ModelSpecifications Webuiltthesentenceem-\\ntributionofsimilarityscores. Furthermore,wefind beddingmodelbydistillinginto INDUS . This\\nSMALL\\nthatitisnecessarytomodifythetrainingstrategy isa4-layermodelwithanembeddingdimension\\nfordistillation,asdescribedbelow. of576. Werefertotheresultingretrievermodelas\\nDistillationLoss Weusedknowledgedistillation\\nINDUS-RETRIEVER SMALL. Itfollowsabi-encoder\\nframework,andherewefindthatusingthevector\\ntechniquesintroducedin(Xuetal.,2023). Specif-\\nrepresentationofthefirsttokenastheembedding\\nically, for a sentence x and its corresponding in-\\ni\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1573}), Document(page_content='i\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-\\ntionp ofsimilarityscoresbetweenpairsandthe\\nt TrainingDetails FortheRetro-MAEstylepre-\\nstudent’sdistribution, p . FollowingHintonetal.\\ns training(Xiaoetal.,2022),wetrainedon8A100\\n(2014), we also scaled the output distribution of\\nGPUs with an effective batch size of 128 for 2\\nbothteacherandstudentbyatemperature,τ :\\nKD epochswithalearningrateof2e−5. Forthestage-\\nn m wisedistillation,wetrainedon2A100GPUsfor\\n(cid:88)(cid:88)\\nL = − p (x ,x )logp (x ,x ) (4) 300K steps with an effective batch size of 2,048,\\nKD t i j s i j\\ni=1 j=1 andlearningrateof7e−4. Throughexperimenta-\\ntion,Wefoundthatτ = 4performedthebest.\\nKD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2369}), Document(page_content='KD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)\\nt i j (cid:80)m est(xi,x k)/τKD ingthelanguageunderstandingcapabilitiesmodels.\\nk=1\\nHowever,tothebestourknowledge,thereisano-\\nHere,s (x ,x )ands (x ,x )representthesim-\\ns i j t i j\\nticeableabsenceofdatasetstailoredforthediverse\\nilarityscoresbetweentwopairs{x ,x },definedin\\ni j\\nand multidisciplinary field under study. Thus, to\\nEquation3forthestudentandteacherrespectively.\\neffectively benchmark the proposed NLP models\\nTrainingData Wefirstconductedaembedding- and further accelerate research in this multidisci-\\noriented pretraining step, as presented in Retro- plinarydomain,Weintroducedthreenewdatasets,\\nMAE (Xiao et al., 2022), on English Wikipedia, anNERtask,aQAtask,andanIRtask,described\\nBooksCorpus,andStackExchangedata,totalling below.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3142}), Document(page_content='Train Validation Test spansoftheparagraphwhichanswerthequestions.\\nNum.Abstracts 382 77 75\\nWeused29paragraphs(with145QApairsintotal)\\nNum.Tokens 32,031 6,443 5,850\\nEntityLabels asthetrainingsetandtheremaining10paragraphs\\nclimate-nature,climate-greenhouse-gases,climate-assets,\\n(with 50 questions in total) as the evaluation set.\\nclimate-problem-origins,climate-mitigations,\\nclimate-properties,climate-impacts,climate-datasets, Thetrainingsetwasfurtheraugmentedwithpara-\\nclimate-organizations,climate-observations,\\ngraphsand QA pairsrelatedtoEarthsciencefrom\\nclimate-models,climate-hazards,climate-organisms\\nthe SQuADdataset(Rajpurkaretal.,2018). Specif-\\nTable4: CLIMATE-CHANGE NER statisticsandentity ically,thoserelatedtooxygen,Amazonrainforest\\nlabels\\nandgeologywereused. Thisresultedinapruned\\nSQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='SQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-\\nby augmenting these SQuAD pairs to the training\\nfer some assistance in exploring data related to\\ndatasourcedfromEarthsciencepapers,whilekeep-\\nclimatechange,thecomplexityofclimate-related\\ningtheevaluationsetintact.\\nqueries often requires more sophisticated natural\\nlanguageprocessingtechniques. Thisnecessityis 5.3 NASA-IR\\nunderscoredbytheextensivearrayofclimatemod-\\nWe introduced a domain-specific information re-\\nels, datasets, and organizations involved, which\\ntrieval benchmark, NASA-IR10, spanning almost\\ndemand meticulous curation and continuous up-\\n500question-answerpairsrelatedtotheEarthsci-\\ndates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 793}), Document(page_content='dates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-\\ndata,comprehensiveoverviewsofclimatemodels\\nically, we sampled a set of 166 paragraphs from\\nandimpactassessmentsarescarceandnoteasily\\nAGU, AMS, ADS, PMC and Pubmed (§2) and\\naccessible.\\nmanually annotated with 3 questions that are an-\\nInordertobridgethisgap,weintroducedacom-\\nswerablefromeachoftheseparagraphs,resulting\\nprehensive dataset for developing and evaluating\\nin498questions. Weused398ofthesequestions\\nNLP models tailored towards understanding and\\nas the training set and the remaining 100 as the\\naddressing climate-related topics across various\\nvalidation set. To comprehensively evaluate the\\ndomains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1589}), Document(page_content='domains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-\\nwhichthenamedentitiesofinterestoriginatefrom\\nstracts with these annotated paragraphs. On an\\ncomplextaxonomiesusedinclimate-relatedlitera-\\naverage, each query is 12 words long, and each\\nture. Thisdatasetcomprises534abstractssourced\\nparagraphis120wordslong. WeusedRecall@10\\nfromSemanticScholarAcademicGraph(Kinney\\nasevaluationmetricsinceeachquestionhasonly\\netal.,2023),collectedusingaseedsetofclimate-\\nonerelevantdocument.\\nrelated keywords such as wildfire or floods. The\\nabstractswereannotatedusingtheIOB(inside,out-\\n6 ExperimentalResults\\nside,beginning)taggingschemeandencompasses\\nadiversearrayofentitytypes,asshowninTable4. Baselines Wecompared INDUS modelsagainst\\nopensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2378}), Document(page_content='opensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.\\nsweringtaskfocusedontheEarthsciencedomain. • INDUS was compared to MINILM (6-\\nSMALL\\nFirst, 39 paragraphs from Earth science papers\\nlayer)13 andTINYBERT(4-layer)14.\\nwhichappearedinAGUandAMSjournals(§2)were\\n10https://huggingface.co/datasets/nasa-impact/nasa-smd-\\nsourced. Subject matter experts from NASA for- IR-benchmark\\nmulatedquestionsandmarkedthecorresponding 11https://huggingface.co/FacebookAI/roberta-base\\n12https://huggingface.co/allenai/scibert_scivocab_uncased\\n8https://huggingface.co/datasets/ibm/Climate-Change- 13https://huggingface.co/nreimers/MiniLM-L6-H384-\\nNER uncased\\n9https://huggingface.co/datasets/nasa-impact/nasa-smd- 14https://huggingface.co/huawei-\\nqa-benchmark noah/TinyBERT_General_4L_312D', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3221}), Document(page_content='Basemodel(125Mparams.) Smallmodel(∼30Mparams.)\\nTask Metric Dataset RoBERTa SCIBERT INDUSBASE TINYBERT MINILM INDUSSMALL\\nBC5-chem 90.3(0.2) 91.4(0.2) 93.3(0.2) 84.6(0.2) 86.1(0.3) 90.7(0.1)\\nBC5-disease 81.5(0.3) 83.7(0.3) 85.2(0.3) 74.0(0.4) 77.4(0.3) 81.3(0.3)\\nNER EntityF1 NCBI-disease 87.6(0.6) 87.6(0.4) 88.3(0.4) 81.2(0.4) 83.1(0.5) 85.6(0.6)\\nBC2GM 82.1(0.3) 82.3(0.2) 84.0(0.3) 74.7(0.4) 77.1(0.2) 79.7(0.3)\\nJNLPBA 79.1(0.2) 78.2(0.2) 80.3(0.2) 70.3(0.2) 73.4(0.3) 75.7(0.2)\\nPICO MacroF1 EBMPICO 72.3(0.3) 72.4(0.3) 73.1(0.2) 67.4(0.2) 70.3(0.1) 73.1(0.2)\\nChemProt 50.4(28.2) 73.9(0.7) 76.9(0.5) 56.2(3.2) 55.9(2.1) 71.7(0.9)\\nRelation\\nMicroF1 DDI 78.6(1.5) 80.1(1.0) 81.7(0.5) 39.3(5.3) 51.5(2.9) 69.0(1.2)\\nExtraction\\nGAD 80.0(1.1) 81.6(1.2) 79.4(5.6) 76.4(1.3) 77.3(1.0) 81.3(0.7)\\nDocument\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='Document\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy\\nAnswering BioASQ 69.1(4.8) 74.6(4.5) 69.6(5.8) 74.3(3.6) 66.7(2.3) 75.4(3.3)\\nSentence\\nSimilarity Pearson BIOSSES 79.8(6.3) 86.3(3.5) 72.2(9.5) 88.2(1.1) 26.6(8.7) 70.4(3.3)\\nMicroAverage - - 75.9(3.7) 79.2(1.3) 78.9(2.4) 67.6(1.9) 66.1(1.9) 76.2(1.0)\\nMacroAverage - - 74.9(3.7) 78.2(1.6) 76.4(3.2) 65.6(2.4) 60.6(3.0) 74.3(1.3)\\nTable5:EvaluationresultsonBLURB.Resultsreportedareaveragedon10randomseedswithstandarddeviationin\\nparenthesis. Microaverageisreportedacrossdatasetswhilemacroaverageiscomputedbyfirstaveragingscoreson\\neachtask(say,taskaverage),followedbyaveragingthetaskaverageacrosstasks. Resultsinboldindicatehighest\\nperformancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 787}), Document(page_content='performancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE\\nBGE 15 andaRoBERTa modelfinetuned RoBERTa 60.8(0.8)\\nBASE BASE\\nwiththesamemethodpresentedin§4.1. SCIBERT 61.8(0.7)\\n• INDUS-RETRIEVER\\nSMALL\\nwas compared to INDUS\\nBASE\\n64.0(1.0)\\nMINILM-V216 and BGE 17. TINYBERT 34.3(1.6)\\nSMALL\\nMINILM 44.7(1.3)\\n6.1 NaturalLanguageUnderstanding INDUS 54.8(0.8)\\nSMALL\\nBenchmarks\\nTable 6: CLIMATE-CHANGE NER benchmark results.\\nWe evaluated our models on BLURB (Gu et al.,\\nStandard deviation over 10 random seeds shown in\\n2021),abenchmarksuitefornaturallanguageun-\\nparenthesis. Resultsinboldandunderlineindicatehigh-\\nderstandingandreasoningtasksinthebiomedical estperformanceandsignificantdifferencefromsecond\\ndomain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1596}), Document(page_content='domain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.\\nTable 5 shows the evaluation results. Among\\nbase models, INDUS BASE significantly outper- WealsonoticedSCIBERTtendstoperformbetter\\nformsthegeneral-purposeRoBERTamodelonmi-\\nthan our model on paired input-text tasks, such\\ncro/macroaveragewhileachievingcompetitiveper-\\nas QA and semantic similarity tasks, although\\nformance to the bio-domain-specific counterpart,\\nthe results have relatively large standard devia-\\nSCIBERT.\\ntions. We hypothesized that the additional next\\nAsforsmallermodels,wenoticed INDUS SMALL sentence prediction objective during training in\\noutperformed the baselines, TINYBERT and BERT-stylemodels(suchasSCIBERT)incontrastto\\nMINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2418}), Document(page_content='MINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text\\ninNER,PICO,relationextraction,anddocument\\ntasks. Thistrendwasconsistentwiththeobserva-\\nclassification tasks. This demonstrates the effec-\\ntionsofTinnetal.(2023).\\ntivenessofknowledgedistillationfromourdomain-\\nspecificteachermodel,INDUS . 6.2 CLIMATE-CHANGE NER\\nBASE\\nAs shown in Table 6, our models clearly outper-\\n15https://huggingface.co/BAAI/bge-base-en-v1.5\\n16sentence-transformers/all-MiniLM-L6-v2 formedthecorrespondingbaselinemodelsonthe\\n17https://huggingface.co/BAAI/bge-small-en-v1.5 CLIMATE-CHANGE NERtask,suggestingtheeffec-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3215}), Document(page_content='tivenessoftrainingonlargedomain-specificdata. Model NASA-IR↑ BEIRAvg.↑ Retrieval\\nTime↓\\n6.3 NASA-QA BRo GB EE BAR ST Ea BASE 0 0. .6 66\\n7\\n0 0. .3 57\\n2\\n1 1. .2 10\\n8\\nAs mentioned in §5, we augmented the training\\nINDUS-RETRIEVERBASE 0.71 0.41 1.19\\nMINILM-V2 0.62 0.39 0.24\\nsetwithrelevantSQuADpairsforfine-tuning. All BGESMALL 0.66 0.51 0.42\\nmodelsarefinetunedfor15epochs,andtheresults INDUS-RETRIEVERSMALL 0.73 0.42 0.26\\nareshowninTable7. WeobservedthatINDUS\\nBASE Table 8: Evaluation results on NASA-IR and BEIR.\\noutperformedallmodelsofsimilarsizes,while IN-\\nNASA-IRshowedRecall@10whileBEIRreportedthe\\nDUS hadrelativelystrongperformancecom- averagenDCG@10acrossalltasks. Retrievaltimeper\\nSMALL\\nparedtoitscounterparts. queryontheNQtaskfromBEIR,reportedinseconds.\\nModel F1(SD)\\n7 Conclusions\\nRoBERTa 66.8(3.1)\\nSCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='SCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-\\nMINILM 59.2(3.9) enizer and in-domain data for training high qual-\\nINDUS 47.4(1.8) ityencodermodelsandsentenceembeddingmod-\\nSMALL\\nels. Further, we created smaller versions of the\\nTable7: NASA-QA benchmarkresults. Standarddevi- proposedmodelssuitableforapplicationswithla-\\nation over 3 random seeds shown in parenthesis. Re-\\ntencyorresourceconstraintsthroughstate-of-the-\\nsultsinboldandunderlineindicatehighestperformance\\nartknowledgedistillationtechniques. Fortheben-\\nandsignificantdifferencefromsecondhighestresultby\\nefit of the scientific community, we will release\\nmorethantwostandarddeviationsineachmodelsize,\\nthedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 804}), Document(page_content='thedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.\\nDoguAraci.2019. Finbert: Financialsentimentanaly-\\nsiswithpre-trainedlanguagemodels.\\n6.4 InformationRetrievalBenchmarks\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nWeevaluatedourmodelsonthe NASA-IR dataset\\nERT:Apretrainedlanguagemodelforscientifictext.\\naswellasBEIRBenchmark(Thakuretal.,2021), InProceedingsofthe2019ConferenceonEmpirical\\nwhichconsistsof12retrievaltasksspanningavari- Methods in Natural Language Processing and the\\n9thInternationalJointConferenceonNaturalLan-\\netyofdomains. TheBEIRbenchmarkusedtheNor-\\nguageProcessing(EMNLP-IJCNLP),pages3615–\\nmalized Cumulative Discount Gain (nDCG@10)\\n3620,HongKong,China.AssociationforComputa-\\n(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1610}), Document(page_content='(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\ntenceembeddingmodels,alongwithourbaselines.\\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\\nAs shown, both of our sentence embedding mod- Neelakantan,PranavShyam,GirishSastry,Amanda\\nelssignificantlyoutperformedthebaselinesonthe Askell, Sandhini Agarwal, Ariel Herbert-Voss,\\nNASA-IRtaskwhilestillmaintaininggoodperfor- Gretchen Krueger, Tom Henighan, Rewon Child,\\nAdityaRamesh,DanielZiegler,JeffreyWu,Clemens\\nmanceonseveraloftheBEIRtasks. (Wepresented\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nresultsforeachBEIRtaskinAppendixC).\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nWe also measured the average time per query Clark, ChristopherBerner, SamMcCandlish, Alec\\nforretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2447}), Document(page_content='forretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-\\nquestionssetofBEIR,onasingleA100GPU.This\\nvances in Neural Information Processing Systems,\\ntime includes the time to encode the query, cor-\\nvolume 33, pages 1877–1901. Curran Associates,\\npus,andtimetoretrieverelevantdocuments. No- Inc.\\ntably, INDUS-RETRIEVER outperformed IN-\\nSMALL Colin B. Clement, Matthew Bierbaum, Kevin P.\\nDUS-RETRIEVER BASE,onbothNASA-IRandBEIR,\\nO’Keeffe, and Alexander A. Alemi. 2019. On the\\nwhilebeingabout4.6xfaster. useofarxivasadataset.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3276}), Document(page_content='Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Sarna, Yonglong Tian, Phillip Isola, Aaron\\nDowney, and Daniel S. Weld. 2020. SPECTER: Maschinot, CeLiu, andDilipKrishnan.2020. Su-\\nDocument-level Representation Learning using pervisedcontrastivelearning. InAdvancesinNeural\\nCitation-informedTransformers. InACL. InformationProcessingSystems,volume33,pages\\n18661–18673.CurranAssociates,Inc.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of Rodney Kinney, Chloe Anastasiades, Russell Authur,\\ndeepbidirectionaltransformersforlanguageunder- Iz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\\nstanding. InProceedingsofthe2019Conferenceof ski,IsabelCachola,StefanCandra,YoganandChan-\\ntheNorthAmericanChapteroftheAssociationfor drasekhar, Arman Cohan, Miles Crawford, Doug\\nComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='ComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-\\n4171–4186,Minneapolis,Minnesota.Associationfor ham, Fangzhou Hu, Regan Huff, Daniel King, Se-\\nComputationalLinguistics. bastianKohlmeier,BaileyKuehl,MichaelLangan,\\nDaniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\\nMatthewDunn,LeventSagun,MikeHiggins,V.Ugur Kelsey MacMillan, Tyler Murray, Chris Newell,\\nGuney, Volkan Cirik, and Kyunghyun Cho. 2017. Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\\nSearchqa: Anewq&adatasetaugmentedwithcon- Shen,AmanpreetSingh,LucaSoldaini,Shivashankar\\ntextfromasearchengine. Subramanian,AmberTanaka,AlexD.Wade,Linda\\nWagner, Lucy Lu Wang, Chris Wilhelm, Caroline\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 804}), Document(page_content='Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic\\ntractedknowledgebases. InProceedingsofthe20th scholaropendataplatform.\\nACMSIGKDDInternationalConferenceonKnowl-\\nedge Discovery and Data Mining, KDD ’14, page TomKwiatkowski, JennimariaPalomaki, OliviaRed-\\n1156–1165, New York, NY, USA. Association for field,MichaelCollins,AnkurParikh,ChrisAlberti,\\nComputingMachinery. DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\\ntonLee,KristinaToutanova,LlionJones,Matthew\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nSimCSE:Simplecontrastivelearningofsentenceem- Uszkoreit,QuocLe,andSlavPetrov.2019. Natural\\nbeddings. In Proceedings of the 2021 Conference Questions: A Benchmark for Question Answering\\nonEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1574}), Document(page_content='onEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-\\nJinhyukLee,WonjinYoon,SungdongKim,Donghyeon\\nminican Republic. Association for Computational\\nKim,SunkyuKim,ChanHoSo,andJaewooKang.\\nLinguistics.\\n2019. BioBERT:apre-trainedbiomedicallanguage\\nrepresentation model for biomedical text mining.\\nYuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto\\nBioinformatics,36(4):1234–1240.\\nUsuyama,XiaodongLiu,TristanNaumann,Jianfeng\\nGao,andHoifungPoon.2021. Domain-specificlan-\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nguagemodelpretrainingforbiomedicalnaturallan-\\nGhazvininejad,AbdelrahmanMohamed,OmerLevy,\\nguageprocessing. ACMTrans.Comput.Healthcare,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\n3(1).\\nBART:Denoisingsequence-to-sequencepre-training\\nfornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2447}), Document(page_content='fornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In\\ningoftheAssociationforComputationalLinguistics,\\nNeurIPSDeepLearningWorksop.\\npages7871–7880,Online.AssociationforComputa-\\ntionalLinguistics.\\nZhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon\\nDuede,KyleChard,andIanFoster.2023. Thedimin- PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-\\nishingreturnsofmaskedlanguagemodelstoscience. ervini,HeinrichKüttler,AleksandraPiktus,Pontus\\nStenetorp,andSebastianRiedel.2021. PAQ:65Mil-\\nShuHuangandJacquelineMCole.2022. Batterybert:\\nlionProbably-AskedQuestionsandWhatYouCan\\nA pretrained language model for battery database\\nDoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3244}), Document(page_content='DoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,\\nVladimirKarpukhin,BarlasOguz,SewonMin,Patrick Pengjun Xie, and Meishan Zhang. 2023. Towards\\nLewis,LedellWu,SergeyEdunov,DanqiChen,and generaltextembeddingswithmulti-stagecontrastive\\nWen-tauYih.2020. Densepassageretrievalforopen- learning.\\ndomainquestionanswering. InProceedingsofthe\\n2020ConferenceonEmpiricalMethodsinNatural YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-\\nLanguageProcessing(EMNLP),pages6769–6781, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nOnline.AssociationforComputationalLinguistics. Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron proach. arXivpreprintarXiv:1907.11692.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3997}), Document(page_content='KyleLo,LucyLuWang,MarkNeumann,RodneyKin- and VERification. In Proceedings of the 2018\\nney,andDanielWeld.2020. S2ORC:Thesemantic Conference of the North American Chapter of\\nscholaropenresearchcorpus. InProceedingsofthe the Association for Computational Linguistics:\\n58thAnnualMeetingoftheAssociationforCompu- Human Language Technologies, Volume 1 (Long\\ntationalLinguistics,pages4969–4983,Online.Asso- Papers), pages 809–819, New Orleans, Louisiana.\\nciationforComputationalLinguistics. AssociationforComputationalLinguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, RobertTinn,HaoCheng,YuGu,NaotoUsuyama,Xi-\\nDarioAmodei,andIlyaSutskever.2019. Language aodong Liu, Tristan Naumann, Jianfeng Gao, and\\nmodelsareunsupervisedmultitasklearners. HoifungPoon.2023. Fine-tuninglargeneurallan-\\nguagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='guagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe HugoTouvron,ThibautLavril,GautierIzacard,Xavier\\nlimitsoftransferlearningwithaunifiedtext-to-text Martinet,Marie-AnneLachaux,TimothéeLacroix,\\ntransformer. JournalofMachineLearningResearch, BaptisteRozière,NamanGoyal,EricHambro,Faisal\\n21(1). Azhar,AurelienRodriguez,ArmandJoulin,Edouard\\nGrave,andGuillaumeLample.2023. Llama: Open\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nandefficientfoundationlanguagemodels.\\nKnowwhatyoudon’tknow:Unanswerablequestions\\nforsquad. CoRR,abs/1806.03822.\\nAashka Trivedi, Takuma Udagawa, Michele Merler,\\nRameswarPanda,YousefEl-Kurdi,andBishwaran-\\nPranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 790}), Document(page_content='PranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in\\nMachineComprehensionofText. InEMNLP.\\nlanguagemodels. arXivpreprintarXiv:2303.09639.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nTakumaUdagawa,AashkaTrivedi,MicheleMerler,and\\nBERT:SentenceembeddingsusingSiameseBERT-\\nBishwaranjanBhattacharjee.2023. Acomparative\\nnetworks. InProceedingsofthe2019Conferenceon\\nanalysisoftask-agnosticdistillationmethodsforcom-\\nEmpiricalMethodsinNaturalLanguageProcessing\\npressingtransformerlanguagemodels. InProceed-\\nandthe9thInternationalJointConferenceonNatu-\\ningsofthe2023ConferenceonEmpiricalMethodsin\\nralLanguageProcessing(EMNLP-IJCNLP),pages\\nNaturalLanguageProcessing: IndustryTrack,pages\\n3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1581}), Document(page_content='3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 2401}), Document(page_content='Beteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria\\nEmilio,CarlesFalcon,SherezadeFuentes,LauraHer- Nicholas Walker, Amalie Trewartha, Haoyan Huo,\\nnandez, Gema Huesa, Jordi Huguet, Paula Marne, SanghoonLee,KevinCruse,JohnDagdelen,Alexan-\\nTaniaMenchón,GrégoryOperto,AlbinaPolo,San- der Dunn, Kristin Persson, Gerbrand Ceder, and\\ndra Pradas, Anna Soteras, Marc Vilanova, and Na- AnubhavJain.2021. Theimpactofdomain-specific\\ntalia Vilor-Tejedor. 2020. Novel tau biomarkers pre-trainingonnamedentityrecognitiontasksinma-\\nphosphorylatedatt181,t217ort231riseintheini- terialsscience. AvailableatSSRN3950755.\\ntial stages of the preclinical alzheimer&#x2019;s\\n<i>continuum</i> when only subtle changes in Liang Wang, Nan Yang, Xiaolong Huang, Binxing\\na&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 3230}), Document(page_content='a&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-\\nsupervisedcontrastivepre-training.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishekSrivastava,andIrynaGurevych.2021. Beir: WenhuiWang,HangboBao,ShaohanHuang,LiDong,\\nAheterogenousbenchmarkforzero-shotevaluation and Furu Wei. 2021. MiniLMv2: Multi-head self-\\nofinformationretrievalmodels. attention relation distillation for compressing pre-\\ntrainedtransformers. InFindingsoftheAssociation\\nJames Thorne, Andreas Vlachos, Christos forComputationalLinguistics: ACL-IJCNLP2021,\\nChristodoulopoulos, and Arpit Mittal. 2018. pages2140–2151,Online.AssociationforComputa-\\nFEVER: a large-scale dataset for fact extraction tionalLinguistics.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 4056}), Document(page_content='Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and Model Training NASA-IR BEIRAvg.\\nTie-Yan Liu. 2013. A theoretical analysis of ndcg INDUS-RETRIEVERSMALL One-Stage 0.73 0.42\\ntyperankingmeasures. InProceedingsofthe26th INDUS-RETRIEVERSMALL Stagewise 0.72 0.41\\nAnnualConferenceonLearningTheory,volume30\\nofProceedingsofMachineLearningResearch,pages\\nTable9: AblationStudy: EvaluationresultsonNASA-\\n25–54,Princeton,NJ,USA.PMLR. QAandBEIR.NASA-QAshowedRecall10whileBEIR\\nreportednDCG10.\\nShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,\\nMarkDredze,SebastianGehrmann,PrabhanjanKam-\\nbadur, David Rosenberg, and Gideon Mann. 2023. C CompleteResultsonBEIRBenchmark\\nBloomberggpt: Alargelanguagemodelforfinance.\\nTable11showstheper-datasetresultsontheBEIR\\nShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao.\\ntasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='tasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 779}), Document(page_content='In Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.\\nA SentenceEmbeddingTrainingData\\nTable 10 shows the various data sources used for\\ntrainingembeddingmodels. Alldataispresented\\nin the form of text-pairs, where each item in the\\npairmaybeasentenceoraparagraph. Inthetable,\\nDataFormatdenotess2pforsentence-to-paragraph\\nmappings,s2sforsentence-to-sentencemappings,\\nandp2pforparagraph-to-paragraphmappings. We\\nusedabout360millionpairsfortrainingandused\\nin-batchnegatives.\\nB AblationStudy: Stage-wiseDistillation\\nforEmbeddingModel\\nFor the distilled embedding models, we find that\\nstage-wisedistillationdoesnotbenefitperformance\\nasmuchasaone-stepprocess, combiningallthe\\nsupervised and unsupervised data. As shown in\\nTable9,thestage-wiseapproachunderperformed\\ntheone-stageapproachby1percentagepointfor\\nbothNASA-QAandonBEIR.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1608}), Document(page_content='Dataset Num. Pairs DataCategory DataFormat\\nStackOverflow† 18562443 Title-Body s2p\\nStackExchangeMath† 2201906 Title-Body s2p\\nS2ORC[title-abstract](Loetal.,2020) 41769185 Title-Body s2p\\nS2ORCCitationPairs[Abstracts](Loetal.,2020) 52603982 Title-Body p2p\\nStackExchange[title-body]† 5415570 Title-Body s2p\\nWikipedia(Faderetal.,2014) 6458670 Title-Body s2p\\nArxiv(Clementetal.,2019) 2358545 Title-Body s2p\\nNASAADS[title-abstract](§2) 2633240 Title-Body s2p\\nPubMed[title-abstract](§2) 24001387 Title-Body s2p\\nPMC[title-abstract](§2) 2585537 Title-Body s2p\\nStackExchangeDuplicateQuestions[title-body-title-body]† 250460 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[body-body]† 250519 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[title-title]† 304525 DuplicateQuestions s2s\\nWikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 0}), Document(page_content='WikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s\\nSQuAD(Rajpurkaretal.,2016) 87599 QuestionAnswers s2p\\nNQ(Kwiatkowskietal.,2019) 100231 QuestionAnswers s2p\\nSearchQA(Dunnetal.,2017) 582261 QuestionAnswers s2p\\nStackExchange[title-answer]† 4067139 QuestionAnswers s2p\\nStackExchange[title-body-answer]† 187195 QuestionAnswers p2p\\nPAQ(Lewisetal.,2021) 64371441 QuestionAnswers s2p\\nFEVER(Thorneetal.,2018)∗ 109810 FactVerification s2p\\nHotpotQA(Yangetal.,2018)∗ 85000 QuestionAnswering s2p\\nTable10:TrainingDataforEmbeddingModels. Thetrainingdatatotalstoaround360Mpairs. DataFormatdenotes\\ns2pforsentence-to-paragraphmappings,s2sforsentence-to-sentencemappings,andp2pforparagraph-to-paragraph\\nmappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 785}), Document(page_content='mappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval\\nTREC- NFCorpus NQ HotPotQA FiQA ArguaAna Touche DBPedia Scidocs FEVER Climate SciFact AVG.\\nCovid FEVER BEIR\\nRoBERTa\\nBASE\\n0.47 0.30 0.54 0.34 0.38 0.52 0.18 0.25 0.22 0.46 0.14 0.67 0.37\\nBGEBASE 0.78 0.37 0.54 0.73 0.41 0.64 0.26 0.41 0.22 0.86 0.31 0.74 0.52\\nINDUS-RETRIEVERBASE 0.56 0.32 0.54 0.49 0.36 0.54 0.17 0.31 0.21 0.56 0.14 0.74 0.41\\nMINILM-V2 0.47 0.32 0.44 0.47 0.35 0.50 0.17 0.32 0.22 0.52 0.25 0.65 0.39\\nBGESMALL 0.76 0.34 0.50 0.70 0.40 0.60 0.26 0.40 0.21 0.87 0.32 0.71 0.51\\nINDUS-RETRIEVERSMALL 0.55 0.31 0.53 0.48 0.29 0.50 0.21 0.33 0.23 0.61 0.23 0.71 0.42\\nTable11: EvaluationresultsBEIR.', metadata={'source': 'https://arxiv.org/pdf/2405.10725', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'start_index': 1607})]\n"
     ]
    }
   ],
   "source": [
    "# total 57 chunks\n",
    "print(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c57698-c4fa-48a0-b435-547ed5bcc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=doc_splits, embedding=CohereEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb1d1d91-b3f5-45b1-b470-024183915e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatCohere(model='command-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abef0bb1-6499-4576-9d17-a3ca04ab90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6114a8aa-e5f7-4647-a3d2-82a4686071d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f908a3ae-2bfd-4376-a34d-6a3cf5a08f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INDUS: Effective and Efficient Language Models for Scientific Applications\\nBishwaranjanBhattacharjee1,AashkaTrivedi1,MasayasuMuraoka1,\\nMuthukumaranRamasubramanian3,TakumaUdagawa1,IkshaGurung3,RongZhang1,\\nBharathDandala1,RahulRamachandran2,ManilMaskey2,KaylinBugbee2,MikeLittle4,\\nElizabethFancher2,LaurenSanders5,SylvainCostes5,SergiBlanco-Cuaresma6,KellyLockhart6,\\nThomasAllen6,FelixGrezes6,MeganAnsdell7,AlbertoAccomazzi6,YousefEl-Kurdi1,\\nDavisWertheimer1,BirgitPfitzmann1,CesarBerrospiRamis1,MicheleDolfi1,RafaelTeixeiradeLima1,\\nPanagiotisVagenas1,S.KarthikMukkavilli1,PeterStaar1,SanazVahidinia7,RyanMcGranaghan8,\\nArminMehrabian9,TsendgarLee7\\n1IBMResearchAI,2 NASAMFSC,3 UAH,4 Navteca,5 NASAAmes,6 Harvard-SmithsonianCfA,\\n7 NASAHQ,8 JPL,9 NASAGSFC\\nAbstract generation tasks. Most popular LLMs rely on the\\ntransformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-\\nlike Wikipedia or CommonCrawl (Devlin et al.,\\nsults on natural language processing (NLP)\\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\\ntasks. However, previous research demon-\\net al., 2020; Brown et al., 2020; Touvron et al.,\\nstratedLLMstrainedusingdomain-focusedcor-\\npora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\\nspired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\\nINDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\\nfortheEarthscience,biology,physics,helio- domain-specific natural language understanding\\nphysics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific\\ncorporadrawnfromdiversedatasources. The\\nLLMs such as SCIBERT (Beltagy et al., 2019),\\nsuiteofmodelsinclude: (1)anencodermodel\\ntrainedusingdomain-specificvocabularyand BIOBERT (Lee et al., 2019), MATBERT (Walker\\ncorpora to address natural language under- et al., 2021), BATTERYBERT (Huang and Cole,\\nstandingtasks,(2)acontrastive-learning-based 2022)andSCHOLARBERT(Hongetal.,2023)were\\ngeneral text embedding model trained using developedwiththegoalofimprovingaccuracyon\\na diverse set of datasets drawn from multi- in-domainNLPtasks(Leeetal.,2019;Araci,2019;\\nple sources to address information retrieval\\nWuetal.,2023).\\ntasks and (3) smaller versions of these mod-\\nels created using knowledge distillation tech-\\nInthisresearch,wespecificallyfocusedoninter-\\nniquestoaddressapplicationswhichhavela-\\ndisciplinaryfieldsrelatedtotheEarth,celestialbod-\\ntencyorresourceconstraints. Wealsocreated\\nies, the Sun, and planets within our solar system\\nthreenewscientificbenchmarkdatasetsnamely,\\nsuchasphysics,Earthscience,astrophysics,helio-\\nCLIMATE-CHANGE NER (entity-recognition),\\nNASA-QA(extractiveQA)andNASA-IR(IR)to physics,planetarysciencesandbiology. Whilethe\\naccelerateresearchinthesemulti-disciplinary trainingcorporaofexistingdomain-specificmod-\\nfields. Finally, we show that our mod- elssuchasSCIBERT,BIOBERTandSCHOLARBERT\\nelsoutperformbothgeneral-purposeencoders partially cover some of these fields, there is cur-\\n(RoBERTa) and existing domain-specific en-\\nrentlynospecificmodelavailablethatencompasses\\ncoders(SCIBERT)onthesenewtasksaswell\\nallofthefieldsofinterestcollectively. Further,the\\nasexistingbenchmarktasksinthedomainsof\\ninterdisciplinarynatureofthesedomainsofinter-\\ninterest.\\nestisreflectedinavastbodyofliteraturescattered\\n1 Introduction acrossdiversesources. Thus,wedevelopedINDUS,\\na collection of encoder-based LLMs focused on\\nLarge language models (LLMs) trained on huge thesedomainsofinterest(Figure1)trainedusing\\namountsofdatahavedemonstratedimpressiveca- meticulouslycuratedcorporafromdiversesources.\\npabilities on natural language understanding and Webelievethisworkwillfacilitateresearchorgani-\\nzationsandenterprisesworkinginthesefieldsby\\nContact:bhatta@us.ibm.com,aashka.trivedi@ibm.com,\\nmuthukumaranr17@gmail.com,rahul.ramachandran@nasa.gov providingefficientaccesstorelevantliteratureand\\n4202\\nyaM\\n02\\n]LC.sc[\\n2v52701.5042:viXra\\n\\n\\nEarth Science Data Pretraining\\n(Masked Finetuning Scientific Corpora\\nBioMedical Data Language Indus-Base (Contrastive\\nModelling) Learning) Open QA\\nAstrophysics Data\\nDuplicate Pairs\\nAstronomy Data\\nRepresentation Indus-Retriever-Base Citation Pairs\\nGeneral Science Distillation\\nData\\nFact Verification\\nGeneral English\\nData\\nGeneric Corpora\\nOutput\\nEncoder Training Corpus Indus-Small Distillation Embedding Training\\nCorpus\\nData Teacher (KD)\\nInitialization Output Indus-Retriever-\\nSmall\\nBC5-CHEM BC5-Disease NCBI-Disease BC2GM\\nNASA-QA TREC-Covid NFCorpus NQ HotpotQA\\nJNLPBA EBM-PICO ChemProt DDI\\nClimate FiQA Arguana Touche DBPedia NASA-IR\\nGAD HoC PubMedQA BioASQ Change NER\\nSciDocs FEVER C Fl Eim VEat Re SciFact\\nBIOSSES BLURB Benchmark BEIR Benchmark\\nNatural Language Understanding Benchmarks Information Retrieval Benchmarks\\nFigure1: OverviewofINDUSmodels: thegeneral-purposeencodermodelandtheretrieverbuiltfromit,andtheir\\ndistilled counterparts. Also shown are the benchmarks used for evaluation, highlighting our new benchmarks,\\nNASA-QA,CLIMATE-CHANGENERandNASA-IR.\\nenablingthemininformeddecision-making. (Beltagy et al., 2019). We also show that the\\nSpecifically, we make the following contribu- knowledge-distilledmodelsachievedasignifi-\\ntions: cantboostinlatencywhilemaintainingstrong\\n1. Utilizing the byte-pair encoding algorithm, empiricalperformancecomparedtotheoriginal\\nwe constructed INDUSBPE, a customized to- modelsonmostofthebenchmarktasks.\\nkenizerfromthecuratedscientificcorpus.\\n2 Data\\n2. Wepretrainedmultipleencoder-onlyLLMsus-\\ning the curated scientific corpora and the IN-\\nSufficienthigh-qualityin-domaincorporaisessen-\\nDUSBPEtokenizer(§2,§3). Wefurthercreated\\ntial to develop models that perform better than\\nsentence-embeddingmodelsbyfine-tuningthe\\ntheircounterpartstrainedonopen-domaincorpora.\\nencoder-only models with a contrastive learn-\\nWemeticulouslyidentifiedcorporaforeachofthe\\ningobjectivetolearn“universal”sentenceem-\\naforementioneddomains,andcreatedEnglish-only\\nbeddings (Gao et al., 2021) (§4). We also\\nmodels for the sake of containment. Specifically,\\ntrained smaller, more efficient versions of\\nforeachofthedomains,weusedopen-sourcedata\\nthesemodelsusingknowledge-distillationtech-\\nwhich has a permissive license, and further aug-\\nniques(§3.3,§4.2).\\nmentedthemwithspecificdatafromNASAandits\\n3. We created three new scientific benchmark\\ndata providers. To aid in the learning of general\\ndatasets, CLIMATE-CHANGE NER (an entity English, we also included English Wikipedia in\\nrecognitiontask),NASA-QA(anextractiveques-\\nourtrainingcorpora. Webrieflydescribeeachdata\\ntionansweringtask)and NASA-IR (aretrieval\\nsourcebelow,andpresentstatisticsofthedatain\\ntask)(§5)tofurtheraccelerateresearchinthis\\nTable1.\\nmulti-disciplinaryfield.\\n• SAO/NASAAstrophysicsDataSystem(ADS)1:\\n4. Throughexperimentalresults,weshowstrong\\nADSisthebiggestsourceofdata,coveringpub-\\nperformancebyourmodelsonthesebenchmark\\nlications in the areas of astronomy and astro-\\ntasks as well as on existing domain-specific\\nphysics,physicsandgeneralscienceincluding\\nbenchmarks, outperforming general-purpose\\nallarXive-prints.\\nmodels like RoBERTa (Liu et al., 2019) as\\nwellasscientific-domainencoderslikeSCIBERT 1https://ui.adsabs.harvard.edu\\n\\n\\nDataset Domain #Tokens Ratio Tokenizer ADS PMC Wikipedia\\nNASACMR EarthScience 0.3B 1% RoBERTa 12,867,439 7,549,075 15,859\\nAMSandAGUpapers EarthScience 2.8B 4% +lower_cased 12,862,227 7,557,868 16,901\\nEnglishWikipedia General 5.0B 8% INDUSBPE 12,309,023 6,920,659 16,056\\nPubMedAbstracts Biomedical 6.9B 10%\\nTable2: NumberoftokensproducedbyRoBERTaand\\nPMC Biomedical 18.5B 28%\\nINDUSBPEtokenizersappliedto1ksamplesfromeach\\nSAO/NASAADS Astronomy, 32.7B 49%\\ndataset. Fewer tokens lead to a smaller computation\\nAstrophysics,\\ncost.\\nPhysics,\\nGeneralScience\\nTotal 66.2B 100% ingdataset(§2)6. Forafaircomparison,wesetthe\\nvocabularysizeto50,265,whichisequaltothatof\\nTable1: Basicstatisticsofourpretrainingdataset.\\nthe RoBERTatokenizer(Liuetal.,2019)andused\\ntheuncasedvariationofboththetokenizers.\\n• PubMedCentral(PMC)2 : PMC isafull-text Weperformedabriefanalysistounderstandthe\\narchiveofbiomedicalandlifesciencejournal differences between the vocabularies of INDUS-\\nliterature maintained by National Library of BPE and the RoBERTa tokenizer. Out of 50,265\\nMedicineandNationalInstitutesofHealth. We tokens,22,355(44.5%)tokensarecommoninboth\\nusedtheportionofPMCthathasacommercial- thetokenizerswhiletheremaining27,910(55.5%)\\nfriendly license, along with the PubMed ab- tokensareincludedonlyineithertokenizer, indi-\\nstractsofallthearticlesinPMC. catingasignificantdistributionalshiftindomain.\\n• American Meteorological Society (AMS)3: Tofurther understandthe effect, weapplied both\\nWeusedfull-textjournaldocumentsspanning RoBERTaandINDUSBPEon1,000randomlysam-\\ntopicsinEarthsystems,Earthinteractions,ap- pledtextfragmentsfromourdatasets. Thesetext\\nplied meteorology and climatology, physical fragmentsvariedfromfulldocumentstoabstracts\\noceanography, atmospheric sciences, climate, tosinglesentences. AsshowninTable2, INDUS-\\nhydrometeorology, weather and forecasting, BPE tokenizer produced fewer tokens than the\\nandsocietalimpacts. RoBERTa tokenizer, leading to 8˜% drop in com-\\n• American Geophysical Union (AGU)4: The putationcostduringtraining.\\nAGUdatasetincludedjournaldocumentsacross Table3comparestheRoBERTatokenzierandIN-\\nthe topics of atmospheres, biogeosciences, DUSBPE tokenizer,illustratingthattheproposed\\nEarth surface, machine learning and compu- tokenizertreatedscientificterms(suchasbiomak-\\ntation, oceans, planets, solid earth, and space ers, phosphorylated, alzheimer) as single tokens\\nphysics. while RoBERTa tokenizer splits these words into\\n• NASA Common Metadata Repository multiplesubwordpieces.\\n(CMR)5: CMR is a high-performance, high-\\n3.2 EncoderModel\\nquality,continuouslyevolvingmetadatasystem\\nthat catalogs all data and service metadata Wefirsttrainedanencoder-onlymodel,INDUS ,\\nBASE\\nrecords for NASA’s Earth Science Data and usingamaskedlanguagemodelingobjective. The\\nInformation System (ESDIS). It contains text modelarchitecturefollowsRoBERTa (Liuetal.,\\nBASE\\ndescriptions of the NASA Earth science data 2019),whichconsistsof12layersandhas125M\\nproducts. parameters. Weadoptedthedefaulthyperparame-\\nters7 butwithaneffectivebatchsizeof92,16. We\\n3 Methodology: EncoderModels trainedthemodelfor500Kstepsusing192V100\\nGPUs.\\n3.1 INDUSBPE Tokenizer\\n3.3 KnowledgeDistillationforEfficient\\nWe trained BPE tokenizer (Radford et al., 2019),\\nEncoderModel\\nINDUSBPEfromscratchusingasubsetofourtrain-\\nWealsotrainedasmallermodel,INDUS ,with\\nSMALL\\n2https://www.ncbi.nlm.nih.gov/pmc 38M parameters through knowledge distillation\\n3https://www.ametsoc.org/index.cfm/ams/publications/\\n4https://agupubs.onlinelibrary.wiley.com/ 6We used HF tokenizers, https://github.com/\\n5https://www.earthdata.nasa.gov/eosdis/science-system- huggingface/tokenizers\\ndescription/eosdis-components/cmr 7WereferreaderstoTable9in(Liuetal.,2019).\\n\\n\\nInputtext\\nnoveltaubiomarkersphosphorylatedatt181,t217ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected\\nTokenizationbyRoBERTa\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTokenizationby INDUSBPE\\n<s>noveltaubiomarkersphosphorylatedatt181,t217,ort231riseintheinitialstagesofthepreclinical\\nalzheimer’scontinuumwhenonlysubtlechangesinapathologyaredetected</s>\\nTable3: TokenizationcomparisonbetweenRoBERTaandourtokenizers. InputtextadaptedfromSuárez-Calvet\\netal.(2020).\\n(cid:88) (cid:88)\\ntechniquesbyusingINDUS\\nBASE\\nastheteacher. IN- Z\\ni\\n= es(qi,pj)+ es(qj,pi)\\nDUS follows a 4-layer architecture recom- j j\\nSMALL (2)\\nmendedbytheNeuralArchitectureSearchengine +(cid:88) es(qi,qj)+(cid:88) es(pi,pj)\\n(Trivedietal.,2023)withanoptimaltrade-offbe-\\nj̸=i j̸=i\\ntween performance and latency. We adopted the\\nwheres(q,p)isameasureoftemperature-scaled\\ndistillationobjectiveproposedinMiniLMv2(Wang\\ncosinesimilaritybetweentheembeddingsofquery\\netal.,2021)totransferfine-grainedself-attention\\nandapassagemeasuredby:\\nrelations,whichhasbeenshowntobethecurrent\\nstate-of-the-art(Udagawaetal.,2023). Usingthis 1 E(q)·E(p)\\ns(q,p) = (3)\\nobjective,wetrainedthemodelfor500Kstepswith τ ∥E(q)∥∥E(p)∥\\naneffectivebatchsizeof480on30V100GPUs.\\nTrainingData Similartopriorwork(Wangetal.,\\n2022; Li et al., 2023; Xiao et al., 2023), we em-\\n4 Methodology: SentenceEmbedding\\nployedastage-wisetrainingapproachforoursen-\\nModels\\ntenceembeddingmodel:\\n4.1 SentenceEmbeddingModel 1. Unsupervised training: we first trained on a\\nlargecorpusof300millionsamplesofnaturally\\nTextembeddingsrepresenttextaslow-dimensional\\noccurringpairscollectedfrominternetsources,\\nvectors,allowingforefficientuseindenseretrieval\\nsuch as Wikipedia, StackExchange, etc. We\\nsystems,whererelevantpassagesforagivenquery\\nalsoincludedscientificdatafromPubMed,PMC\\nareidentifiedonthebasisofthesimilaritybetween\\n(§2),Arxivand S2ORC (Loetal.,2020)asin-\\ntheirembeddings(Karpukhinetal.,2020).\\ndomaindataforourscience-orientedretriever\\nmodel. Furthermore, we created a domain-\\nContrastiveLearningObjective Sentenceem-\\nspecific dataset from the ADS data (§2) by in-\\nbeddingmodelstrainedusingacontrastivelearn-\\ncludingtitle-abstractpairs.\\ningobjective(Khoslaetal.,2020;Gaoetal.,2021)\\n2. Supervised fine-tuning: we further trained on\\npushestheembeddingsofaqueryclosertothatof\\nhigh-quality annotated datasets, such as NQ\\narelevantpassageandfurtherawayfromthatofa\\n(Kwiatkowskietal.,2019),SQuAD(Rajpurkar\\nnon-relevantpassage.\\net al., 2016), SPECTER pairs (Cohan et al.,\\nInspired by recent work (Li et al., 2023), we\\n2020), etc. We included the aforementioned\\nusedanimprovedcontrastivelossbyintroducing\\nADS data and a sample of the S2ORC data in\\nanadditionalbidirectionalsignal. Specifically,for\\nthisstep,toboostdomain-specificsignals.\\na triple {q,p+,P−} of a query, a relevant (posi-\\nAppendixAcontainscomprehensivedetailsabout\\ntive)passage,andasetofnon-relevant(negative)\\nthe datasets used in training. For both training\\npassagesP− = {p−}m ,WedefinetheInfoNCE\\nj j=1 stages,weusedlargebatchsizesandin-batchnega-\\nloss(vandenOordetal.,2019)as:\\ntivestobetterapproximatethecontrastivelearning\\nobjective. During training, we sampled batches\\n1 (cid:88)n es(qi,p+ i ) from each data source proportionately to its size,\\nL = − log (1)\\nIC\\nn Z i similartoLietal.(2023).\\ni=1\\n\\n\\nModel Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K\\ninthelargermodelbutimprovestheperformance\\nstepsfollowedbyanadditional100Kstepswiththe\\nofthesmallermodel.\\nsuperviseddata. Weusedalearningrateof2e−5\\nFor distilling the sentence embedding model,\\nduringboththesesteps.\\nwefoundthatastage-wisetrainingapproachdoes\\n4.2 KnowledgeDistillationforEmbedding not benefit performance as much as in the non-\\nModels distillation case (ablation presented in Appendix\\nB). We thus distilled in a single step with all the\\nTooptimizethelatencyforretrievalapplications,\\ndatadescribedin§4.1andAppendixAandadded\\nwe also created a small retriever model with the\\nlabelled pairs from FEVER (Thorne et al., 2018)\\naim to transfer the capability of the large teacher\\nandHOTPOTQA(Yangetal.,2018).\\nmodel(INDUS-RETRIEVER )tosmallerstudent\\nBASE\\nmodel(INDUS SMALL),bydistillingtheteacher’sdis- ModelSpecifications Webuiltthesentenceem-\\ntributionofsimilarityscores. Furthermore,wefind beddingmodelbydistillinginto INDUS . This\\nSMALL\\nthatitisnecessarytomodifythetrainingstrategy isa4-layermodelwithanembeddingdimension\\nfordistillation,asdescribedbelow. of576. Werefertotheresultingretrievermodelas\\nDistillationLoss Weusedknowledgedistillation\\nINDUS-RETRIEVER SMALL. Itfollowsabi-encoder\\nframework,andherewefindthatusingthevector\\ntechniquesintroducedin(Xuetal.,2023). Specif-\\nrepresentationofthefirsttokenastheembedding\\nically, for a sentence x and its corresponding in-\\ni\\nbatchelementpairs{x ,x }m ,weminimized (CLS pooling)givesbetterperformancethanusing\\ni j j=1,j̸=i\\nmeanpooling.\\nthe cross entropy between the teacher’s distribu-\\ntionp ofsimilarityscoresbetweenpairsandthe\\nt TrainingDetails FortheRetro-MAEstylepre-\\nstudent’sdistribution, p . FollowingHintonetal.\\ns training(Xiaoetal.,2022),wetrainedon8A100\\n(2014), we also scaled the output distribution of\\nGPUs with an effective batch size of 128 for 2\\nbothteacherandstudentbyatemperature,τ :\\nKD epochswithalearningrateof2e−5. Forthestage-\\nn m wisedistillation,wetrainedon2A100GPUsfor\\n(cid:88)(cid:88)\\nL = − p (x ,x )logp (x ,x ) (4) 300K steps with an effective batch size of 2,048,\\nKD t i j s i j\\ni=1 j=1 andlearningrateof7e−4. Throughexperimenta-\\ntion,Wefoundthatτ = 4performedthebest.\\nKD\\ness(xi,xj)/τKD\\np (x ,x ) = (5)\\ns i j (cid:80)m ess(xi,x k)/τKD 5 CreatingBenchmarks\\nk=1\\nest(xi,xj)/τKD Benchmark datasets play a crucial role in assess-\\np (x ,x ) = (6)\\nt i j (cid:80)m est(xi,x k)/τKD ingthelanguageunderstandingcapabilitiesmodels.\\nk=1\\nHowever,tothebestourknowledge,thereisano-\\nHere,s (x ,x )ands (x ,x )representthesim-\\ns i j t i j\\nticeableabsenceofdatasetstailoredforthediverse\\nilarityscoresbetweentwopairs{x ,x },definedin\\ni j\\nand multidisciplinary field under study. Thus, to\\nEquation3forthestudentandteacherrespectively.\\neffectively benchmark the proposed NLP models\\nTrainingData Wefirstconductedaembedding- and further accelerate research in this multidisci-\\noriented pretraining step, as presented in Retro- plinarydomain,Weintroducedthreenewdatasets,\\nMAE (Xiao et al., 2022), on English Wikipedia, anNERtask,aQAtask,andanIRtask,described\\nBooksCorpus,andStackExchangedata,totalling below.\\n\\n\\nTrain Validation Test spansoftheparagraphwhichanswerthequestions.\\nNum.Abstracts 382 77 75\\nWeused29paragraphs(with145QApairsintotal)\\nNum.Tokens 32,031 6,443 5,850\\nEntityLabels asthetrainingsetandtheremaining10paragraphs\\nclimate-nature,climate-greenhouse-gases,climate-assets,\\n(with 50 questions in total) as the evaluation set.\\nclimate-problem-origins,climate-mitigations,\\nclimate-properties,climate-impacts,climate-datasets, Thetrainingsetwasfurtheraugmentedwithpara-\\nclimate-organizations,climate-observations,\\ngraphsand QA pairsrelatedtoEarthsciencefrom\\nclimate-models,climate-hazards,climate-organisms\\nthe SQuADdataset(Rajpurkaretal.,2018). Specif-\\nTable4: CLIMATE-CHANGE NER statisticsandentity ically,thoserelatedtooxygen,Amazonrainforest\\nlabels\\nandgeologywereused. Thisresultedinapruned\\nSQuADsetcomprising686paragraphswith5,081\\nquestions(2,817answerableand2,264unanswer-\\n5.1 CLIMATE-CHANGE NER\\nable). Weevaluatedtheperformanceofthemodels\\nWhiletraditionalsearchenginesanddatabasesof-\\nby augmenting these SQuAD pairs to the training\\nfer some assistance in exploring data related to\\ndatasourcedfromEarthsciencepapers,whilekeep-\\nclimatechange,thecomplexityofclimate-related\\ningtheevaluationsetintact.\\nqueries often requires more sophisticated natural\\nlanguageprocessingtechniques. Thisnecessityis 5.3 NASA-IR\\nunderscoredbytheextensivearrayofclimatemod-\\nWe introduced a domain-specific information re-\\nels, datasets, and organizations involved, which\\ntrieval benchmark, NASA-IR10, spanning almost\\ndemand meticulous curation and continuous up-\\n500question-answerpairsrelatedtotheEarthsci-\\ndates. While databases like those maintained by\\nence,planetaryscience,heliophysics,astrophysics\\nNASA or the UN provide valuable observational\\nandbiologicalphysicalsciencesdomains. Specif-\\ndata,comprehensiveoverviewsofclimatemodels\\nically, we sampled a set of 166 paragraphs from\\nandimpactassessmentsarescarceandnoteasily\\nAGU, AMS, ADS, PMC and Pubmed (§2) and\\naccessible.\\nmanually annotated with 3 questions that are an-\\nInordertobridgethisgap,weintroducedacom-\\nswerablefromeachoftheseparagraphs,resulting\\nprehensive dataset for developing and evaluating\\nin498questions. Weused398ofthesequestions\\nNLP models tailored towards understanding and\\nas the training set and the remaining 100 as the\\naddressing climate-related topics across various\\nvalidation set. To comprehensively evaluate the\\ndomains. Specifically, we created a new manu-\\ninformation retrieval systems and mimic the real\\nallyannotateddatasetCLIMATE-CHANGE NER8,in\\nworlddata,Wecombined26,839randomADSab-\\nwhichthenamedentitiesofinterestoriginatefrom\\nstracts with these annotated paragraphs. On an\\ncomplextaxonomiesusedinclimate-relatedlitera-\\naverage, each query is 12 words long, and each\\nture. Thisdatasetcomprises534abstractssourced\\nparagraphis120wordslong. WeusedRecall@10\\nfromSemanticScholarAcademicGraph(Kinney\\nasevaluationmetricsinceeachquestionhasonly\\netal.,2023),collectedusingaseedsetofclimate-\\nonerelevantdocument.\\nrelated keywords such as wildfire or floods. The\\nabstractswereannotatedusingtheIOB(inside,out-\\n6 ExperimentalResults\\nside,beginning)taggingschemeandencompasses\\nadiversearrayofentitytypes,asshowninTable4. Baselines Wecompared INDUS modelsagainst\\nopensourcemodelsofsimilarsizes:\\n5.2 NASA-QA • INDUS was compared to RoBERTa 11\\nBASE BASE\\nWepresent NASA-QA9,anextractivequestionan- andSCIBERT12.\\nsweringtaskfocusedontheEarthsciencedomain. • INDUS was compared to MINILM (6-\\nSMALL\\nFirst, 39 paragraphs from Earth science papers\\nlayer)13 andTINYBERT(4-layer)14.\\nwhichappearedinAGUandAMSjournals(§2)were\\n10https://huggingface.co/datasets/nasa-impact/nasa-smd-\\nsourced. Subject matter experts from NASA for- IR-benchmark\\nmulatedquestionsandmarkedthecorresponding 11https://huggingface.co/FacebookAI/roberta-base\\n12https://huggingface.co/allenai/scibert_scivocab_uncased\\n8https://huggingface.co/datasets/ibm/Climate-Change- 13https://huggingface.co/nreimers/MiniLM-L6-H384-\\nNER uncased\\n9https://huggingface.co/datasets/nasa-impact/nasa-smd- 14https://huggingface.co/huawei-\\nqa-benchmark noah/TinyBERT_General_4L_312D\\n\\n\\nBasemodel(125Mparams.) Smallmodel(∼30Mparams.)\\nTask Metric Dataset RoBERTa SCIBERT INDUSBASE TINYBERT MINILM INDUSSMALL\\nBC5-chem 90.3(0.2) 91.4(0.2) 93.3(0.2) 84.6(0.2) 86.1(0.3) 90.7(0.1)\\nBC5-disease 81.5(0.3) 83.7(0.3) 85.2(0.3) 74.0(0.4) 77.4(0.3) 81.3(0.3)\\nNER EntityF1 NCBI-disease 87.6(0.6) 87.6(0.4) 88.3(0.4) 81.2(0.4) 83.1(0.5) 85.6(0.6)\\nBC2GM 82.1(0.3) 82.3(0.2) 84.0(0.3) 74.7(0.4) 77.1(0.2) 79.7(0.3)\\nJNLPBA 79.1(0.2) 78.2(0.2) 80.3(0.2) 70.3(0.2) 73.4(0.3) 75.7(0.2)\\nPICO MacroF1 EBMPICO 72.3(0.3) 72.4(0.3) 73.1(0.2) 67.4(0.2) 70.3(0.1) 73.1(0.2)\\nChemProt 50.4(28.2) 73.9(0.7) 76.9(0.5) 56.2(3.2) 55.9(2.1) 71.7(0.9)\\nRelation\\nMicroF1 DDI 78.6(1.5) 80.1(1.0) 81.7(0.5) 39.3(5.3) 51.5(2.9) 69.0(1.2)\\nExtraction\\nGAD 80.0(1.1) 81.6(1.2) 79.4(5.6) 76.4(1.3) 77.3(1.0) 81.3(0.7)\\nDocument\\nClassification MicroF1 HoC 82.2(0.7) 83.1(0.6) 83.7(0.5) 41.6(6.8) 62.8(4.7) 80.2(0.6)\\nQuestion PubMedQA 53.1(3.3) 54.3(3.8) 58.2(6.7) 50.3(1.4) 51.6(1.7) 56.1(1.4)\\nAccuracy\\nAnswering BioASQ 69.1(4.8) 74.6(4.5) 69.6(5.8) 74.3(3.6) 66.7(2.3) 75.4(3.3)\\nSentence\\nSimilarity Pearson BIOSSES 79.8(6.3) 86.3(3.5) 72.2(9.5) 88.2(1.1) 26.6(8.7) 70.4(3.3)\\nMicroAverage - - 75.9(3.7) 79.2(1.3) 78.9(2.4) 67.6(1.9) 66.1(1.9) 76.2(1.0)\\nMacroAverage - - 74.9(3.7) 78.2(1.6) 76.4(3.2) 65.6(2.4) 60.6(3.0) 74.3(1.3)\\nTable5:EvaluationresultsonBLURB.Resultsreportedareaveragedon10randomseedswithstandarddeviationin\\nparenthesis. Microaverageisreportedacrossdatasetswhilemacroaverageiscomputedbyfirstaveragingscoreson\\neachtask(say,taskaverage),followedbyaveragingthetaskaverageacrosstasks. Resultsinboldindicatehighest\\nperformancewhileunderlinedresultsindicatesignificantdifferencefromsecondhighestresultbymorethantwo\\nstandarddeviationsineachmodelsize.\\n• INDUS-RETRIEVER was compared to Model F1(SD)\\nBASE\\nBGE 15 andaRoBERTa modelfinetuned RoBERTa 60.8(0.8)\\nBASE BASE\\nwiththesamemethodpresentedin§4.1. SCIBERT 61.8(0.7)\\n• INDUS-RETRIEVER\\nSMALL\\nwas compared to INDUS\\nBASE\\n64.0(1.0)\\nMINILM-V216 and BGE 17. TINYBERT 34.3(1.6)\\nSMALL\\nMINILM 44.7(1.3)\\n6.1 NaturalLanguageUnderstanding INDUS 54.8(0.8)\\nSMALL\\nBenchmarks\\nTable 6: CLIMATE-CHANGE NER benchmark results.\\nWe evaluated our models on BLURB (Gu et al.,\\nStandard deviation over 10 random seeds shown in\\n2021),abenchmarksuitefornaturallanguageun-\\nparenthesis. Resultsinboldandunderlineindicatehigh-\\nderstandingandreasoningtasksinthebiomedical estperformanceandsignificantdifferencefromsecond\\ndomain. Wefollowedtheoriginalworktocompute highestresultbymorethantwostandarddeviationsin\\ntheoverallscore(i.e.,macroaverage). eachmodelsize,respectively.\\nTable 5 shows the evaluation results. Among\\nbase models, INDUS BASE significantly outper- WealsonoticedSCIBERTtendstoperformbetter\\nformsthegeneral-purposeRoBERTamodelonmi-\\nthan our model on paired input-text tasks, such\\ncro/macroaveragewhileachievingcompetitiveper-\\nas QA and semantic similarity tasks, although\\nformance to the bio-domain-specific counterpart,\\nthe results have relatively large standard devia-\\nSCIBERT.\\ntions. We hypothesized that the additional next\\nAsforsmallermodels,wenoticed INDUS SMALL sentence prediction objective during training in\\noutperformed the baselines, TINYBERT and BERT-stylemodels(suchasSCIBERT)incontrastto\\nMINILM, by a large margin in most cases, show-\\ntheRoBERTa-stylemodels(suchasRoBERTa\\nBASE\\ningsignificantdifferencefromsecondbestmodels\\nandINDUS)maybebeneficialforpairedinput-text\\ninNER,PICO,relationextraction,anddocument\\ntasks. Thistrendwasconsistentwiththeobserva-\\nclassification tasks. This demonstrates the effec-\\ntionsofTinnetal.(2023).\\ntivenessofknowledgedistillationfromourdomain-\\nspecificteachermodel,INDUS . 6.2 CLIMATE-CHANGE NER\\nBASE\\nAs shown in Table 6, our models clearly outper-\\n15https://huggingface.co/BAAI/bge-base-en-v1.5\\n16sentence-transformers/all-MiniLM-L6-v2 formedthecorrespondingbaselinemodelsonthe\\n17https://huggingface.co/BAAI/bge-small-en-v1.5 CLIMATE-CHANGE NERtask,suggestingtheeffec-\\n\\n\\ntivenessoftrainingonlargedomain-specificdata. Model NASA-IR↑ BEIRAvg.↑ Retrieval\\nTime↓\\n6.3 NASA-QA BRo GB EE BAR ST Ea BASE 0 0. .6 66\\n7\\n0 0. .3 57\\n2\\n1 1. .2 10\\n8\\nAs mentioned in §5, we augmented the training\\nINDUS-RETRIEVERBASE 0.71 0.41 1.19\\nMINILM-V2 0.62 0.39 0.24\\nsetwithrelevantSQuADpairsforfine-tuning. All BGESMALL 0.66 0.51 0.42\\nmodelsarefinetunedfor15epochs,andtheresults INDUS-RETRIEVERSMALL 0.73 0.42 0.26\\nareshowninTable7. WeobservedthatINDUS\\nBASE Table 8: Evaluation results on NASA-IR and BEIR.\\noutperformedallmodelsofsimilarsizes,while IN-\\nNASA-IRshowedRecall@10whileBEIRreportedthe\\nDUS hadrelativelystrongperformancecom- averagenDCG@10acrossalltasks. Retrievaltimeper\\nSMALL\\nparedtoitscounterparts. queryontheNQtaskfromBEIR,reportedinseconds.\\nModel F1(SD)\\n7 Conclusions\\nRoBERTa 66.8(3.1)\\nSCIBERT 63.5(1.9) Inthisresearch,wepresented INDUS,aconstella-\\nINDUS 68.2(2.9) tionofmodelsforuseinthesciencedomain. We\\nBASE\\nTINYBERT 43.2(2.3) demonstrated the effectiveness of a custom tok-\\nMINILM 59.2(3.9) enizer and in-domain data for training high qual-\\nINDUS 47.4(1.8) ityencodermodelsandsentenceembeddingmod-\\nSMALL\\nels. Further, we created smaller versions of the\\nTable7: NASA-QA benchmarkresults. Standarddevi- proposedmodelssuitableforapplicationswithla-\\nation over 3 random seeds shown in parenthesis. Re-\\ntencyorresourceconstraintsthroughstate-of-the-\\nsultsinboldandunderlineindicatehighestperformance\\nartknowledgedistillationtechniques. Fortheben-\\nandsignificantdifferencefromsecondhighestresultby\\nefit of the scientific community, we will release\\nmorethantwostandarddeviationsineachmodelsize,\\nthedevelopedmodelsandbenchmarkdatasetson\\nrespectively.\\nHuggingFace.\\nWesawthatINDUS outperformedallmodels\\nBASE\\nofsimilarsizes,while INDUS SMALL hadrelatively References\\nstrongperformance.\\nDoguAraci.2019. Finbert: Financialsentimentanaly-\\nsiswithpre-trainedlanguagemodels.\\n6.4 InformationRetrievalBenchmarks\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\\nWeevaluatedourmodelsonthe NASA-IR dataset\\nERT:Apretrainedlanguagemodelforscientifictext.\\naswellasBEIRBenchmark(Thakuretal.,2021), InProceedingsofthe2019ConferenceonEmpirical\\nwhichconsistsof12retrievaltasksspanningavari- Methods in Natural Language Processing and the\\n9thInternationalJointConferenceonNaturalLan-\\netyofdomains. TheBEIRbenchmarkusedtheNor-\\nguageProcessing(EMNLP-IJCNLP),pages3615–\\nmalized Cumulative Discount Gain (nDCG@10)\\n3620,HongKong,China.AssociationforComputa-\\n(Wang et al., 2013) as their main metric. Table 8 tionalLinguistics.\\nshowstheperformanceofourdomain-specificsen-\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\ntenceembeddingmodels,alongwithourbaselines.\\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\\nAs shown, both of our sentence embedding mod- Neelakantan,PranavShyam,GirishSastry,Amanda\\nelssignificantlyoutperformedthebaselinesonthe Askell, Sandhini Agarwal, Ariel Herbert-Voss,\\nNASA-IRtaskwhilestillmaintaininggoodperfor- Gretchen Krueger, Tom Henighan, Rewon Child,\\nAdityaRamesh,DanielZiegler,JeffreyWu,Clemens\\nmanceonseveraloftheBEIRtasks. (Wepresented\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nresultsforeachBEIRtaskinAppendixC).\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\\nWe also measured the average time per query Clark, ChristopherBerner, SamMcCandlish, Alec\\nforretrievalonthe4,202testqueriesofthenatural Radford, Ilya Sutskever, and Dario Amodei. 2020.\\nLanguage models are few-shot learners. In Ad-\\nquestionssetofBEIR,onasingleA100GPU.This\\nvances in Neural Information Processing Systems,\\ntime includes the time to encode the query, cor-\\nvolume 33, pages 1877–1901. Curran Associates,\\npus,andtimetoretrieverelevantdocuments. No- Inc.\\ntably, INDUS-RETRIEVER outperformed IN-\\nSMALL Colin B. Clement, Matthew Bierbaum, Kevin P.\\nDUS-RETRIEVER BASE,onbothNASA-IRandBEIR,\\nO’Keeffe, and Alexander A. Alemi. 2019. On the\\nwhilebeingabout4.6xfaster. useofarxivasadataset.\\n\\n\\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug Sarna, Yonglong Tian, Phillip Isola, Aaron\\nDowney, and Daniel S. Weld. 2020. SPECTER: Maschinot, CeLiu, andDilipKrishnan.2020. Su-\\nDocument-level Representation Learning using pervisedcontrastivelearning. InAdvancesinNeural\\nCitation-informedTransformers. InACL. InformationProcessingSystems,volume33,pages\\n18661–18673.CurranAssociates,Inc.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of Rodney Kinney, Chloe Anastasiades, Russell Authur,\\ndeepbidirectionaltransformersforlanguageunder- Iz Beltagy, Jonathan Bragg, Alexandra Buraczyn-\\nstanding. InProceedingsofthe2019Conferenceof ski,IsabelCachola,StefanCandra,YoganandChan-\\ntheNorthAmericanChapteroftheAssociationfor drasekhar, Arman Cohan, Miles Crawford, Doug\\nComputationalLinguistics: HumanLanguageTech- Downey, Jason Dunkelberger, Oren Etzioni, Rob\\nnologies,Volume1(LongandShortPapers),pages Evans,SergeyFeldman,JosephGorney,DavidGra-\\n4171–4186,Minneapolis,Minnesota.Associationfor ham, Fangzhou Hu, Regan Huff, Daniel King, Se-\\nComputationalLinguistics. bastianKohlmeier,BaileyKuehl,MichaelLangan,\\nDaniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner,\\nMatthewDunn,LeventSagun,MikeHiggins,V.Ugur Kelsey MacMillan, Tyler Murray, Chris Newell,\\nGuney, Volkan Cirik, and Kyunghyun Cho. 2017. Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\\nSearchqa: Anewq&adatasetaugmentedwithcon- Shen,AmanpreetSingh,LucaSoldaini,Shivashankar\\ntextfromasearchengine. Subramanian,AmberTanaka,AlexD.Wade,Linda\\nWagner, Lucy Lu Wang, Chris Wilhelm, Caroline\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. Wu,JiangjiangYang,AngeleZamarron,Madeleine\\n2014. Openquestionansweringovercuratedandex- VanZuylen,andDanielS.Weld.2023. Thesemantic\\ntractedknowledgebases. InProceedingsofthe20th scholaropendataplatform.\\nACMSIGKDDInternationalConferenceonKnowl-\\nedge Discovery and Data Mining, KDD ’14, page TomKwiatkowski, JennimariaPalomaki, OliviaRed-\\n1156–1165, New York, NY, USA. Association for field,MichaelCollins,AnkurParikh,ChrisAlberti,\\nComputingMachinery. DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-\\ntonLee,KristinaToutanova,LlionJones,Matthew\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nSimCSE:Simplecontrastivelearningofsentenceem- Uszkoreit,QuocLe,andSlavPetrov.2019. Natural\\nbeddings. In Proceedings of the 2021 Conference Questions: A Benchmark for Question Answering\\nonEmpiricalMethodsinNaturalLanguageProcess- Research. TransactionsoftheACL.\\ning,pages6894–6910,OnlineandPuntaCana,Do-\\nJinhyukLee,WonjinYoon,SungdongKim,Donghyeon\\nminican Republic. Association for Computational\\nKim,SunkyuKim,ChanHoSo,andJaewooKang.\\nLinguistics.\\n2019. BioBERT:apre-trainedbiomedicallanguage\\nrepresentation model for biomedical text mining.\\nYuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto\\nBioinformatics,36(4):1234–1240.\\nUsuyama,XiaodongLiu,TristanNaumann,Jianfeng\\nGao,andHoifungPoon.2021. Domain-specificlan-\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nguagemodelpretrainingforbiomedicalnaturallan-\\nGhazvininejad,AbdelrahmanMohamed,OmerLevy,\\nguageprocessing. ACMTrans.Comput.Healthcare,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\n3(1).\\nBART:Denoisingsequence-to-sequencepre-training\\nfornaturallanguagegeneration,translation,andcom-\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\\nprehension. InProceedingsofthe58thAnnualMeet-\\nDistilling the Knowledge in a Neural Network. In\\ningoftheAssociationforComputationalLinguistics,\\nNeurIPSDeepLearningWorksop.\\npages7871–7880,Online.AssociationforComputa-\\ntionalLinguistics.\\nZhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon\\nDuede,KyleChard,andIanFoster.2023. Thedimin- PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMin-\\nishingreturnsofmaskedlanguagemodelstoscience. ervini,HeinrichKüttler,AleksandraPiktus,Pontus\\nStenetorp,andSebastianRiedel.2021. PAQ:65Mil-\\nShuHuangandJacquelineMCole.2022. Batterybert:\\nlionProbably-AskedQuestionsandWhatYouCan\\nA pretrained language model for battery database\\nDoWithThem. TransactionsoftheAssociationfor\\nenhancement. J. Chem. Inf. Model., page DOI:\\nComputationalLinguistics,9:1098–1115.\\n10.1021/acs.jcim.2c00035.\\nZehanLi,XinZhang,YanzhaoZhang,DingkunLong,\\nVladimirKarpukhin,BarlasOguz,SewonMin,Patrick Pengjun Xie, and Meishan Zhang. 2023. Towards\\nLewis,LedellWu,SergeyEdunov,DanqiChen,and generaltextembeddingswithmulti-stagecontrastive\\nWen-tauYih.2020. Densepassageretrievalforopen- learning.\\ndomainquestionanswering. InProceedingsofthe\\n2020ConferenceonEmpiricalMethodsinNatural YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-\\nLanguageProcessing(EMNLP),pages6769–6781, dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nOnline.AssociationforComputationalLinguistics. Luke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron proach. arXivpreprintarXiv:1907.11692.\\n\\n\\nKyleLo,LucyLuWang,MarkNeumann,RodneyKin- and VERification. In Proceedings of the 2018\\nney,andDanielWeld.2020. S2ORC:Thesemantic Conference of the North American Chapter of\\nscholaropenresearchcorpus. InProceedingsofthe the Association for Computational Linguistics:\\n58thAnnualMeetingoftheAssociationforCompu- Human Language Technologies, Volume 1 (Long\\ntationalLinguistics,pages4969–4983,Online.Asso- Papers), pages 809–819, New Orleans, Louisiana.\\nciationforComputationalLinguistics. AssociationforComputationalLinguistics.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, RobertTinn,HaoCheng,YuGu,NaotoUsuyama,Xi-\\nDarioAmodei,andIlyaSutskever.2019. Language aodong Liu, Tristan Naumann, Jianfeng Gao, and\\nmodelsareunsupervisedmultitasklearners. HoifungPoon.2023. Fine-tuninglargeneurallan-\\nguagemodelsforbiomedicalnaturallanguagepro-\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather- cessing. Patterns,4(4).\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe HugoTouvron,ThibautLavril,GautierIzacard,Xavier\\nlimitsoftransferlearningwithaunifiedtext-to-text Martinet,Marie-AnneLachaux,TimothéeLacroix,\\ntransformer. JournalofMachineLearningResearch, BaptisteRozière,NamanGoyal,EricHambro,Faisal\\n21(1). Azhar,AurelienRodriguez,ArmandJoulin,Edouard\\nGrave,andGuillaumeLample.2023. Llama: Open\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nandefficientfoundationlanguagemodels.\\nKnowwhatyoudon’tknow:Unanswerablequestions\\nforsquad. CoRR,abs/1806.03822.\\nAashka Trivedi, Takuma Udagawa, Michele Merler,\\nRameswarPanda,YousefEl-Kurdi,andBishwaran-\\nPranavRajpurkar,JianZhang,KonstantinLopyrev,and\\njanBhattacharjee.2023. Neuralarchitecturesearch\\nPercyLiang.2016. SQuAD:100,000+Questionsfor\\nfor effective teacher-student knowledge transfer in\\nMachineComprehensionofText. InEMNLP.\\nlanguagemodels. arXivpreprintarXiv:2303.09639.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-\\nTakumaUdagawa,AashkaTrivedi,MicheleMerler,and\\nBERT:SentenceembeddingsusingSiameseBERT-\\nBishwaranjanBhattacharjee.2023. Acomparative\\nnetworks. InProceedingsofthe2019Conferenceon\\nanalysisoftask-agnosticdistillationmethodsforcom-\\nEmpiricalMethodsinNaturalLanguageProcessing\\npressingtransformerlanguagemodels. InProceed-\\nandthe9thInternationalJointConferenceonNatu-\\ningsofthe2023ConferenceonEmpiricalMethodsin\\nralLanguageProcessing(EMNLP-IJCNLP),pages\\nNaturalLanguageProcessing: IndustryTrack,pages\\n3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria\\nEmilio,CarlesFalcon,SherezadeFuentes,LauraHer- Nicholas Walker, Amalie Trewartha, Haoyan Huo,\\nnandez, Gema Huesa, Jordi Huguet, Paula Marne, SanghoonLee,KevinCruse,JohnDagdelen,Alexan-\\nTaniaMenchón,GrégoryOperto,AlbinaPolo,San- der Dunn, Kristin Persson, Gerbrand Ceder, and\\ndra Pradas, Anna Soteras, Marc Vilanova, and Na- AnubhavJain.2021. Theimpactofdomain-specific\\ntalia Vilor-Tejedor. 2020. Novel tau biomarkers pre-trainingonnamedentityrecognitiontasksinma-\\nphosphorylatedatt181,t217ort231riseintheini- terialsscience. AvailableatSSRN3950755.\\ntial stages of the preclinical alzheimer&#x2019;s\\n<i>continuum</i> when only subtle changes in Liang Wang, Nan Yang, Xiaolong Huang, Binxing\\na&#x3b2; pathology are detected. EMBO Molec- Jiao,LinjunYang,DaxinJiang,RanganMajumder,\\nularMedicine,12(12):e12921. and Furu Wei. 2022. Text embeddings by weakly-\\nsupervisedcontrastivepre-training.\\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\nhishekSrivastava,andIrynaGurevych.2021. Beir: WenhuiWang,HangboBao,ShaohanHuang,LiDong,\\nAheterogenousbenchmarkforzero-shotevaluation and Furu Wei. 2021. MiniLMv2: Multi-head self-\\nofinformationretrievalmodels. attention relation distillation for compressing pre-\\ntrainedtransformers. InFindingsoftheAssociation\\nJames Thorne, Andreas Vlachos, Christos forComputationalLinguistics: ACL-IJCNLP2021,\\nChristodoulopoulos, and Arpit Mittal. 2018. pages2140–2151,Online.AssociationforComputa-\\nFEVER: a large-scale dataset for fact extraction tionalLinguistics.\\n\\n\\nYining Wang, Liwei Wang, Yuanzhi Li, Di He, and Model Training NASA-IR BEIRAvg.\\nTie-Yan Liu. 2013. A theoretical analysis of ndcg INDUS-RETRIEVERSMALL One-Stage 0.73 0.42\\ntyperankingmeasures. InProceedingsofthe26th INDUS-RETRIEVERSMALL Stagewise 0.72 0.41\\nAnnualConferenceonLearningTheory,volume30\\nofProceedingsofMachineLearningResearch,pages\\nTable9: AblationStudy: EvaluationresultsonNASA-\\n25–54,Princeton,NJ,USA.PMLR. QAandBEIR.NASA-QAshowedRecall10whileBEIR\\nreportednDCG10.\\nShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,\\nMarkDredze,SebastianGehrmann,PrabhanjanKam-\\nbadur, David Rosenberg, and Gideon Mann. 2023. C CompleteResultsonBEIRBenchmark\\nBloomberggpt: Alargelanguagemodelforfinance.\\nTable11showstheper-datasetresultsontheBEIR\\nShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao.\\ntasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.\\nA SentenceEmbeddingTrainingData\\nTable 10 shows the various data sources used for\\ntrainingembeddingmodels. Alldataispresented\\nin the form of text-pairs, where each item in the\\npairmaybeasentenceoraparagraph. Inthetable,\\nDataFormatdenotess2pforsentence-to-paragraph\\nmappings,s2sforsentence-to-sentencemappings,\\nandp2pforparagraph-to-paragraphmappings. We\\nusedabout360millionpairsfortrainingandused\\nin-batchnegatives.\\nB AblationStudy: Stage-wiseDistillation\\nforEmbeddingModel\\nFor the distilled embedding models, we find that\\nstage-wisedistillationdoesnotbenefitperformance\\nasmuchasaone-stepprocess, combiningallthe\\nsupervised and unsupervised data. As shown in\\nTable9,thestage-wiseapproachunderperformed\\ntheone-stageapproachby1percentagepointfor\\nbothNASA-QAandonBEIR.\\n\\n\\nDataset Num. Pairs DataCategory DataFormat\\nStackOverflow† 18562443 Title-Body s2p\\nStackExchangeMath† 2201906 Title-Body s2p\\nS2ORC[title-abstract](Loetal.,2020) 41769185 Title-Body s2p\\nS2ORCCitationPairs[Abstracts](Loetal.,2020) 52603982 Title-Body p2p\\nStackExchange[title-body]† 5415570 Title-Body s2p\\nWikipedia(Faderetal.,2014) 6458670 Title-Body s2p\\nArxiv(Clementetal.,2019) 2358545 Title-Body s2p\\nNASAADS[title-abstract](§2) 2633240 Title-Body s2p\\nPubMed[title-abstract](§2) 24001387 Title-Body s2p\\nPMC[title-abstract](§2) 2585537 Title-Body s2p\\nStackExchangeDuplicateQuestions[title-body-title-body]† 250460 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[body-body]† 250519 DuplicateQuestions p2p\\nStackExchangeDuplicateQuestions[title-title]† 304525 DuplicateQuestions s2s\\nWikiAnswerPairs(Faderetal.,2014) 77427422 DuplicateQuestions s2s\\nSpecterPairs(Cohanetal.,2020) 684100 CitationPairs s2s\\nS2ORCCitationPairs[Titles](Loetal.,2020) 52603982 CitationPairs s2s\\nSQuAD(Rajpurkaretal.,2016) 87599 QuestionAnswers s2p\\nNQ(Kwiatkowskietal.,2019) 100231 QuestionAnswers s2p\\nSearchQA(Dunnetal.,2017) 582261 QuestionAnswers s2p\\nStackExchange[title-answer]† 4067139 QuestionAnswers s2p\\nStackExchange[title-body-answer]† 187195 QuestionAnswers p2p\\nPAQ(Lewisetal.,2021) 64371441 QuestionAnswers s2p\\nFEVER(Thorneetal.,2018)∗ 109810 FactVerification s2p\\nHotpotQA(Yangetal.,2018)∗ 85000 QuestionAnswering s2p\\nTable10:TrainingDataforEmbeddingModels. Thetrainingdatatotalstoaround360Mpairs. DataFormatdenotes\\ns2pforsentence-to-paragraphmappings,s2sforsentence-to-sentencemappings,andp2pforparagraph-to-paragraph\\nmappings. †Downloaded from https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml.\\n∗OnlyusedforDistillation.\\nModel BEIREval\\nTREC- NFCorpus NQ HotPotQA FiQA ArguaAna Touche DBPedia Scidocs FEVER Climate SciFact AVG.\\nCovid FEVER BEIR\\nRoBERTa\\nBASE\\n0.47 0.30 0.54 0.34 0.38 0.52 0.18 0.25 0.22 0.46 0.14 0.67 0.37\\nBGEBASE 0.78 0.37 0.54 0.73 0.41 0.64 0.26 0.41 0.22 0.86 0.31 0.74 0.52\\nINDUS-RETRIEVERBASE 0.56 0.32 0.54 0.49 0.36 0.54 0.17 0.31 0.21 0.56 0.14 0.74 0.41\\nMINILM-V2 0.47 0.32 0.44 0.47 0.35 0.50 0.17 0.32 0.22 0.52 0.25 0.65 0.39\\nBGESMALL 0.76 0.34 0.50 0.70 0.40 0.60 0.26 0.40 0.21 0.87 0.32 0.71 0.51\\nINDUS-RETRIEVERSMALL 0.55 0.31 0.53 0.48 0.29 0.50 0.21 0.33 0.23 0.61 0.23 0.71 0.42\\nTable11: EvaluationresultsBEIR.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "format_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34a383ee-ae87-45a8-bd5a-7cf593787a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {'context' : retriever | format_docs, 'question' : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "865852f7-878d-47aa-9691-d064a77c7b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'CohereEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x162ec4590>)\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
       "| ChatCohere(client=<cohere.client.Client object at 0x162e8e510>, async_client=<cohere.client.AsyncClient object at 0x162ec4b90>, model='command-r', cohere_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a308ad-9505-415d-b91d-8fa180144ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The research focuses on developing domain-specific models for use in the science domain, namely INDUS. This collection of encoder-based LLMs aims to improve natural language understanding in various scientific fields. According to the benchmark results, INDUS performs better than comparable models, with strong results also for its smaller version, INDUS SMALL.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('Can you brief me the summary of this research paper in a very plain english which is easy to understand ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2f82a-9ae5-4573-a9bc-dadd43f17e19",
   "metadata": {},
   "source": [
    "Now, I have a follow-up question to understand about encoder models, LLM, everything about INDUS. How did it prove to be better than the other models?\n",
    "\n",
    "But for this, it would be ideal if the model knows about the conversation history so it can boil down the exact question very precisely and become time efficient.\n",
    "\n",
    "Thus let's add a chat history to this RAG Chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abc030-3cd0-4277-8986-4e5e95570bb9",
   "metadata": {},
   "source": [
    "##### 2 things : \n",
    "##### a) contextualize input question based on chat history\n",
    "##### b) add chat history to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c791b2-aa8a-43cb-8070-891135d2d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"Based on the given chat history and the most recent user question which\\\n",
    "could reference context in the chat history, create a standalone question which could be understood without\\\n",
    "the chat history. Do not answer the question. Just reformulate it if needed otherwise return it as it is.\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder('chat_history'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ab39dd-5798-4b19-82fe-87b7ac6c87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"You are an assistant for question answering tasks. Use the retrieved context to answer\\\n",
    "the user input. If you don't know the answer, just say you do not know.\\\n",
    "{context}\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', qa_system_prompt),\n",
    "    MessagesPlaceholder('chat_history'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "775e0377-521d-4eb9-ac16-94248466ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f2c2454-db48-4d6b-aa51-e79162b1ed1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
       " AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "chat_history.extend([\n",
    "    HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
    "    AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.')\n",
    "])\n",
    "\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "670d08e6-4816-4ddd-93cd-a95dde385487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
       " AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.'),\n",
       " HumanMessage(content='what are encoder models?'),\n",
       " AIMessage(content='Encoder models are a type of neural network that process sequential data such as text, speech, or time series data. They work by taking input data and producing a fixed-size vector representation, known as an embedding, that captures the semantics of the input. This embedding can then be used as a concise summary of the input sequence, enabling various natural language processing tasks.\\n\\nIn the context of text, an encoder model reads a sentence or a passage and produces a compact vector that encapsulates the meaning of the text. This vector, often called a sentence embedding or context representation, captures the important information present in the text. Encoder models are typically used as a first step in more complex models that perform tasks like text classification, information retrieval, or language translation.\\n\\nEncoders play a crucial role in many state-of-the-art natural language processing models, including those for sentiment analysis, named entity recognition, and machine translation. They help to efficiently transform variable-length input sequences into a fixed-size representation, making it easier to build and train subsequent layers for specific tasks.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what are encoder models?\"\n",
    "response = rag_chain.invoke({'input' : question, 'chat_history' : chat_history})\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=response['answer'])\n",
    "])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd7c8d5-38b6-481e-b75d-79eb76b77696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you brief me the summary of this research paper in a very plain english which is easy to understand ?'),\n",
       " AIMessage(content='This paper introduces INDUS, a family of encoder-based LLMs designed for use in the science domain. It compares their performance with other models and finds that INDUS outperformed them all. The researchers also created smaller versions of the models for resource-constrained applications.'),\n",
       " HumanMessage(content='what are encoder models?'),\n",
       " AIMessage(content='Encoder models are a type of neural network that process sequential data such as text, speech, or time series data. They work by taking input data and producing a fixed-size vector representation, known as an embedding, that captures the semantics of the input. This embedding can then be used as a concise summary of the input sequence, enabling various natural language processing tasks.\\n\\nIn the context of text, an encoder model reads a sentence or a passage and produces a compact vector that encapsulates the meaning of the text. This vector, often called a sentence embedding or context representation, captures the important information present in the text. Encoder models are typically used as a first step in more complex models that perform tasks like text classification, information retrieval, or language translation.\\n\\nEncoders play a crucial role in many state-of-the-art natural language processing models, including those for sentiment analysis, named entity recognition, and machine translation. They help to efficiently transform variable-length input sequences into a fixed-size representation, making it easier to build and train subsequent layers for specific tasks.'),\n",
       " HumanMessage(content='what is an encoder based LLM? Tell me about its architecture.'),\n",
       " AIMessage(content=\"An encoder-based LLM, or large language model, refers to a type of neural network model that uses an encoder architecture to process and understand natural language. These models are designed to learn the intricacies of human language by analyzing vast amounts of text data. They excel at capturing the semantic meaning and context of the input text, which makes them useful for a wide range of NLP tasks.\\n\\nThe architecture of an encoder-based LLM typically consists of several layers, including:\\n\\n1. Input Layer: The model starts by tokenizing the input text into smaller units, such as words or subwords. Each token is then embedded into a real-value vector of a certain dimension, creating an input layer.\\n\\n2. Encoder Layers: The core of the model is formed by a stack of encoder layers. These layers use mechanisms like self-attention or transformers to process the input sequentially. Each encoder layer transforms the previous layer's output into a new representation that captures different aspects of the text. The self-attention mechanism allows the model to weigh the importance of different words in the sentence, helping it to focus on relevant information.\\n\\n3. Pooling Layer: After the encoder layers, there is often a pooling layer that aggregates the sequence of encodings into a fixed-size vector. This vector represents the entire input sequence and can be used as a sentence embedding. Common pooling methods include mean pooling or max pooling.\\n\\n4. Decoder Layers (optional): In some models, such as those used for language generation or translation, there are additional decoder layers that take the fixed-size vector as input and generate an output sequence.\\n\\nThe encoder-based architecture has proven effective for various NLP tasks, as it can capture long-range dependencies in text and produce meaningful sentence embeddings. These embeddings can be used for information retrieval, text classification, or as a starting point for generating human-like text. Models like BERT, GPT, and RoBERTa are well-known examples of encoder-based LLMs that have achieved impressive results in natural language understanding and generation tasks.\")]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is an encoder based LLM? Tell me about its architecture.\"\n",
    "response = rag_chain.invoke({'input' : question, 'chat_history' : chat_history})\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question),\n",
    "    AIMessage(content=response['answer'])\n",
    "])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aac69a2d-fbf1-42b7-bee6-8f2a54f5bc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='transformer architecture (Vaswani et al., 2017)\\nLargelanguagemodels(LLMs)trainedongen- and are trained using general-purpose corpora\\neral domain corpora showed remarkable re-\\nlike Wikipedia or CommonCrawl (Devlin et al.,\\nsults on natural language processing (NLP)\\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\\ntasks. However, previous research demon-\\net al., 2020; Brown et al., 2020; Touvron et al.,\\nstratedLLMstrainedusingdomain-focusedcor-\\npora perform better on specialized tasks. In- 2023). Althoughthesegeneral-purposemodelsex-\\nspired by this pivotal insight, we developed hibitedstrongperformance,thedistributionalshift\\nINDUS,acomprehensivesuiteofLLMstailored ofvocabularyledtosub-optimalperformanceon\\nfortheEarthscience,biology,physics,helio- domain-specific natural language understanding\\nphysics, planetary sciences and astrophysics\\nand generation tasks (Beltagy et al., 2019). Fol-\\ndomains and trained using curated scientific\\nlowing this observation, several domain-specific', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 0, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 808, 'total_pages': 12}),\n",
       " Document(page_content='Model Specifications We created our sentence approximately56Msentences. Thismaskedauto-\\nembeddingmodelbyfine-tuningINDUS . Here- encoder model consisted of a full encoder along\\nBASE\\nafter, we refer to the resulting retriever model as with a shallow decoder. The model uses masked\\nINDUS-RETRIEVER . Wefollowedabi-encoder languagemodelingwithatrainingobjectivetore-\\nBASE\\nframework(ReimersandGurevych,2019),andex- covertheoriginalsentencebasedonthedecoder’s\\nperimented with multiple pooling strategies and maskedinputandthesentenceembeddinggener-\\nfoundthatthemeanpoolingofthecontextualized atedfromtheencoder’smaskedinput,viamasked\\ntransformerrepresentationsperformedthebest. language modelling. There is no distillation loss\\ncontributingtothisstep,whichcanbeviewedasan\\nTraining Details We trained each stage on 2\\nextendedpretrainingmechanism. Wefindthatthe\\nA100GPUswithaneffectivebatchsizeof1,024.\\nRetroMAEpretrainingdoesnotgiveusgoodgains\\nWe first trained with unsupervised data for 300K', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 4, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 0, 'total_pages': 12}),\n",
       " Document(page_content='3982–3992,HongKong,China.AssociationforCom-\\n20–31, Singapore. Association for Computational\\nputationalLinguistics.\\nLinguistics.\\nMarc Suárez-Calvet, Thomas K Karikari, Nicholas J\\nAshton,JuanLanteroRodríguez,MartaMilà-Alomà, AaronvandenOord,YazheLi,andOriolVinyals.2019.\\nJuan Domingo Gispert, Gemma Salvadó, Car- Representationlearningwithcontrastivepredictive\\nolina Minguillon, Karine Fauria, Mahnaz Shekari, coding.\\nOriolGrau-Rivera,EiderMArenaza-Urquijo,Aleix\\nSala-Vila,GonzaloSánchez-Benavides,JoséMaria Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nGonzález-de-Echávarri,GwendlynKollmorgen,Erik Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nStoops,EugeenVanmechelen,HenrikZetterberg,Kaj Kaiser,andIlliaPolosukhin.2017. Attentionisall\\nBlennow,JoséLuisMolinuevo,nullnull,Annabella youneed. InAdvancesinNeuralInformationPro-\\nBeteta, Raffaele Cacciaglia, Alba Cañas, Carme cessingSystems,volume30.CurranAssociates,Inc.\\nDeulofeu,IreneCumplido,RuthDominguez,Maria', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 9, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 2401, 'total_pages': 12}),\n",
       " Document(page_content='tasks.\\n2022. RetroMAE:Pre-trainingretrieval-orientedlan-\\nguagemodelsviamaskedauto-encoder. InProceed-\\ningsofthe2022ConferenceonEmpiricalMethodsin\\nNaturalLanguageProcessing,pages538–548,Abu\\nDhabi,UnitedArabEmirates.AssociationforCom-\\nputationalLinguistics.\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\\nMuennighoff. 2023. C-pack: Packaged resources\\ntoadvancegeneralchineseembedding.\\nJiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu.\\n2023. DistillCSE:Distilledcontrastivelearningfor\\nsentence embeddings. In Findings of the Associa-\\ntionforComputationalLinguistics: EMNLP2023,\\npages8153–8165,Singapore.AssociationforCom-\\nputationalLinguistics.\\nZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,\\nWilliamCohen,RuslanSalakhutdinov,andChristo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainablemulti-hopquestionanswering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncalMethodsinNaturalLanguageProcessing,pages\\n2369–2380,Brussels,Belgium.AssociationforCom-\\nputationalLinguistics.', metadata={'Author': '', 'CreationDate': 'D:20240522001021Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240522001021Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': 'https://arxiv.org/pdf/2405.10725', 'page': 10, 'source': 'https://arxiv.org/pdf/2405.10725', 'start_index': 779, 'total_pages': 12})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12248edf-291b-4a63-89c9-dd08d09f0be4",
   "metadata": {},
   "source": [
    "Now we can see that it's easy to ask follow-up questions to our assistant by attaching a conversational chat history for it to refer before giving an answer\n",
    "\n",
    "But one problem with this system is the effort needed to attach each question-answer response to this chat history after each invocation call. \n",
    "\n",
    "So to counter this, we need to modify it in a way that chat history gets updated after every invocation automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5433e2-4612-41af-a29b-51f3848f8bf0",
   "metadata": {},
   "source": [
    "##### Adding message history using RunnableWithMessageHistory to store messages based on session\n",
    "##### For now, storing it in-memory using a dictionary with session id and an instance of BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51b6f377-4096-4983-84cc-248ed4f9ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='input',\n",
    "    history_messages_key='chat_history',\n",
    "    output_messages_key='answer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca6d4e0d-57a5-40cf-90a6-a7e50b25a070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'RunnableWithChatHistoryInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'input': {'title': 'Input',\n",
       "   'anyOf': [{'type': 'string'},\n",
       "    {'$ref': '#/definitions/BaseMessage'},\n",
       "    {'type': 'array', 'items': {'$ref': '#/definitions/BaseMessage'}}]}},\n",
       " 'required': ['input'],\n",
       " 'definitions': {'BaseMessage': {'title': 'BaseMessage',\n",
       "   'description': 'Base abstract Message class.\\n\\nMessages are the inputs and outputs of ChatModels.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type', 'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'type']}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27199cb1-00bc-49ed-83c0-c7c470dc1c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'RunnableSequenceOutput',\n",
       " 'type': 'object',\n",
       " 'properties': {'chat_history': {'title': 'Chat History',\n",
       "   'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'},\n",
       "     {'$ref': '#/definitions/ToolMessage'}]}},\n",
       "  'input': {'title': 'Input', 'type': 'string'},\n",
       "  'answer': {'title': 'Answer', 'type': 'string'}},\n",
       " 'definitions': {'ToolCall': {'title': 'ToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id']},\n",
       "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'error': {'title': 'Error', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
       "    'tool_calls': {'title': 'Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/ToolCall'}},\n",
       "    'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/InvalidToolCall'}}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']}}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bf38d-e154-4dc9-bad4-f0ee4673090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {'input': 'Summarize the research paper for me.'},\n",
    "    config={\n",
    "        'configurable': {\n",
    "            'session_id': \n",
    "        }\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
